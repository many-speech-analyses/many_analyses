---
title: "Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses"
runninghead: "Multidimensional signals and analytic flexibility"
author:
- name: Stefano Coretta*
  num: 1, 2
- name: Joseph V. Casillas
  num: 3
- name: Simon Roessig
  num: 4
- name: Michael Franke
  num: 5
- name: Byron Ahn
  num: 6
- name: Ali H. Al-Hoorie
  num: 7
- name: Jalal Al-Tamimi
  num: 8
- name: Najd E. Alotaibi
  num: 9
- name: Mohammed K. AlShakhori
  num: 10
- name: Ruth M. Altmiller
  num: 11
- name: Pablo Arantes
  num: 12
- name: Angeliki Athanasopoulou
  num: 13
- name: Melissa M. Baese-Berk
  num: 14
- name: George Bailey
  num: 15
- name: Cheman Baira A Sangma
  num: 16
- name: Eleonora J. Beier
  num: 17
- name: Gabriela M. Benavides
  num: 18
- name: Nicole Benker
  num: 19
- name: Emelia P. BensonMeyer
  num: 20
- name: Nina R. Benway
  num: 21
- name: Grant M. Berry
  num: 22
- name: Liwen Bing
  num: 23
- name: Christina Bjorndahl
  num: 24
- name: Mariska Bolyanatz
  num: 25
- name: Aaron Braver
  num: 26
- name: Violet A. Brown
  num: 27
- name: Alicia M. Brown
  num: 28
- name: Alejna Brugos
  num: 29
- name: Erin M. Buchanan
  num: 30
- name: Tanna Butlin
  num: 31
- name: Andrés Buxó-Lugo
  num: 32
- name: Coline Caillol
  num: 33
- name: Francesco Cangemi
  num: 34
- name: Christopher Carignan
  num: 35
- name: Sita Carraturo
  num: 36
- name: Tiphaine Caudrelier
  num: 37
- name: Eleanor Chodroff
  num: 38
- name: Michelle Cohn
  num: 39
- name: Johanna Cronenberg
  num: 40
- name: Olivier Crouzet
  num: 41
- name: Erica L. Dagar
  num: 42
- name: Charlotte Dawson
  num: 43
- name: Carissa A. Diantoro
  num: 44
- name: Marie Dokovova
  num: 45
- name: Shiloh Drake
  num: 46
- name: Fengting Du
  num: 47
- name: Margaux Dubuis
  num: 48
- name: Florent Duême
  num: 49
- name: Matthew Durward
  num: 50
- name: Ander Egurtzegi
  num: 51
- name: Mahmoud M. Elsherif
  num: 52
- name: Janina Esser
  num: 53
- name: Emmanuel Ferragne
  num: 54
- name: Fernanda Ferreira
  num: 55
- name: Lauren K. Fink
  num: 56
- name: Sara Finley
  num: 57
- name: Kurtis Foster
  num: 58
- name: Paul Foulkes
  num: 59
- name: Rosa Franzke
  num: 60
- name: Gabriel Frazer-McKee
  num: 61
- name: Robert Fromont
  num: 62
- name: Christina García
  num: 63
- name: Jason Geller
  num: 64
- name: Camille L. Grasso
  num: 65
- name: Pia Greca
  num: 66
- name: Martine Grice
  num: 67
- name: Magdalena S. Grose-Hodge
  num: 68
- name: Amelia J. Gully
  num: 69
- name: Caitlin Halfacre
  num: 70
- name: Ivy Hauser
  num: 71
- name: Jen Hay
  num: 72
- name: Robert Haywood
  num: 73
- name: Sam Hellmuth
  num: 74
- name: Allison I. Hilger
  num: 75
- name: Nicole Holliday
  num: 76
- name: Damar Hoogland
  num: 77
- name: Yaqian Huang
  num: 78
- name: Vincent Hughes
  num: 79
- name: Ane Icardo Isasa
  num: 80
- name: Zlatomira G. Ilchovska
  num: 81
- name: Hae-Sung Jeon
  num: 82
- name: Jacq Jones
  num: 83
- name: Mágat N. Junges
  num: 84
- name: Stephanie Kaefer
  num: 85
- name: Constantijn Kaland
  num: 86
- name: Matthew C. Kelley
  num: 87
- name: Niamh E. Kelly
  num: 88
- name: Thomas Kettig
  num: 89
- name: Ghada Khattab
  num: 90
- name: Ruud Koolen
  num: 91
- name: Emiel Krahmer
  num: 92
- name: Dorota Krajewska
  num: 93
- name: Andreas Krug
  num: 94
- name: Abhilasha A. Kumar
  num: 95
- name: Anna Lander
  num: 96
- name: Tomas O. Lentz
  num: 97
- name: Wanyin Li
  num: 98
- name: Yanyu Li
  num: 99
- name: Maria Lialiou
  num: 100
- name: Ronaldo M. Lima Jr.
  num: 101
- name: Justin J. H. Lo
  num: 102
- name: Julio Cesar Lopez Otero
  num: 103
- name: Bradley Mackay
  num: 104
- name: Bethany MacLeod
  num: 105
- name: Mel Mallard
  num: 106
- name: Carol-Ann Mary McConnellogue
  num: 107
- name: George Moroz
  num: 108
- name: Mridhula Murali
  num: 109
- name: Ladislas Nalborczyk
  num: 110
- name: Filip Nenadić
  num: 111
- name: Jessica Nieder
  num: 112
- name: Dušan Nikolić
  num: 113
- name: Francisco G. S. Nogueira
  num: 114
- name: Heather M. Offerman
  num: 115
- name: Elisa Passoni
  num: 116
- name: Maud Pélissier
  num: 117
- name: Scott J. Perry
  num: 118
- name: Alexandra M. Pfiffner
  num: 119
- name: Michael Proctor
  num: 120
- name: Ryan Rhodes
  num: 121
- name: Nicole Rodríguez
  num: 122
- name: Elizabeth Roepke
  num: 123
- name: Jan P. Röer
  num: 124
- name: Lucia Sbacco
  num: 125
- name: Rebecca Scarborough
  num: 126
- name: Felix Schaeffler
  num: 127
- name: Erik Schleef
  num: 128
- name: Dominic Schmitz
  num: 129
- name: Alexander Shiryaev
  num: 130
- name: Márton Sóskuthy
  num: 131
- name: Malin Spaniol
  num: 132
- name: Joseph A. Stanley
  num: 133
- name: Alyssa Strickler
  num: 134
- name: Alessandro Tavano
  num: 135
- name: Fabian Tomaschek
  num: 136
- name: Benjamin V. Tucker
  num: 137
- name: Rory Turnbull
  num: 138
- name: Kingsley O. Ugwuanyi
  num: 139
- name: Iñigo Urrestarazu-Porta
  num: 140
- name: Ruben van de Vijver
  num: 141
- name: Kristin J. Van Engen
  num: 142
- name: Emiel van Miltenburg
  num: 143
- name: Bruce Wang
  num: 144
- name: Natasha Warner
  num: 145
- name: Simon Wehrle
  num: 146
- name: Hans Westerbeek
  num: 147
- name: Seth Wiener
  num: 148
- name: Stephen Winters
  num: 149
- name: Sidney G.-J. Wong
  num: 150
- name: Anna Wood
  num: 151
- name: Jane Wottawa
  num: 152
- name: Chenzi Xu
  num: 153
- name: Germán Zárate-Sández
  num: 154
- name: Georgia Zellou
  num: 155
- name: Cong Zhang
  num: 156
- name: Jian Zhu
  num: 157
- name: Timo B. Roettger
  num: 158

address:
- num: 1, 2
  org: Department of Linguistics and English Language, University of Edinburgh, United Kingdom
- num: 3
  org: Department of Spanish and Portuguese, Rutgers University, United States
- num: 4
  org: Department of Linguistics, Cornell University, United States
- num: 5
  org: Department of General and Computational Linguistics, University of Tübingen, Germany
- num: 6
  org: Program in Linguistics, Princeton University, United States
- num: 7
  org: Jubail English Language and Preparatory Year Institute, Royal Commission for Jubail and Yanbu, Saudi Arabia
- num: 8
  org: Laboratoire de Lingusitique Formelle (LLF), CNRS, Université Paris Cité, France
- num: 9
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 10
  org: Department of Linguistics, University of Arizona, United States
- num: 11
  org: Psychological and Brain Sciences, Washington University in Saint Louis, United States
- num: 12
  org: Departamento de Letras, Universidade Federal de São Carlos, Brazil
- num: 13
  org: School of Languages, Linguistics, Literatures and Cultures, University of Calgary, Canada
- num: 14
  org: Department of Linguistics, University of Oregon, United States
- num: 15
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 16
  org: School of Languages, Linguistics, Literatures and Cultures, University of Calgary, Canada
- num: 17
  org: Department of Psychology, University of California, Davis, United States
- num: 18
  org: Department of Linguistics, University of Arizona, United States
- num: 19
  org: Institute of Phonetics and Speech Processing, University of Munich, Germany
- num: 20
  org: NA, University of Pennsylvania, United States
- num: 21
  org: Department of Communication Sciences and Disorders, Syracuse University, United States
- num: 22
  org: Department of Spanish, Villanova University, United States
- num: 23
  org: Department of English Language and Linguistics, University of Birmingham, United Kingdom
- num: 24
  org: Department of Philosophy, Carnegie Mellon University, United States
- num: 25
  org: Department of Spanish & French Studies, Occidental College, United States
- num: 26
  org: Department of English, Texas Tech University, United States
- num: 27
  org: Psychological and Brain Sciences, Washington University in Saint Louis, United States
- num: 28
  org: Department of Spanish and Portuguese, University of Arizona, United States
- num: 29
  org: Division of Mathematics and Computer Science, Simmons University, United States
- num: 30
  org: Analytics, Harrisburg University of Science and Technology, United States
- num: 31
  org: School of Languages, Linguistics, Literatures and Cultures, University of Calgary, Canada
- num: 32
  org: Department of Psychology, University at Buffalo, SUNY, United States
- num: 33
  org: Université Paris Cité, France
- num: 34
  org: IfL-Phonetik, University of Cologne, Germany
- num: 35
  org: Department of Speech, Hearing and Phonetic Sciences, University College London, United Kingdom
- num: 36
  org: Psychological and Brain Sciences, Washington University in Saint Louis, United States
- num: 37
  org: Basque Center on Cognition Brain and Language, Spain
- num: 38
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 39
  org: Department of Linguistics, University of California, Davis, United States
- num: 40
  org: Institute of Phonetics and Speech Processing, University of Munich, Germany
- num: 41
  org: LLING, UMR6310, Nantes Université / CNRS, France
- num: 42
  org: Department of Linguisticss and TESOL, University of Texas at Arlington, United States
- num: 43
  org: School of Psychology, Newcastle University, United Kingdom
- num: 44
  org: Department of Linguistics, University of Oregon, United States
- num: 45
  org: School of Psychological Sciences and Health, University of Strathclyde, United Kingdom
- num: 46
  org: Department of Linguistics, University of Oregon, United States
- num: 47
  org: School of English Literature, Language and Linguistics, Newcastle University, United Kingdom
- num: 48
  org: Department of comparative language science, Universität Zürich, Swizerland, Switzerland
- num: 49
  org: Basque Center on Cognition Brain and Language, Spain
- num: 50
  org: Department of Linguistics, University of Canterbury, New Zealand
- num: 51
  org: IKER (UMR 5478), Centre National de la Recherche Scientifique (CNRS), France
- num: 52
  org: Department of Neuroscience, Psychology and Behaviour, University of Leicester, United Kingdom
- num: 53
  org: Statistics Group, Association for Diversity in Linguistics, Germany
- num: 54
  org: CLILLAC-ARP, Université Paris Cité, France
- num: 55
  org: Department of Psychology, University of California, Davis, United States
- num: 56
  org: Department of Music, Max Planck Institute for Empirical Aesthetics, Germany
- num: 57
  org: Department of Psychology, Pacific Lutheran University, United States
- num: 58
  org: Department of Linguistics, University of Oregon, United States
- num: 59
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 60
  org: Institute of Phonetics and Speech Processing, University of Munich, Germany
- num: 61
  org: Department of languages, linguistics, and translation, Université Laval, Canada
- num: 62
  org: New Zealand Institute of Language, Brain and Behaviour, University of Canterbury, New Zealand
- num: 63
  org: Department of Languages, Literatures, and Cultures, Saint Louis University, United States
- num: 64
  org: Department of Psychology, Princeton University, United States
- num: 65
  org: LPC, Aix Marseille Univ, CNRS, France
- num: 66
  org: Institute of Phonetics and Speech Processing, University of Munich, Germany
- num: 67
  org: IfL-Phonetik, University of Cologne, Germany
- num: 68
  org: Department of English Language and Linguistics, University of Birmingham, United Kingdom
- num: 69
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 70
  org: School of English Literature, Language and Linguistics, Newcastle University, United Kingdom
- num: 71
  org: Department of Linguistics and TESOL, University of Texas at Arlington, United States
- num: 72
  org: New Zealand Institute of Language, Brain and Behaviour, University of Canterbury, New Zealand
- num: 73
  org: Ao Tawhiti Unlimited Discovery, New Zealand
- num: 74
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 75
  org: Department of Speech, Language, and Hearing Sciences, University of Colorado Boulder, United States
- num: 76
  org: Department of Linguistics and Cognitive Science, Pomona College, United States
- num: 77
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 78
  org: University of California, Los Angeles, United States
- num: 79
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 80
  org: Department of Modern and Classical Languages and Literatures, California State University, Northridge, United States
- num: 81
  org: School of Psychology, University of Birmingham, United Kingdom
- num: 82
  org: School of Humanities, Language and Global Studies, University of Central Lancashire, United Kingdom
- num: 83
  org: Department of Linguistics, University of Canterbury, Aotearoa
- num: 84
  org: Programa de Pós-Graduação em Letras, Federal University of Rio Grande do Sul, Brazil
- num: 85
  org: Department of Linguistics, University of Canterbury, New Zealand
- num: 86
  org: Institute of Linguistics, University of Cologne, Germany
- num: 87
  org: Department of Linguistics, University of Washington, United States
- num: 88
  org: School of English Literature, Language and Linguistics, Newcastle University, United Kingdom
- num: 89
  org: Department of Language and Linguistic Science, University of York, United Kingdom
- num: 90
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 91
  org: Department of Communication and Cognition, Tilburg University, Netherlands
- num: 92
  org: Department of Communication and Cognition, Tilburg University, the Netherlands
- num: 93
  org: Department of Linguistics and Basque Studies, University of the Basque Country UPV/EHU, Spain
- num: 94
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 95
  org: Department of Psychology, Bowdoin College, ME, United States
- num: 96
  org: Linguistic Convergence Laboratory, HSE University, Russia
- num: 97
  org: Department of Communication and Cognition, Tilburg University, the Netherlands
- num: 98
  org: School of Psychology, University of Birmingham, United Kingdom
- num: 99
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 100
  org: Institute of German Language I Linguistics, University of Cologne, Germany
- num: 101
  org: Department of English Language Studies, Federal University of Ceará, Brazil
- num: 102
  org: Department of Speech, Hearing and Phonetic Sciences, University College London, United Kingdom
- num: 103
  org: Department of Hispanic Studies, University of Houston, United States
- num: 104
  org: Department of English and American Studies, University of Salzburg, Austria
- num: 105
  org: School of Linguistics & Language Studies, Carleton University, Canada
- num: 106
  org: Psychological and Brain Sciences, Washington University in Saint Louis, United States
- num: 107
  org: Population Health Sciences Institute - Faculty of Medical Sciences, Newcastle University, United Kingdom
- num: 108
  org: HSE University, Russia
- num: 109
  org: Speech and Language Therapy, University of Strathclyde, United Kingdom
- num: 110
  org: LPC, Aix Marseille Univ, CNRS, France
- num: 111
  org: Department of Psychology, Faculty of Media and Communications, Singidunum University, Serbia
- num: 112
  org: Institute of Linguistics, Heinrich-Heine University Düsseldorf, Germany
- num: 113
  org: School of Languages, Linguistics, Literatures and Cultures, University of Calgary, Canada
- num: 114
  org: Graduate Program in Linguistics, Federal University of Ceará, Brazil
- num: 115
  org: World Languages, Literatures and Cultures Department, University of Arkansas, United States
- num: 116
  org: Deparment of Linguistics, Queen Mary University of London, United Kingdom
- num: 117
  org: CLILLAC-ARP, Université Paris Cité, France
- num: 118
  org: Department of Linguistics, University of Alberta, Canada
- num: 119
  org: Department of Linguistics, University of California, Berkeley, United States
- num: 120
  org: Department of Linguistics, Macquarie University, Australia
- num: 121
  org: Center for Cognitive Science, Rutgers University, United States
- num: 122
  org: Department of Spanish and Portuguese, Rutgers University, United States
- num: 123
  org: Department of Speech, Language, and Hearing Sciences, Saint Louis University, United States
- num: 124
  org: Department of Psychology, Witten/Herdecke University, Germany
- num: 125
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 126
  org: Department of Linguistics, University of Colorado Boulder, United States
- num: 127
  org: Clinical Audiology, Speech and Language (CASL) Research Centre, Queen Margaret University Edinburgh, United Kingdom
- num: 128
  org: Department of English and American Studies, University of Salzburg, Austria
- num: 129
  org: Department of English and American Studies, Heinrich Heine University Düsseldorf, Germany
- num: 130
  org: Linguistic Convergence Laboratory, HSE University, Russia
- num: 131
  org: Department of Linguistics, University of British Columbia, Canada
- num: 132
  org: Department of Psychiatry and Psychotherapy, University Hospital Cologne, Germany
- num: 133
  org: Department of Linguistics, Brigham Young University, United States
- num: 134
  org: Department of Linguistics, University of Colorado Boulder, United States
- num: 135
  org: Department of Neuroscience, Max Planck Institute for Empirical Aesthetics, Germany
- num: 136
  org: Department of General Linguistics, University of Tübingen, Germany
- num: 137
  org: Department of Linguistics, University of Alberta, Canada
- num: 138
  org: School of English Literature, Language and Linguistics, Newcastle University, United Kingdom
- num: 139
  org: Department of English & Literary Studies, University of Nigeria, Nsukka, Nigeria
- num: 140
  org: IKER-UMR5478, Centre National de la Recherche Scientifique (CNRS), France
- num: 141
  org: Institute of Linguistics, Heinrich-Heine University Düsseldorf, Germany
- num: 142
  org: Psychological and Brain Sciences, Washington University in Saint Louis, United States
- num: 143
  org: Department of Communication and Cognition, Tilburg University, the Netherlands
- num: 144
  org: Chinese and Bilingual Studies, Hong Kong Polytechnic University, Hong Kong SAR, China
- num: 145
  org: Department of Linguistics, University of Arizona, United States
- num: 146
  org: IfL-Phonetik, University of Cologne, Germany
- num: 147
  org: Department of Languages, Literature and Communication, Utrecht University, Netherlands
- num: 148
  org: Department of Modern Languages, Carnegie Mellon University, United States
- num: 149
  org: School of Languages, Linguistics, Literatures and Cultures, University of Calgary, Canada
- num: 150
  org: Geospatial Research Institute, University of Canterbury, New Zealand
- num: 151
  org: Department of Linguistics, University of Oregon, United States
- num: 152
  org: Département de Lettres modernes, LIUM, LST, Le Mans Université, France
- num: 153
  org: NA, University of Oxford, United Kingdom
- num: 154
  org: Department of Spanish, Western Michigan University, United States
- num: 155
  org: Department of Linguistics, University of California, Davis, United States
- num: 156
  org: School of Education, Communication and Language Studies - ECLS, Newcastle University, United Kingdom
- num: 157
  org: School of Information, University of Michigan, Ann Arbor, United States
- num: 158
  org: Department of Linguistics and Scandinavian Studies, University of Oslo, Norway

corrauth: "Timo B. Roettger"
email: "timo.b.roettger@iln.uio.no"
abstract: "Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis which can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling, but also from decisions regarding the quantification of the measured behavior. In the present study, we gave the same speech production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further find little to no evidence that the observed variability can be explained by analysts' prior beliefs, expertise or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system and calibrate their (un)certainty in their conclusions"
keywords: "crowdsourcing science, data analysis, scientific transparency, speech, acoustic analysis"
classoption:
  - Review
  - times
bibliography: references
bibliographystyle: sageh
output:
  bookdown::pdf_book:
    base_format: rticles::sage_article
    keep_tex: yes
    template: template.tex
    includes:
      in_header: preamble.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
knitr::opts_knit$set(root.dir = here::here())
library(tidyverse)
library(patchwork)
library(brms)
library(extraDistr)
library(faux)
library(tidybayes)
library(here)
library(googlesheets4)
library(broom.mixed)
library(kableExtra)
```

```{r analysis-preferences}
my_seed <- 115116106
```

```{r}
#| label: helper-functions

# Round and format numbers to exactly N digits
specify_decimal <- function(x, k) {
  out <- trimws(format(round(x, k), nsmall = k))
  return(out)
}

# Print values from tibble
pull_from_tib <- function(df, col, row, val) {
  col <- enquo(col)
  row <- enquo(row)
  val <- enquo(val)
  val <- filter(df, !!col == !!row) %>% pull(!!val)
  return(val)
}

# Report posteriors in prose
report_posterior <- function(df, param) {
  # Extract wanted value from model output
  est  <- df[df$param == param, "estimate"]
  cis  <- df[df$param == param, "ci"]

  capture.output(
    paste0("($\\beta$ = ", est, ", 95\\% CrI = ", cis, ")", "\n") %>% 
      cat()) %>% 
      paste()
}

pm  <- function(x) {
  line <- glue::glue("$\\pm {x}$")
  return(line)
}
```


# Introduction

In order to effectively accumulate knowledge, science needs (i) to produce data that can be replicated using the original methods and (ii) to arrive at robust conclusions substantiated by such data.
In recent coordinated efforts to replicate published findings, scientific disciplines have uncovered surprisingly low success rates [e.g., @open2015estimating; @camerer2018evaluating] leading to what is now referred to as the *replication crisis*.
Beyond the difficulties of replicating scientific findings, a growing body of evidence suggests that researchers' conclusions often vary even when they have access to the same data.
The latter situation has been referred to as the *inference crisis* [@rotello2015more; @starns2019assessing] and is, among other things, rooted in the inherent flexibility of data analysis [often referred to as researcher degrees of freedom: @simmons2011false; @gelman2014statistical].
Data analysis involves many different steps, such as inspecting, organizing, transforming, and modeling data, to name a few.
Along the way, different methodological and analytic choices need to be made, all of which may influence the final interpretation of the data.

These researcher degrees of freedom are both a blessing and a curse.
They are a blessing because they afford us the opportunity to look at nature from different angles, which, in turn, allows us to make important discoveries and generate new hypotheses [e.g., @box1976science; @tukey1977exploratory; @de2014thought].
They are a curse because idiosyncratic choices can lead to categorically different interpretations, which eventually find their way into the publication record where they are taken for granted [@simmons2011false].
Recent projects have shown that the variability between different data analysts is vast and can lead independent researchers to draw different conclusions from the same data set [e.g., @silberzahn2018many; @starns2019assessing; @botvinik-nezer2020].
These studies, however, might still underestimate the extent to which analysts vary because data analysis is not restricted to the statistical analysis of ready-made numeric data.
These data can in fact be the result of complex measurement processes that translate a phenomenon, such as human behavior, into numbers.
This is particularly true for fields that draw conclusions about human behavior and cognition from multidimensional data like audio or video data.
In fields working on speech production, for example, researchers need to make numerous decisions about what to measure and how to measure it, in other words, how to operationalize the phenomenon under investigation.
This is not trivial, given the temporal extension of the acoustic signal and its complex structural composition.
<!-- Not only can decisions about measuring the signal influence downstream decisions about statistical modeling, but statistical results or modeling issues can also lead researchers to go back and revise earlier decisions about the measuring process itself. -->

In this article, we investigate the impact of analytic choices on research results when many analyst teams examine the same speech production data set, a process that involves both decisions regarding the *operationalization* of linguistically relevant constructs and decisions regarding *statistical analysis*.
Specifically, we discuss the degree of variability in research results obtained by 46 teams who had to choose the operationalization and statistical procedures to answer the same research question, on the basis of the same set of raw data (here, speech recordings).
Our goals are twofold: (i) our study conceptually replicates previous many-analyses projects, by probing the effects of different statistical analyses and by assessing the generalizability of published findings to other disciplines (here, the speech sciences);
(ii) our study extends the scope of inquiry to include flexibility in the operationalization of complex human behavior (here, speech).
This is an important addition in that the increased number of "forking paths" in the "garden of analytic choices"---derived from the many decisions involved in quantification---might reveal a higher degree of variability across analysts than previously observed, thus giving us a more realistic estimate of variability. 

## Researcher degrees of freedom

Data analysis comes with many decisions, for example how to measure a given phenomenon or behavior, which data to submit to statistical modeling and which to exclude in the final analysis, or what inferential decision-making procedure to apply.
This can be problematic because humans show cognitive biases that can lead to erroneous inferences.
Humans are biased [e.g., @tversky1974judgment], e.g. they see coherent patterns in randomness [@brugger2001], convince themselves of the validity of prior expectations ["I knew it", @nickerson1998confirmation], and perceive events as being plausible in hindsight ["I knew it all along", @fischhoff1975hindsight].
In conjunction with an academic incentive system that rewards certain discovery processes more than others [@sterling1959publication; @koole2012rewarding], we often find ourselves exploring many possible analytic pipelines, but only reporting a selected few.

This issue is particularly amplified in fields in which the raw data lend themselves to many possible ways of being measured [@roettger2019researcher].
Combined with a wide variety of methodological and theoretical traditions as well as varying levels of quantitative training across subfields, the inherent flexibility of data analysis might lead to a vast plurality of analytic approaches that can lead to different scientific conclusions [@roettger2019emergent].
Analytic flexibility has been widely discussed from a conceptual point of view [@simmons2011false; @wagenmakers2012agenda; @nosek2014method] and in regard to its application in individual scientific fields [e.g. @wicherts2016; @charles2019; @roettger2019researcher].
This notwithstanding, there are still many unknowns regarding the extent of analytic plurality in practice.

Consequently, a substantial body of published papers likely present overconfident interpretations of data and statistical results based on idiosyncratic analytic strategies [e.g., @simmons2011false; @gelman2014statistical].
These interpretations, and the conclusions that derive from them, are thus associated with an unknown degree of uncertainty (dependent on the strength of evidence provided) and with an unknown degree of generalizability (dependent on the chosen analysis).
Moreover, the same data could lead to very different conclusions depending on the analytic path taken by the researcher.
However, instead of being critically evaluated, scientific results often remain unchallenged in the publication record.
Despite recent efforts to improve transparency and reproducibility [e.g. @miguel2014promoting; @klein2018practical] and the advent of freely available and accessible infrastructures, such as those provided by the Open Science Framework ([osf.io](https://osf.io)), critical re-analyses of published analytic strategies are still uncommon because data sharing remains rare [@wicherts2006poor].

## Crowd-sourcing alternative analyses

Recent collaborative attempts have started to shed light on how different analysts tackle the same data set and have revealed a large amount of variability.
In a pioneering collaborative effort, @silberzahn2018many let twenty-nine independent analysis teams address the same research hypothesis: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players.
The analytic approaches and, consequently, the results varied widely between teams.
Twenty teams (69%) found support for the hypothesis, and 9 (31%) did not.
Out of the 29 analytic strategies, there were 21 unique combinations of covariates.
Importantly, the observed variability was neither predicted by the teams' preconceptions about the phenomenon under investigation nor by peer ratings of the quality of their analyses. 
The authors' results suggest that analytic plurality may be an inevitable byproduct of the scientific process and not necessarily driven by different levels of expertise or bias.

Several other recent studies corroborated this analytic flexibility across different disciplines.
@dutilh2019 and @starns2019assessing investigated analysts' choices when inferring theoretical constructs based on the same data set using computational models.
Both studies revealed vastly different modeling strategies, even though scientific conclusions were similar across analysis teams [see also @Parker2020, and @botvinik-nezer2020, regarding analytic flexibility in ecology and neuroimaging data, respectively].
@bastiaansen2020 crowd-sourced clinical recommendations based on analyses of an individual patient.
Their results suggest that analysts differed substantially regarding decisions related to both the statistical analysis of the data and the theoretical rationale behind interpreting the statistical results.

Building on the many-analysts approach, @landy2020crowdsourcing asked 15 research teams to independently design studies to answer five different research questions related to moral judgments.
Again, they found vast heterogeneity across researchers' conclusions.
The observed variation was not predicted by the researchers' expertise, but seem to vary for the five different research questions which might exhibit different degrees of theoretical underspecification.
This is in line with @auspurg2021has who re-analyzed the red card study mentioned above.
The authors argue that some of the observed heterogeneity across analysts in @silberzahn2018many might have been driven by flexibility in statistically interpreting the research question.

While these studies attested a large degree of analytic flexibility with possibly impactful consequences, they focused on analytic decisions related to the study design, the statistical analysis or the architecture of computational models. 
In these studies the data sets were fixed and neither data collection nor measurement could be changed.
Thus the estimates of variability found in the literature might reflect a lower bound only, ignoring large parts of the forking paths related to measurement. 
However, in many fields the primary raw data are complex signals, for which theoretical constructs need to be operationalized relative to a theoretically motivated research question.
This is especially true in the Social Sciences, where the phenomenon under investigation corresponds to both observable and unobservable human behavior.

Decisions about how to measure theoretical constructs related to human behavior and cognition might interact with downstream decisions about statistical modeling and vice versa.
For instance, @flake2020 discuss the cascading impact that different practices can have on psychometric research.
The authors highlight, among others, the following degrees of freedom in the choice and development of measures: definition of the theoretical construct, justification of the selected measure, description of the measure and of how it maps onto the construct, response coding and related transformations, as well as post-hoc modifications to the chosen measure.
Taken together, these aspects alone dramatically increase the combinations of possible analytic choices, and hence flexibility in research outcomes.

In those disciplines concerned with communication, human behavior often corresponds to multidimensional visual and/or acoustic signals.
The complex nature of this data exponentiates the number of possible analytic approaches, thus further increasing analytic flexibility.
In order to estimate this increased flexibility, the present study looks at experimentally elicited speech production data.  
<!--in order to understand how this flexibility manifests itself in a scenario where several procedures are involved not only in the statistical analysis of numeric data, but also in the operationalization and measuring of complex signals necessary to derive such data.-->

## Operationalizing speech {#s:operspeech}

Research on speech lies at the intersection of the cognitive sciences, informing psychological models of language, categorization, and memory, guiding methods for diagnosis and treatment of speech disorders, and facilitating advancement in automatic speech recognition and speech synthesis.
One major challenge in the Speech Sciences is the mapping between communicative intentions (the unobserved behavior) and their physical manifestation (the observed behavior).

Speech signals are complex as they are characterized by structurally different acoustic parameters distributed throughout different temporal domains.
Thus, choosing how to assess a communicative intention of interest is an important analytic step.
Take for example the sentence in (1).

\vspace{1em}
(1) "I can't bear another meeting on Zoom."
\vspace{1em}

\noindent Depending on the speaker's intention, this sentence can be said in different ways.
For instance, if the speaker is exhausted by all their meetings, they might acoustically highlight the word *another* or *meeting* to contrast it with more pleasant activities.
If, on the other hand, the speaker is just tired of video conferences, as opposed to say face-to-face meetings, they might acoustically highlight the word *Zoom*.

If we decide to compare the speech signal associated with these two intentions, how can we quantify the difference between them?
In other words, given their physical manifestation (speech), what do we measure and how do we measure it?
Because of the continuous and transient nature of speech, identifying speech parameters and temporal domains within which to measure those parameters becomes a non-trivial task.
Utterances stretch over several thousand milliseconds and contain different levels of linguistically relevant units such as phrases, words, syllables, and individual sounds.
The researcher is thus confronted with a considerable number of parameters and combinations thereof to choose from.

From a phonetic viewpoint, linguistically relevant units are inherently multidimensional and dynamic: they consist of clusters of parameters that are modulated over time.
The acoustic parameters of units are usually asynchronous, i.e. they appear at different time points in the unfolding signal, and overlap with parameters of other units [e.g. @jongman2000acoustic; @lisker1986voicing; @summerfield1981articulatory; @winter2014spoken].
A classic example is the distinction between voiced and voiceless stops in English (i.e. /b/ and /p/ in *bear* vs. *pear*).
This contrast is manifested by many acoustic features which can differ depending on several factors, such as the position of the consonant in the word and context of surrounding sounds [@lisker1977rapid].
Furthermore, correlates of the contrast can even be found away from the consonant, in temporally distant speech units.
For example, the initial /l/ of the English words *led* and *let* is affected by the voicing of the final consonant (/d, t/) [@hawkins2004influence].

The multiplicity of phonetic measurements grows exponentially if we look at larger temporal domains, as is the case with suprasegmental aspects of speech.
For example, studies investigating acoustic correlates of word stress (e.g. the difference between *ínsight* and *incíte*) use a wide variety of measurements, including temporal characteristics (duration of certain segments or sub-segmental intervals), spectral characteristics (intensity, formants, and spectral tilt), and measurements related to fundamental frequency (f0) [e.g., @gordon2017acoustic].
Moving on to the expression of higher-level communicative functions, like information structure and discourse pragmatics, relevant acoustic cues can be distributed throughout even larger domains, such as phrases and whole utterances [e.g., @ladd2008intonational].
Differences in position, shape, and alignment of f0 modulations over multiple locations within a sentence are correlated with differences in discourse functions [e.g., @niebuhr2011].
The latter can also be expressed by global vs. local pitch modulations [@heuven2002], as well as acoustic information within the temporal or spectral domain [e.g., @van2005speech].
Extra-linguistic information, like the speaker's intentions, levels of emotional arousal or social identity, are also conveyed by broad-domain parameters, such as voice quality, rhythm, and pitch [@foulkes2006; @ogden2004; @white2009].

In short, when testing hypotheses on speakers' intentions using speech production data, researchers are faced with many choices and possibilities.
The larger the functional domain (e.g. segments vs. words vs. utterances), the higher the number of conceivable operationalizations.
For example, several decisions have to be made when comparing the two realizations of the sentence in (1), one of which is intended to signal emphasis on *another* and one of which emphasizes *Zoom* (see 2a and 2b).

\vspace{1em}

(2a) I can't bear *ANOTHER* meeting on Zoom.

(2b) I can't bear another meeting on *ZOOM*.

\vspace{1em}

\noindent Do we compare only the word *another* in (2a) and (2b), or also the word *Zoom*?
Do we measure utterance-wide acoustic profiles, whole words, or just stressed syllables?
Do we average across the chosen time domain or do we measure a specific point in time?
Do we measure f0, intensity, or something else [@stevens2000]?

<!-- FIGURE -->
(ref:forkingPaths) Illustration of the analytic flexibility associated with acoustic analyses. (A) An example of multiple possible and justifiable decisions when comparing two utterances; (B) Waveform and f0 track of the utterances *I can't bear ANOTHER meeting on Zoom* and *I can't bear another meeting on ZOOM*. The green boxes mark the word *another* in both sentences; (C) Spectrogram and f0 track of the word *another*, exemplifying possible operationalizations of differences in f0.

```{r forkingPaths, echo=FALSE, fig.cap="(ref:forkingPaths)", out.width="100%"}
knitr::include_graphics(here::here("figs", "ForkingPaths.png"), error = FALSE)
```
<!-- END:FIGURE -->

When looking at phrase-level temporal domains, the number of possible alternative analytic pipelines increases substantially.
Figure \@ref(fig:forkingPaths)A shows a typical example of a decision tree with which speech researchers are often confronted.
Each of the four analytic decisions in the example have different possible options.
Here only one particular path has been taken. A different one would likely produce different results and might lead to different conclusions.
Once we have decided to compare f0 of the word *another* across the two utterances, there are still many choices to be made, all of which need to be justified.
As Figures \@ref(fig:forkingPaths)B-C illustrate, we could measure f0 at specific points in time like the onset of the temporal window, the offset, or the midpoint.
We could also measure the value or time of the f0 minimum or maximum.
We could summarize f0 across the entire window and extract the mean, median or standard deviation of f0, all of which have been used to analyze speech data in previous work [see @gordon2017acoustic].
But the journey in the garden of analytic paths goes on.
Other important operationalization steps could involve filtering the audio signal, smoothing the extracted f0 track, removing values that substantially deviate from surrounding values or expectations, either manually or automatically, and so on.

These decisions are intended to be made prior to any statistical analysis, but are at times revised *a posteriori* in light of unforeseen or surprising outcomes (i.e. after data collection and/or preliminary analyses).
This multitude of possible decisions are multiplied by those researcher degrees of freedom related to statistical analysis [e.g. @wicherts2016].

In sum, speech data is made of complex physical signals that generate an as-of-yet unappreciated amount of analytic flexibility in the choice of measures and operationalizations.
The present paper probes this garden of forking paths in the analysis of speech.
To assess the variability in data analysis pipelines, including both operationalization and statistical analysis, across independent researchers, we provide analytic teams with an experimentally elicited speech production data set.
The data set derives from the unpublished research project *Prosodic encoding of redundant referring expressions*, which set out to investigate whether speakers acoustically modify utterances to signal unexpected referring expressions.^[Results of this research project were neither published nor publicly presented and are stored on a private OSF repository.] 
In the following section we introduce the research question and the experimental procedure of said project, and we describe the resulting data set as used in the current study.

## The data set: The acoustic properties of atypical modifiers {#s:dataset}

Referring is one of the most basic and prevalent uses of language and one of the most widely researched areas in Language Science. 
When trying to refer to a banana, what does a speaker say and how do they say it in a given context?
The context within which an entity occurs (i.e., with other non-fruits, other fruits, or other bananas) plays a large part in determining the choice of referring expressions.
Generally, speakers aim to be as informative as possible to uniquely establish reference to the intended object, but they are also resource-efficient in that they avoid redundancy [@grice1975logic].
Thus one would expect the use of a modifier, for example, only if it is necessary for disambiguation.
For instance, one might use the adjective *yellow* to describe a banana in a situation in which there are both a yellow and a less ripe green banana available, but not when there is only one banana.

Despite the coherent idea that speakers are both rational and efficient, there is much evidence that speakers are often over-informative.
Speakers use referring expressions that are more specific than strictly necessary for the unambiguous identification of the intended referent [@sedivy2003pragmatic; @rubio2016redundant], which has been argued to facilitate object identification and make communication between speakers and listeners more efficient [@arts2011overspecification; @paraboni2007generating; @rubio2016redundant].
Recent findings suggest that the utility of referring expressions depends on how useful they are for a listener (compared to other referring expressions) to identify a target object.
For example, @degen2020redundancy showed that modifiers that are less typical for a given referent (e.g. a blue banana) are more likely to be used in an over-informative scenario (e.g. when there is just one banana)[see also @westerbeek2015stored].
This account, however, has mainly focused on content selection [@gatt2013we], i.e. what words to use.

Even when morphosyntactically identical expressions are involved, speakers can modulate utterances via acoustic properties like temporal and spectral modifications [e.g., @ladd2008intonational].
Most prominently, languages can use intonation to signal discourse relationships between referents.
Intonation marks discourse-relevant referents for being new or given information, to guide the listeners’ interpretation of incoming messages.
Beyond structuring information relative to the discourse, a few studies suggest that speakers might use intonation to signal atypical lexical combinations [e.g. @dimitrova2008prosodic; @dimitrova2009did].
Referential expressions such as *blue banana* were produced with greater prosodic prominence than more typical referents such as *yellow banana*.
These results are in line with the idea of resource-efficient, rational language users who modulate their speech in order to facilitate listeners' comprehension.
However, the above studies are based on a small sample size (10 participants) and on potentially anti-conservative statistical analyses, leaving reason to doubt the generalizability of the studies' conclusions.

To further illuminate the question of whether speakers modify speech to signal atypical referents, and overcome some of the limitations of previous work, thirty native German speakers were recorded in a production study while interacting with a confederate (one of the experimenters) in a referential game, following experimental procedures typical of the field.
The participants had to verbally instruct the confederate to select a specified target object out of four objects presented on a screen.
The subject and confederate were seated at the opposite sides of a table, each facing one of two computer screens.
The participant and the experimenter could not see each other nor each others' screens.
Figure \@ref(fig:procedural) shows the experimental procedure time-line.
After a familiarization phase, the subject first saw four colored objects in the top left, top right, bottom left, and bottom right corners of the screen.
One of the objects served as the target, another as the competitor, and the remaining two objects served as distractors.
Objects were referred to using noun phrases consisting of an adjective modifier denoting color and a modified object (e.g. *gelbe Zitrone* 'yellow lemon', *rote Gurke* 'red cucumber', *rote Socken* 'red socks').

<!-- FIGURE -->
(ref:procedural) Experimental procedure. The upper row illustrates the trial sequence for the speaker (participant) and the lower row illustrates the trial sequence for the confederate. After a preview of 1500ms the speaker sees an arrow indicating one of the referents (b). Reading the orthographic instructions out loud, the speaker gives the confederate verbal instructions onto which referent they should drag the cube (c). The confederate, in turn, drags the black cube onto the target referent (d). Both the arrow and the orthographic instruction disappear from the speaker's screen and a new referent is indicated by an arrow on the same display alongside a new orthographic instruction (e). The speaker gives the confederate verbal instructions (f) which the confederate follows by dragging the cube onto the next referent (g).

```{r procedural, echo=FALSE, fig.cap="(ref:procedural)", out.width="100%"}
knitr::include_graphics(here::here("figs", "procedure_plot.png"), error = FALSE)
```
<!-- END: FIGURE -->

In the center of the screen, a black cube was displayed, which could be moved by the experimenter.
Participant read a sentence prompt out loud (*Du sollst den Würfel auf der COLOR OBJECT ablegen* 'You have to put the cube on top of the COLOR OBJECT') to instruct the experimenter to drag the cube on top of one of the four depicted objects (the *competitor*) using the mouse.
After the experimenter had moved the cube as instructed, the subject would read another sentence prompt (*Und jetzt sollst du den Würfel auf der COLOR OBJECT ablegen* 'And now, you have to put the cube on top of the COLOR OBJECT') instructing the experimenter to move the cube on top of a different object (the *target*).
The second utterance in the trial was the critical trial for analysis.

The two sentence prompts were used to create a focus contrast between the competitor and the target object.
Focused units denote the set of all (contextually relevant) alternatives [e.g., @rooth1992theory].
Concretely, a focus contrast marks one or more elements in a sentence as prominent, by different linguistic means depending on the language [@matic2013meanings; @BURDIN2015254].
For instance, if the competitor and target objects differ but their color does not (e.g. *yellow banana* vs. *yellow tomato*), the noun is said to be in focus (Noun Focus condition, NF).
If the objects are the same but differ in color (e.g. *yellow banana* vs. *blue banana*), the color adjective is in focus (Adjective Focus condition, AF).
If both the color and the object differ (e.g. *yellow banana* vs. *blue tomato*), then the whole noun phrase is in focus (Adjective/Noun Focus condition, ANF).
The NF condition constituted the experimentally relevant condition, while the AF and ANF conditions acted as fillers.
Crucially, the color-object combinations in the Noun Focus (NF) condition were manipulated with respect to their typicality.
The combinations were either typical (e.g. *orange mandarin*), medium typical (e.g. *green tomato*), or atypical (e.g. *yellow cherry*), as established by a norming study that was conducted prior to the production experiment just described.^[A detailed description of the norming and production studies from the *Prosodic encoding of redundant referring expressions* project, which was given to the analysts with the data set, can be found in `methods_norm_prod.pdf` at <https://bit.ly/3Ahawc7>.]
Each subject produced 15 critical trials (NF condition).
Each trial was repeated twice, yielding a total of 30 trials per participant and a grand-total of 900 ($15 \times 2 \times 30$ participants) spoken utterances.

For the present study, 46 analysis teams have received access to the entire data set generated by the production study.
The data set is constituted by audio recordings and annotation files in a format that is typical for the field.
The teams were instructed to answer the following research question, using the provided data set: *Do speakers acoustically modify utterances to signal atypical word combinations?*

# Methods

As outlined in Section [Operationalizing speech](#s:operspeech), researchers are faced with a large number of analytic choices when analyzing a multidimensional signal such as speech.
Analysts must identify and operationalize relevant measurements, as well as the temporal domain(s) from which these measurements are to be taken, and then possibly transform said measurements before submitting them to statistical models, which must be chosen alongside inferential criteria.
The complexity of speech data constitutes the ideal testing ground to assess the upper bound of analytic flexibility that social science might face across disciplines.
We employed a meta-analytic approach to assess (i) the variability of the reported effects, and (ii) how analytic and researcher-related predictors affect the final results.

In this study, we followed the procedures proposed by @Parker2020 and @aczel2021.
The project comprised the following five phases:

1. \textsc{Recruitment}: We recruited independent groups of researchers to analyze the data and review others' data analyses.
2. \textsc{Team Analysis}: We gave researchers access to the speech corpus and let them analyze the data as they saw fit.
3. \textsc{Review}: We asked reviewers to generate peer-review ratings of the analyses based on methods (not results).
4. \textsc{Meta-analysis}: We evaluated variability among the different analyses and how different predictors affected the outcomes.
5. \textsc{Write-up}: We collaboratively produced the final manuscript.

We initially estimated that this process, from the time of an in-principle acceptance of the Stage 1 Registered Report to the end of Phase 5, would take nine months.
<!-- The factor most likely to delay our time-line is the rate of completion of the original set of analyses by independent analysis teams. -->
Phase 4 (meta-analysis) took longer than initially anticipated and the total duration of the project was approximately 12 months.

The project OSF repository contains all the materials mentioned in this paper and can be accessed at <https://osf.io/3bmcp/>.
The repository holds three main OSF components (`Data`, `Teams analyses` and `Questionnaires`), and a link to the project's GitHub repository.
The following sections report the criteria for sample size, data exclusions, data manipulations, and all the measures in the study.

## Phase 1: Recruitment of analysts and initial survey

An online landing page provided a general description of the project, including a short pre-recorded slide-show that summarizes the data set and research question (<https://many-speech-analyses.github.io>).
The project was advertised via social media, using mailing lists for linguistic and psychological societies, and via word of mouth. 
Social media advertising was accompanied by a short recruitment form (`recruitment_form.pdf`).
The target population comprised active speech science researchers with a graduate/doctoral degree (or currently studying for a graduate/doctoral degree) in relevant disciplines. 
All individuals interested in participating were asked to complete a questionnaire detailing their familiarity with numerous analytic approaches common in the speech sciences (`analytic_approach_quest.pdf`).
Researchers could choose to work independently or in small teams.
For the sake of simplicity, we will refer both to a single researcher and teams as \textsc{analysis teams}.^[Terms in small caps in this and later sections are included with their definition in the glossary at the end of the paper for the reader's convenience.]
Recruitment for this project commenced after having received in-principle acceptance.

As outlined above, our primary aim is to assess the variability of the reported effects, rather than the meta-analytic estimate of the investigated effect *per se*.
To estimate the degree of uncertainty around effect variability as driven by number of teams, we ran a series of sample size simulations with values of variability extracted from @silberzahn2018many.
The code is available at <https://many-speech-analyses.github.io/many_analyses/scripts/r/simulations/simulations>, Section 2.^[Cached model outputs can be found at <https://osf.io/wds2m/>.]
Variability among teams was operationalized as the standard deviation of the teams' reported effects from @silberzahn2018many (which we *z*-scored prior to simulations to make it comparable to our study).
For the mean of the teams' true standard deviation (0.68 *z*-score), the simulation indicates that the degree of uncertainty around the estimated teams' standard deviation will be below 1 SD at any sample size greater than 10 teams. 
Thus in order to achieve our main goal, i.e. estimating variability among teams, we considered a minimum sample size of 10 teams as sufficient.
Given the exploratory nature of our study, however, we have sampled as many analysts as possible.
We received initial expressions of interest to participate from more than 200 analysts, though there was a substantial drop-out rate (see Section Results).

After submitting their analyses, we asked the analysts to also function as peer-reviewers.
Each team had to review four other analyses.
All analysts involved share co-authorship on this manuscript and participated in the collaborative process of producing the final manuscript.
Informed consent was obtained as part of the intake form.


## Phase 2: Primary Data Analyses

The analysis teams registered for participation and each of the analysts individually answered a demographic and expertise questionnaire (`intake_form.pdf`).
A PDF version of this and all other questionnaires are available in the repository's `Questionnaires` component, at <https://osf.io/h6z8w/>.
The questionnaire collected information on the analysts' current position and self-estimated breadth and level of statistical expertise and acoustic analysis skills.
We then requested that they answer the research question: *Do speakers acoustically modify utterances to signal atypical word combinations?*
To do so, they were given the data generated by the experiment described in Section [The data set](#s:dataset).
Data included the audio recordings with corresponding time-aligned transcriptions in the form of Praat TextGrid files.
These can be found in the `Data` component at <https://osf.io/5agn9/>.

Once their analysis was complete, they answered a structured questionnaire (`analytic_quest.pdf`), providing information about their analysis technique, an explanation of their analytic choices, their quantitative results, and a statement describing their conclusions.
They also uploaded their analysis files (including the additionally derived data and text files that were used to extract and pre-process the acoustic data), their analysis code (if applicable), and a detailed journal-ready analysis section.

## Phase 3: Peer Review of Analyses

The analyses from each team were evaluated by four different teams who functioned as peer-reviewers.
Each peer-reviewer was randomly assigned to analyses from at least four analysis teams.
Reviewers evaluated the methods of each of their assigned analyses one at a time in a sequence determined by the initiating authors.
The sequences were systematically assigned so that, if possible, each analysis is allocated to each position in the sequence for at least one reviewer.

The process for a single reviewer was as follows.
First, the reviewer received a description of the methods of a single analysis.
This included the narrative methods and results sections, the analysis team's answers to the questionnaire regarding their methods, including analysis code and the data set.
The reviewer was then asked in an online questionnaire (`peer_review_quest.pdf`) to rate both the acoustic and the statistical analyses and to provide an overall rating, using a scale of 0-100, respectively.
To help reviewers calibrate their rating, they were given the following guidelines:

* 100. A perfect analysis with no conceivable improvements from the reviewer.
* 75. An imperfect analysis but the needed changes are unlikely to dramatically alter the final interpretation.
* 50. A flawed analysis likely to produce either an unreliable estimate of the relationship or an over-precise estimate of uncertainty.
* 25. A flawed analysis likely to produce an unreliable estimate of the relationship and an over-precise estimate of uncertainty.
* 0. A dangerously misleading analysis, certain to produce both an estimate that is wrong and a substantially over-precise estimate of uncertainty that places undue confidence in the incorrect estimate.

\noindent The reviewers were also given the option to include further comments in a text box for each of the three ratings.

After submitting the review, a methods section from a second analysis was made available to the reviewer.
This same sequence was followed until all analyses allocated to a given reviewer were provided and reviewed.^[Initially we planned to present simultaneously all four (or more) methods sections to each reviewer after the fourth round, with the option to revise their original ratings and provide an explanation. Ultimately, we decided to skip this step due to time constraints.]


## Phase 4: Evaluating variation

The initiating authors (SC, JC, TR) conducted the analyses outlined in this section.
We did not conduct confirmatory tests of any *a priori* hypotheses.
We consider our analyses exploratory.

### Descriptive statistics

We calculated summary statistics describing variation among analyses, including (a) the nature and number of acoustic measures (e.g. f0 or duration), (b) the operationalization and the temporal domain of measurement (e.g. mean of an interval or value at a specified point in time), (c) the nature and number of model parameters for both fixed and random effects (if applicable), (d) the nature and reasoning behind inferential assessments (e.g. dichotomous decision based on *p*-values, ordinal decision based on a Bayes factor), as well as the (e) mean, (f) standard deviation and (g) range of the standardized effect sizes (see the next section for the standardization procedure).
These summary statistics are reported in *Descriptive statistics* of the Results section.
<!-- Add crossref to final version: \@ref(descr-stats) --> 

### Meta-analytic estimation

We investigated the variability in \textsc{reported effect sizes} using Bayesian meta-analytic techniques.
As the measure of variability, we took the meta-analytic \textsc{group-level standard deviation} ($\sigma_{\alpha_{\text{t}}}$, see below), where each analysis team represents a group.
As we detail in the Results section below, we have also run further non-preregistered analyses.
For these we refer the reader to that section, while we only describe the preregistered analyses in the following paragraphs.

Based on the common practices currently in place within the field, we anticipated that researchers would use multilevel regression models, thus common measurements of effect size, such as Cohen's $d$, might have been inappropriate.
Furthermore, @aczel2021 suggest that directly asking analysts to report standardized effect sizes could bias the choice of analyses towards types that more straightforwardly return a standardized effect.
Since the variables used by the analysis teams might have substantially differed in their measurement scales (e.g, Hertz for frequency vs. milliseconds for duration) which was indeed the case, we have standardized all reported effects by refitting each \textsc{reported model} with centered and scaled continuous variables (*z*-scores, i.e. the observed values subtracted from the mean divided by the standard deviation) and sum-coded factor variables.
Each \textsc{standardized model} was fitted as a Bayesian regression model with Stan [@stan2021], RStan [@stan2020a], and brms [@burkner2017] in R [@R-base].
Model refitting also constituted a way of validating the reported analyses, a step recommended by @aczel2021.
Details about the refitting procedure can be found at <https://many-speech-analyses.github.io/many_analyses/scripts/r/04_refit_workflow>.

The coefficients of the critical predictors (i.e. critical according to the analysis teams' self-reported inferential criteria) obtained from the standardized models were used as the \textsc{standardized effect size} ($\eta_i$) of each reported model.
Moreover, to account for the differing degree of uncertainty around each standardized effect size, we used the standard deviation of each standardized effect size as the \textsc{standardized standard error} ($\text{se}_i$).
This enabled us to fit a so-called "measurement-error" model, in which both the standardized effect sizes and their respective standard errors are entered in the meta-analytic model.
As a desired consequence, effect sizes with a greater standard error are weighted less than those with a smaller standard error in the meta-analytic calculations.

After having obtained the standardized effect sizes $\eta_i$ with related standard errors $\text{se}_i$, for each critical predictor in each reported model, we conducted a \textsc{Bayesian random-effects meta-analysis} using a multi-level (intercept-only) regression model.
The outcome variable was the set of standardized effect sizes $\eta_i$.
The likelihood of $\eta_i$ was assumed to correspond to a normal distribution [@knight2000].
The analysis teams were entered as a group-level effect (i.e., `(1 | team)`, called *random effect* in the frequentist literature).
The standard errors $\text{se}_i$ were included as the standard deviation of $\eta_i$ to fit a measurement-error model, as discussed above.
We used regularizing weakly-informative priors for the intercept $\alpha$ ($Normal(0, 1)$) and for the group-level standard deviation $\sigma_{\alpha_{\text{t}}}$ ($HalfCauchy(0, 1)$).
We fit this model with 4 chains of Hamiltonian Monte-Carlo sampling for the estimation of the joint posterior distribution, using the No U-Turn Sampler (NUTS) as implemented in Stan [@stan2021], and 4000 iterations (2000 for warm-up) per chain, distributed across 8 processing cores and 2 threads in within-chain parallelization.
<!-- In case of divergent transitions, we will increase `adapt_delta`, `tree_depth`, and the number of iterations in this order until we obtain a model fit with no divergent transitions. -->
The model did not incur any divergent transitions ($\hat{R}$ was not greater than 1) and the estimated sample sizes were sufficient.
The code used to run the model can be found at <https://many-speech-analyses.github.io/many_analyses/scripts/r/06_meta-analysis_prereg>.

<!-- From ?brms:brmsformula -->
<!-- For families gaussian, student and skew_normal, it is possible to
specify standard errors of the observations, thus allowing to perform
meta-analysis. Suppose that the variable yi contains the effect sizes
from the studies and sei the corresponding standard errors. Then,
fixed and random effects meta-analyses can be conducted using the
formulas yi | se(sei) ~ 1 and yi | se(sei) ~ 1 + (1|study),
respectively, where study is a variable uniquely identifying every
study. If desired, meta-regression can be performed via yi | se(sei) ~
1 + mod1 + mod2 + (1|study) or  yi | se(sei) ~ 1 + mod1 + mod2 + (1 +
mod1 + mod2|study), where mod1 and mod2 represent moderator variables.
By default, the standard errors replace the parameter sigma. To model
sigma in addition to the known standard errors, set argument sigma in
function se to TRUE, for instance, yi | se(sei, sigma = TRUE) ~ 1. -->

The posterior distribution of the population-level intercept $\alpha$ allowed us to estimate the range of probable values of the standardized effect size $\hat{\eta}$.
The posterior distribution further allowed us to investigate the effect of a set of analytic and researcher-related predictors, detailed in the next section.
Crucially, the posterior distribution of the [group-level standard deviation](.smallcaps) $\sigma_{\alpha_{\text{t}}}$ (i.e. the standard deviation of the group-level effect of team) allowed us to quantify the degree of variation between the teams' analyses on a standardized scale.

### Analytic and researcher-related predictors affecting effect sizes {#anares-preds}

As a second step, we investigated the extent to which the individual standardized effect sizes are affected by a series of \textsc{analytic and researcher-related predictors}.

**Analytic predictors**. We estimated the influence of the following predictors related to the analytic characteristics of each team's reported analysis:

<!-- NOTE: We dropped data exclusion -->
- *Measure of uniqueness* of individual analyses for the set of predictors in each model [numeric].
<!-- - *Measure of conservativeness* of the model specification, as the number of random/group-level effects included [numeric]. -->
<!-- - *Number of post-hoc changes to the acoustic measurements* the teams will report to have carried out [numeric]. -->
- *Number of models* the teams reported to have run [numeric].
- *Major dimension* that has been measured to answer the research question [categorical].
- *Temporal window* that the measurement is taken over [categorical].
- *Average peer-review rating*, as the mean of the overall peer-review ratings for each analysis [numeric].

Following @Parker2020, the measure of uniqueness of predictors was assessed by the Sørensen-Dice Index [SDI, @dice1945; @sorensen1948].
The SDI is an index typically used in ecology research to compare species composition across sites. 
It is a distance measure similar to Euclidean distance measures, but is more sensitive to more heterogeneous data sets and deemphesizes outliers.
For our purposes, we treated predictors as *species* and individual analyses as *sites*.
For each pair of analyses $(X, Y)$ (across and within teams), the SDI was obtained using the following formula:

$$\text{SDI} = \frac{2|X \cap Y|}{|X|+|Y|}$$
\vspace{0.05in}

```{r sdi, eval=FALSE, include=FALSE}
x <- c("a", "b", "c", "d")
y <- c("b", "d", "e", "f")
(2 * length(intersect(x, y))) / (length(x) + length(y))
```

\noindent where $|X \cap Y|$ is the number of variables common to both models in the pair, and $|X|+|Y|$ is the sum of the number of variables that occur in each model. 
For example, if two pairs of models differ in either only one predictor (e.g. DV ~ typicality vs. DV ~ typicality + trial) or in two predictors (e.g. DV ~ typicality vs. DV ~ typicality + trial + speech rate), the latter model pair would exhibit a larger SDI than the former.
In order to generate a unique SDI for each analysis team, we calculated the average of all pairwise SDIs for all pairs of analyses using the `beta.pair()` function in the betapart R package [@baselga2020].

The major measurement dimension of each analysis was categorized according to the following possible groups: *duration*, *intensity*, *f0*, *other spectral properties* (e.g. frequency, center of gravity, harmonics difference, etc.), and *other measures* (e.g. derived measures such principal components, vowel dispersion, etc.).
The temporal window that the measurement is taken over is defined by the target linguistic unit.
We assume the following relevant linguistic units: *segment*, *syllable*, *word*, *phrase*, *sentence*.
Since each analysis received more than one peer-review rating, we calculated the mean rating and its standard deviation for each.
These were entered in the model formula as a measurement-error term (`me(mean, sd)` in brms).

**Researcher-related factors**. We also included the following predictors:

- *Research experience* as the elapsed time from receiving the PhD. Negative values will indicate that the person is a student or graduate student [numeric].
- *Initial belief* in the presence of an effect of atypical noun-adjective pairs on acoustics, as answered during the intake questionnaire [numeric].

To obtain an aggregated research experience score and initial belief score for each team based on the members' individual scores, we calculated the mean and standard deviation of these predictors for each team.
These were entered in the model formula as a measurement-error term (`me(mean, sd)` in brms).
The expedient of using a measurement-error term (which includes the teams' standard deviation) ensures information about within-team variance is not lost (which would be the case if including the mean only).

We had initially planned to also include a measure of conservativeness of the model specification, as the number of random/group-level effects included and the number of post-hoc changes to the acoustic measurements the teams reported to have carried out. 
When fitting the model, we realized that the measure of conservativeness is related to the standard error of the estimates (i.e. more group-level effects = higher standard error).
Moreover, there was no team that declared to have made post-hoc changes to the analyses, this we decided against including these two preregistered predictors in the model.  

**Model specification**. The model was fitted as a measurement-error model, with the predictors detailed in the preceding paragraphs.
The outcome variables of the model were the standardized effect sizes and related standard deviation.

A normal distribution was used as the likelihood function of $\alpha_{t[i]}$.
The mean of $\alpha_{t[i]}$ was modeled on the basis of the overall intercept $\beta$ and on the coefficients of each predictor.
The numeric predictors were centered and scaled and the categorical predictors were sum coded.
We used a normal distribution with mean 0 and standard deviation 1 as the prior for the intercept and the predictors.
The model was run with the same settings as with the meta-analytic model.
The code used to run the model can be found at <https://many-speech-analyses.github.io/many_analyses/scripts/r/06_meta-analysis_prereg>.

### Data management {#ana-archive}

All relevant data, code, and materials have been publicly archived on the Open Science Framework (<https://osf.io/3bmcp/>).
Archived data include the original data set distributed to all analysts, any edited versions of the data analyzed by individual teams, and the data we analyzed with our meta-analyses, which include the standardized effect sizes, the statistics describing variation in model structure among analysis teams, and the anonymized answers to our questionnaires of analysts.
Similarly, we archived both the analysis code used for each individual analysis and the code from our meta-analyses.
We also archived copies of our survey instruments from analysts and peer-reviewers.

We excluded from our synthesis any individual analysis submitted after peer review (Phase 3) or those unaccompanied by analysis files without which it was not possible to follow the research protocol.
We also excluded any individual analysis that does not produce an outcome that could be interpreted as an answer to our primary question.
For a list of exlcusion criteria, see Section Descriptive statistics below.

## Phase 5: Collaborative Write-Up of Manuscript

The initiating authors discussed the limitations, results, and implications of the study and collaborated with the analysts on writing the final manuscript for review as a stage-2 Registered Report.^[The comment history can be found at <https://docs.google.com/document/d/1CFgRo93mRgifpuFOuQE3vNBeMW-H7ps9eD--vxH-6CQ/edit?usp=sharing>.]

# Results

```{r load-models, message=FALSE, warning=FALSE}
msa_models <- readRDS(here("./data/analyses/msa_models.rds")) %>%
  mutate(
    outcome = as.factor(outcome),
    typicality = as.factor(typicality),
    temporal_window = as.factor(temporal_window)
  ) %>%
  droplevels() %>%
  mutate(
    outcome = contr_code_sum(outcome),
    typicality = contr_code_sum(typicality),
    temporal_window = contr_code_sum(temporal_window)
  )

team_sheet <- "https://docs.google.com/spreadsheets/d/1w39rU7O5ijzPmCpClVt5LEDIhzaLBDNrwfdMJUP86q8/edit?usp=sharing"
teams <- read_sheet(team_sheet) 

# Get DF of all members with leader names
team_leaders <- readr::read_csv(
  here::here("data", "questionnaires", "msa_intake_form.csv")) %>% 
  select(ResponseId, Q6) %>% # Questions asking for team leader
  filter(!(row_number() %in% c(1, 2))) %>% 
  mutate(leader = Q6, 
         leader = case_when(
           Q6 %in% c("Ali A-Hoorie", "Ali Al-Hoorie", "Ali H. Al-Hoorie") ~ "Ali H. Al-Hoorie", 
           Q6 %in% c("Alicea Megan Brown", "Alicia Brown") ~ "Alicia M Brown", 
           Q6 %in% c("Dr Rory Turnbull", "Rory Turnbull") ~ "Rory Turnbull", 
           Q6 %in% c("Dr. Reenu Punnoose", "Reenu Punnoose") ~ "Reenu Punnoose", 
           Q6 %in% c("Dušan Nickolić", "Dušan Nikolić", "Dusan Nikolic", "Dusan Nikolic (dusan.nikolic1@ucalgary.ca)") ~ "Dušan Nikolić", 
           Q6 %in% c("Michael Gradoville (Arizona State Univ.)", "Michael Gradoville") ~ "Michael Gradoville", 
           Q6 %in% c("Nicole Rodriquez", "Nicole Rodriguez") ~ "Nicole Rodriguez", 
           Q6 %in% c("Scott James Perru", "Scott Perry") ~ "Scott James Perry", 
           Q6 %in% c("Tiphaine Caudrelier", "Tiphaine Cordelier") ~ "Tiphaine Caudrelier", 
           Q6 %in% c("University of Birmingham team under the team coordinator Zlatomira IIchovska", "Zlatomira IIchovska", "Zlatomira Ilchovska") ~ "Zlatomira Ilchovska", 
           Q6 %in% c("Vincent Hughes", "Vince Hughes") ~ "Vince Hughes", 
           Q6 == "Joey Stanley" ~ "Joseph A. Stanley",
           TRUE ~ leader
         )) %>% 
  select(ResponseId, leader) 

# Teams on OSF
teams_on_osf <- teams %>% 
  filter(added_on_osf == "yes") %>% 
  pull(coordinator)

# Teams with submitted analyses
teams_used_in_analysis <- teams %>% 
  filter(animal %in% msa_models$animal) %>% 
  pull(coordinator)

# Team descriptives
#  - number of teams (initially)
#  - Avg size of each team
team_desc_df <- bind_rows(
  team_leaders %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              n_analysts = sum(n_members), 
              median_size = median(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members), 
              min = min(n_members), 
              max = max(n_members)) %>% 
    mutate(stage = "Initial"), 
  team_leaders %>% 
    filter(leader %in% teams_on_osf) %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              n_analysts = sum(n_members), 
              median_size = median(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members), 
              min = min(n_members), 
              max = max(n_members)) %>% 
    mutate(stage = "Submitted"), 
  team_leaders %>% 
    filter(leader %in% teams_used_in_analysis) %>% 
    group_by(leader) %>% 
    mutate(member_num = seq_along(ResponseId)) %>% 
    group_by(leader) %>% 
    mutate(n_members = max(member_num)) %>% 
    distinct(leader, n_members) %>% 
    ungroup() %>% 
    summarize(n_teams = nrow(.), 
              n_analysts = sum(n_members), 
              median_size = median(n_members), 
              sd_size = sd(n_members), 
              median_size = median(n_members), 
              min = min(n_members), 
              max = max(n_members)) %>% 
    mutate(stage = "Final")
  ) %>% 
  mutate_if(is.numeric, round, digits = 1)

# number of analyses
msa_models_unique <- msa_models %>%
  select(animal, model_n, outcome, typicality_operationalization, framework, 
         model, temporal_window, random_effects, n_random, phon_rating, 
         stat_rating, n_predictors, pop_pred, operationalisation, 
         operationalisation_notes, n_terms, n_coefficients, n_random_int, 
         n_random_slo, n_random_smooth, n_random, n_models) %>%
  distinct()

no_analyses <- msa_models_unique %>%
  nrow()

# average analyses
mean_analyses <- msa_models_unique %>% 
  group_by(animal) %>% 
  summarise(max_n_models = max(model_n)) %>% 
  summarise(
    mean_models = mean(max_n_models), 
    sd_models = sd(max_n_models), 
    min_models = min(max_n_models), 
    max_models = max(max_n_models), 
    median_models = median(max_n_models), 
    n_models = sum(max_n_models)
  )

# outcomes
outcome_n <- msa_models_unique %>% 
  count(outcome) %>% 
  summarise(prop = n / sum(n))

# windows
window_n <- msa_models_unique %>% 
  count(temporal_window) %>% 
  summarise(prop = n / sum(n))

# predictors
predictor_n <- msa_models_unique %>% 
  summarise(
    mean_predictor = mean(n_predictors, na.rm = TRUE), 
    sd_predictor = sd(n_predictors, na.rm = TRUE), 
    min_predictor = min(n_predictors, na.rm = TRUE), 
    max_predictor = max(n_predictors, na.rm = TRUE), 
    median_predictor = median(n_predictors, na.rm = TRUE), 
    n_predictor = sum(n_predictors, na.rm = TRUE)
    ) 

# typicality op
typicality_n <- msa_models_unique %>% 
  count(typicality_operationalization) %>% 
  summarise(prop = n / sum(n))

# framework
framework_n <- msa_models_unique %>% 
  count(framework) %>% 
  summarise(prop = n / sum(n))

# model
model_n <- msa_models_unique %>% 
  count(model) %>% 
  summarise(prop = n / sum(n))

# random
random_n <- msa_models_unique %>% 
  filter(model == "linear model") %>% 
  count(random_effects) %>% 
  summarise(prop = n / sum(n))

# random terms
rterms_n <- msa_models_unique %>% 
  filter(n_random != 0) %>% 
  summarise(
    mean_rterms = mean(n_random, na.rm = TRUE), 
    sd_rterms = sd(n_random, na.rm = TRUE), 
    min_rterms = min(n_random, na.rm = TRUE), 
    max_rterms = max(n_random, na.rm = TRUE), 
    median_rterms = median(n_random, na.rm = TRUE), 
    n_rterms = sum(n_random, na.rm = TRUE)
    ) 

# random intercepts
rterms_int_n <- msa_models_unique %>% 
  filter(n_random != 0) %>% 
  summarise(
    mean_rint = mean(n_random_int, na.rm = TRUE), 
    sd_rint = sd(n_random_int, na.rm = TRUE), 
    min_rint = min(n_random_int, na.rm = TRUE), 
    max_rint = max(n_random_int, na.rm = TRUE), 
    median_rint = median(n_random_int, na.rm = TRUE), 
    n_rint = sum(n_random_int, na.rm = TRUE)
    ) 

# random slopes
rterms_slope_n <- msa_models_unique %>% 
  filter(n_random != 0) %>% 
  summarise(
    mean_rslope = mean(n_random_slo, na.rm = TRUE), 
    sd_rslope = sd(n_random_slo, na.rm = TRUE), 
    min_rslope = min(n_random_slo, na.rm = TRUE), 
    max_rslope = max(n_random_slo, na.rm = TRUE), 
    median_rslope = median(n_random_slo, na.rm = TRUE), 
    n_rslope = sum(n_random_slo, na.rm = TRUE)
    ) 

# unique model specs, NOT including random effects
unique_model_specs <- msa_models_unique %>%
  select(framework:outcome, pop_pred) %>%
  unnest(pop_pred) %>%
  distinct()

# unique acoustic and model specs, NOT including random effects
unique_acou_stat <- msa_models %>%
  select(model_n, outcome, typicality_operationalization, framework, model, 
         temporal_window, random_effects, n_random, phon_rating, stat_rating, 
         n_predictors, pop_pred, operationalisation, operationalisation_notes) %>%
  unnest(pop_pred) %>%
  distinct()

# review ratings
review_mean <- msa_models %>% 
  select(animal, phon_rating, stat_rating) %>% 
  distinct() %>% 
  summarise(
    mean_prating = mean(phon_rating, na.rm = TRUE),
    sd_prating = sd(phon_rating, na.rm = TRUE), 
    min_prating = min(phon_rating, na.rm = TRUE),
    max_prating = max(phon_rating, na.rm = TRUE),
    median_prating = median(phon_rating, na.rm = TRUE),
    n_prating = sum(phon_rating, na.rm = TRUE),
    mean_srating = mean(stat_rating, na.rm = TRUE),
    sd_srating = sd(stat_rating, na.rm = TRUE), 
    min_srating = min(stat_rating, na.rm = TRUE), 
    max_srating = max(stat_rating, na.rm = TRUE), 
    median_srating = median(stat_rating, na.rm = TRUE), 
    n_srating = sum(stat_rating, na.rm = TRUE)
    )

# found-effect
found_effect <- msa_models %>% 
  select(animal, model_n, found_effect) %>% 
  na.omit() %>% 
  mutate(effect = if_else(found_effect == "yes", 1, 0))

atleast_1_effect <- found_effect %>% 
  group_by(animal) %>% 
  summarize(total = sum(effect)) %>% 
  filter(total != 0)

# Overall N of analyses claiming an effect
n_effects <- sum(found_effect$effect)
# Percent of effects
perc_effects <- (n_effects / nrow(msa_models) * 100) %>% round(., 2)
# N of teams claiming an effect
n_atleast_1 <- nrow(atleast_1_effect)
# Percent of teams claiming an effect
perc_atleast_1 <- (n_atleast_1 / length(unique(msa_models$animal))) * 100



#
# TABLE PREP
#

# Team descriptives
tbl_team_desc <- msa_models %>% 
  select(animal, years_from_phd, prior_belief, phon_rating, stat_rating, 
         all_rating) %>% 
  group_by(animal) %>% 
  summarize(across(years_from_phd:all_rating, median)) %>% 
  pivot_longer(cols = -animal, names_to = "metric", values_to = "vals") %>% 
  na.omit() %>% 
  group_by(metric) %>% 
  summarize(min = min(vals), max = max(vals), Median = median(vals, na.rm = T), 
            sd = sd(vals, na.rm = T)) %>% 
  add_row(.before = 1, 
    metric = "size", 
    min = team_desc_df %>% filter(stage == "Final") %>% pull(min), 
    max = team_desc_df %>% filter(stage == "Final") %>% pull(max), 
    Median = team_desc_df %>% filter(stage == "Final") %>% pull(median_size), 
    sd =  team_desc_df %>% filter(stage == "Final") %>% pull(sd_size)) %>% 
  mutate(across(-metric, specify_decimal, k = 1)) %>% 
  mutate(desc = "team_desc", 
    Range = glue::glue("{min} -- {max}"), 
    Metric = case_when(
      metric == "size" ~ "Team size", 
      metric == "years_from_phd" ~ "Years after PhD", 
      metric == "stat_rating" ~ "Statistical analysis peer rating", 
      metric == "prior_belief" ~ "Prior belief", 
      metric == "phon_rating" ~ "Acoustic analysis peer rating", 
      metric == "all_rating" ~ "Overall peer rating"), 
    label = glue::glue("{desc}_{metric}")) %>% 
  select(label, desc, Metric, Min. = min, Max = max, Median, Range)

#
# Descriptives of categorical variables in acoustic analyses
#
tbl_categorical_desc <- msa_models_unique %>% 
  select(animal, outcome, temporal_window, typicality_operationalization, 
  framework, model) %>% 
  pivot_longer(cols = -animal, names_to = "metric", values_to = "val") %>% 
  na.omit() %>% 
  group_by(metric) %>% 
  count(val) %>% 
  group_by(metric) %>% 
  mutate(prop = n / sum(n), perc = prop * 100, 
    Metric = case_when(
      metric == "temporal_window" ~ "Temporal window", 
      metric == "typicality_operationalization" ~ "Typicality operationalization", 
      TRUE ~ metric
    ), 
    Metric = str_to_sentence(Metric), 
    Type = case_when(
      val != "GAM" ~ str_to_sentence(val), 
      TRUE ~ val
    )) %>% 
  mutate(across(c("prop", "perc"), specify_decimal, k = 0)) %>% 
  unite(label, c("metric", "val")) %>% 
  mutate(label = str_replace_all(label, " ", "_"), 
         label = str_remove_all(label, "\\(|\\)"))

#
# Model descriptives 
#

tbl_model_desc <- bind_rows(
  mean_analyses %>% 
  pivot_longer(cols = everything(), names_to = "label", values_to = "vals"), 
  predictor_n %>% 
  pivot_longer(cols = everything(), names_to = "label", values_to = "vals"), 
  rterms_n %>% 
  pivot_longer(cols = everything(), names_to = "label", values_to = "vals"), 
  rterms_int_n %>% 
  pivot_longer(cols = everything(), names_to = "label", values_to = "vals"), 
  rterms_slope_n %>% 
  pivot_longer(cols = everything(), names_to = "label", values_to = "vals")
) %>% 
  separate(label, c("Metric", "item"), sep = "_") %>% 
  pivot_wider(names_from = Metric, values_from = vals) %>% 
  mutate(across(c("mean", "sd"), specify_decimal, k = 0)) %>% 
  mutate(
    Metric = item, 
    Metric = case_when(
      Metric == "models" ~ "Models", 
      Metric == "predictor" ~ "Predictors", 
      Metric == "rterms" ~ "Random terms", 
      Metric == "rint" ~ "Intercept", 
      Metric == "rslope" ~ "Slope"
    ), 
    desc = "model_desc", 
    label = glue::glue("{desc}_{item}"), 
    Median = glue::glue("{median}"), 
    Range = glue::glue("{min} -- {max}")) %>% 
  select(label, desc, Metric, Min. = min, Max = max, Median, avg = mean, 
         sd, Range) %>% 
  mutate(across(-c("desc", "Metric"), as.character))

```

The results section is divided into three parts.
We first provide a statistical description of team composition, nature of acoustic analyses and statistical approaches, and peer-review ratings.
Second, we report the results of the meta-analytic model, focusing on between-team and between-model variability. 
<!-- and the estimated meta-analytic effect of word combination typicality on speech acoustics.-->
Finally, we present the analysis of the effect of analytic and researcher-related predictors on the meta-analytic effect.
The research compendium of the study, containing all the code and data presented here, can be found in the GitHub repository linked in the research compendium at <https://osf.io/3bmcp/>, in the `scripts/r/` folder.

## Descriptive statistics {#descr-stats}

In the following sections, we will describe the characteristics of the analysis teams that participated in the study and the analytic approaches they adopted.
An important aspect that emerges from the descriptive analysis is the large variation in analytic strategies. 

### Characteristics of analysis teams

`r pull_from_tib(team_desc_df, stage, "Initial", n_teams) %>% xfun::numbers_to_words() %>% stringr::str_to_sentence()` teams initially signed up to participate in the study, comprising `r pull_from_tib(team_desc_df, stage, "Initial", n_analysts)` analysts.
`r (pull_from_tib(team_desc_df, stage, "Initial", n_teams) - pull_from_tib(team_desc_df, stage, "Submitted", n_teams)) %>% xfun::numbers_to_words() %>% stringr::str_to_sentence()` of the signed-up teams dropped out during the analysis phase.

`r pull_from_tib(team_desc_df, stage, "Submitted", n_teams) %>% xfun::numbers_to_words() %>% stringr::str_to_sentence()` teams submitted their analyses by the established deadline. 
Only analyses from which it was possible to extract an effect size were included in the meta-analysis. 
Of the analysis submitted by the `r pull_from_tib(team_desc_df, stage, "Submitted", n_teams)` teams, the initiating authors identified analyses from `r pull_from_tib(team_desc_df, stage, "Final", n_teams)` teams to be eligible to be included in the meta-analytic model.
Reasons for exclusion were: use of Generalized Additive Models (4 teams) which do not lend themselves easily to the meta-analytic methods employed in this study, use of machine learning techniques (3 teams), use of typicality as the outcome variable/response (3 teams), or use of other methods that returned statistics that could not be included in the meta-analytic model.

In what follows, we describe the characteristics of those teams whose analyses were included in the meta-analytic model.
A complete summary of all the analyses from the `r pull_from_tib(team_desc_df, stage, "Submitted", n_teams)` submitting teams is available in the supplementary materials at <https://many-speech-analyses.github.io/many_analyses/RR_manuscript/supplementary_materials.pdf>.

The included analyses were provided by `r pull_from_tib(team_desc_df, stage, "Final", n_teams)` teams, comprising `r pull_from_tib(team_desc_df, stage, "Final", n_analysts)` analysts, with a median of `r pull_from_tib(tbl_team_desc, label, "team_desc_size", Median)` individuals per team.
Upon sign-up, we collected background information from each analyst through the intake form, which was administered during Phase 1, prior to the data being released to the teams.
Analysts had a median of `r pull_from_tib(tbl_team_desc, label, "team_desc_years_from_phd", Median)` years of experience after completing their PhD, ranging from `r pull_from_tib(tbl_team_desc, label, "team_desc_years_from_phd", Min.)` years, i.e. PhD students (or less experienced) to `r pull_from_tib(tbl_team_desc, label, "team_desc_years_from_phd", Max)` years, suggesting that, on average, analysts were experienced researchers.
The analysts' prior belief in the effect under investigation, on a scale from 0 to 100, ranged from `r pull_from_tib(tbl_team_desc, label, "team_desc_prior_belief", Min.)` to `r pull_from_tib(tbl_team_desc, label, "team_desc_prior_belief", Max)` with a median of `r pull_from_tib(tbl_team_desc, label, "team_desc_prior_belief", Median)`.
We take this to suggest that, overall, analysts had a rather high positive prior belief in the investigated relationship between acoustics and word combination typicality.

At the end of Phase 2 (primary data analysis), the teams had submitted a grand total of `r no_analyses` individual models (including `r nrow(msa_models)` critical model coefficients, given that some models returned more than one critical coefficient) to answer the research question, with a median of `r pull_from_tib(tbl_model_desc, label, "model_desc_models", Median)` models per team. 
Table \@ref(tab:descriptives-table) provides a summary of the contributing teams and their analyses. 

### Acoustic analysis

The analytic teams differed in their approach to the acoustic analysis of the speech signal, including choices related to specific acoustic measures, the temporal window used, and how the measures were transformed. 
`r (round(outcome_n[[2,1]],2) * 100) %>% xfun::numbers_to_words() %>% str_to_sentence()` percent of the models used f0 as the outcome variable, `r (round(outcome_n[[1,1]],2) * 100)`\% used a measure of duration, `r round(outcome_n[[3,1]],2) * 100`\% used vowel formants, `r round(outcome_n[[4,1]],2) * 100`\% intensity, and `r round(outcome_n[[5,1]],2) * 100`\% other measures.

```{r unique-acou-oper}
# acoustic operationalizations
# Gross operationalization of acoustic operationalization
unique_acou_oper <- msa_models_unique %>%
  unite("unique_oper", outcome, operationalisation, operationalisation_notes, na.rm = TRUE) %>%
  count(unique_oper)
```

`r (round(window_n[[1,1]],2) * 100) %>% xfun::numbers_to_words() %>% stringr::str_to_sentence()` percent of models used acoustic measures taken at the level of the segment (e.g. comparing the acoustic profile of a vowel), `r round(window_n[[2,1]],2) * 100`\% from the word level (e.g. comparing the acoustic profile of *Banane* 'banana'), `r round(window_n[[3,1]],2) * 100`\% at the level of the phrase (e.g. the noun phrase including determiner and adjective, e.g. "the green banana"), `r round(window_n[[4,1]],2) * 100`\% from the whole sentence, and `r round(window_n[[5,1]],2) * 100`\% used a different time window. 
Based on a coarse coding of how acoustic measures were operationalized, we find a total of `r nrow(unique_acou_oper)` different measurement specifications.
For example, if we consider those analyses that target f0, we find that it is operationalized in many different ways including the minimum, the maximum, the mean, the median, as a range in an interval or a ratio between two intervals.
The measurement is sometimes taken from the interval of a vowel in the article, the adjective or noun; it is sometimes taken from the word interval of the article, adjective or noun; or it is taken from either the noun phrase interval or the entire sentence. 
Some of these measures were normalized relative to other elements in the sentence or relative to the speaker.

### Statistical analysis

The large decision space related to how the acoustic signal was measured is further expanded by the choices in the statistical analysis, including the chosen inferential framework, the type of model, and the model specification, including choice of predictors, interactions and group-level effects.

The mean of the number of different predictors included in teams' models was `r round(predictor_n$mean_predictor,1)` (defined as variables or columns in the data table).
This means that, in addition to the critical predictor (typicality of the adjective noun combinations), models had on average one additional predictor (range = `r round(predictor_n$min_predictor,1)`-`r `round(predictor_n$max_predictor,1)`).
Possible information that was used as predictors included the information structure of the sentence, trial number, semantic dimensions of the referent, part of speech, and speaker gender.

The data given to the teams allowed them to operationalize the predictor of interest, word typicality, in different ways.
Among the possible operationalizations, `r round(typicality_n[[1,1]],2) * 100`\% of models contained typicality as a categorical variable (e.g. atypical vs. typical), `r round(typicality_n[[2,1]],2) * 100`\%  used a continuous typicality scale from 0-100 by calculating the mean typicality for each word combination as obtained from the norming study, while `r round(typicality_n[[3,1]],2) * 100`\% of the models used the median typicality rating.
Note that the design of the experiment alongside its description indicated that the experiment was designed to categorically operationalize typicality. This possibly explains analysts' strong preference. 

The majority of models were run within a frequentist framework (`r round(framework_n[[2,1]],2) * 100`\%).
`r (round(framework_n[[1,1]],2) * 100) %>% xfun::numbers_to_words() %>% str_to_sentence()` percent were run within a Bayesian framework.
While teams almost exclusively used linear models to analyze their data (`r round(model_n[[2,1]],2) * 100`\%), teams differed drastically in how they accounted for dependencies within the data.
<!-- TODO: The calculations in these sections are based on the INCLUDED models only. We are not counting the SUBMITTED models that have been excluded. We might want to add counts for when including the excluded models -->

The data contains several dependencies between data points, with multiple data points coming from the same subject and with multiple data points being associated with the same adjective or noun.
An appropriate way to account for this non-independence is by using models that include so-called random or group level effects [e.g., @gelman2006data; @schielzeth2009conclusions], variably known as mixed-effects, hierarchical, multi-level, or nested models (among other names).
`r (round(random_n[[3,1]],2) * 100) %>% xfun::numbers_to_words() %>% str_to_sentence()` percent of the linear models specified no random effects at all (without pooling their data), effectively ignoring these non-independences [@hurlbert1984pseudoreplication].
`r (round(random_n[[1,1]],2) * 100) %>% xfun::numbers_to_words() %>% str_to_sentence()` percent specified random intercepts only, and `r round(random_n[[2,1]],2) * 100`\% specified both random intercepts and random slopes to account for the non-independence.
On average, teams that specified random effects included `r round(rterms_n$mean_rterms,1)` random terms in their models. 
Based on statistical framework, type of model, distribution family, fixed terms, and not including random effects, there were a total of `r nrow(unique_model_specs)` different model specifications.
  
<!-- TR: I am still confused, so if we were to ignore different random effect specifications that the number is x? Can we find a clearer way to express this, e.g. "When considering choices related to only statistical framework, type of model, distribution family and fixed terms, there were..." --> 

<!-- Moved 'found-effect' code to beginning of results section --> 

When considering both acoustic and statistical analyses, we have found a grand total of `r nrow(unique_acou_stat)` different analytic pipelines.
In other words, each individual analysis submitted was unique.

Our quantitative assessment did not include other degrees of freedom, all of which are additional sources of variation: 
Teams differed with regard to how the acoustic signal was segmented ranging from fully automated forced-alignment with minimal manual correction to complete manual alignment performed by the analysts; teams differed in whether the statistical analysis was based on a subset of the data or the whole dataset; and they differed whether and if so how measurements were excluded based on both qualitative (i.e. whether specific speech production instances were excluded or not) and quantitative grounds (i.e. whether data were trimmed or not).

The question arises whether these unique analysis pipelines led to different conclusions. 
`r n_atleast_1 %>% xfun::numbers_to_words() %>% str_to_sentence()` teams out of the `r length(unique(msa_models$animal)) %>% xfun::numbers_to_words()` (`r perc_atleast_1`%) reported to have found at least one statistically reliable effect (based on the inferential criteria they specified).
Of the 104 submitted models, `r n_effects` were reported to show a statistically reliable effect (`r round(perc_effects,1)`%). 

<!-- Begin Table --> 
```{r}
#| label: descriptives-table
#| results: 'asis'
bind_rows(
  tbl_team_desc %>% 
    select(desc, Metric, val1 = Range, val2 = Median), 
  tbl_categorical_desc %>% 
    select(desc = Metric, Metric = Type, val1 = n, val2 = perc) %>% 
    mutate(val1 = glue::glue("{val1}")), 
  tbl_model_desc %>% 
    select(desc, Metric, val1 = Range, val2 = Median)
) %>% 
  mutate(order = c(
    1, 6, 4, 3, 5, 2, 
    21, 20, 23, 22, 24, 
    8, 7, 10, 9, 11, 
    16, 15, 12, 14, 13, 
    17, 18, 19, 
    25:29
    )
  ) %>% 
  arrange(order) %>% 
  select(-order) %>% 
  mutate(desc = str_replace_all(desc, "team_desc", " "), 
    desc = str_replace_all(desc, "model_desc", "N"), 
    desc = case_when(
      Metric != "Models" & desc == "N" ~ " ", 
      Metric != "Frequentist" & desc == "Framework" ~ " ", 
      Metric != "Linear model" & desc == "Model" ~ " ", 
      Metric != "F0" & desc == "Outcome" ~ " ", 
      Metric != "Segment" & desc == "Temporal window" ~ " ", 
      Metric != "Categorical" & desc == "Typicality operationalization" ~ " ", 
      TRUE ~ desc)
    ) %>% 
  filter(Metric != "n. random smooths") %>% 
  mutate(Metric = str_remove_all(Metric, "n. ")) %>% 
  rename("\\bf{Team characteristics}" = desc, "  " = Metric, `\\bf{Range}` = val1, 
    `\\bf{Median}` = val2) %>% 
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ", 
    `\\bf{Median}` = " ", .before = 7) %>%
  add_row(`\\bf{Team characteristics}` = "\\bf{Acoustic analyses}", `  ` = " ", 
    `\\bf{Range}` = "\\bf{n}", `\\bf{Median}` = "\\bf{\\%}", .before = 8) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ", 
    `\\bf{Median}` = " ",  .before = 14) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ", 
    `\\bf{Median}` = " ", .before = 20) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ", 
    `\\bf{Median}` = " ", .before = 24) %>%
  add_row(`\\bf{Team characteristics}` = "\\bf{Statistical analyses}", `  ` = " ", 
    `\\bf{Range}` = "\\bf{n}", `\\bf{Median}` = "\\bf{\\%}", .before = 25) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ",
    `\\bf{Median}` = " ", .before = 28) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", `\\bf{Range}` = " ", 
    `\\bf{Median}` = " ", .before = 32) %>%
  add_row(`\\bf{Team characteristics}` = " ", `  ` = " ", 
    `\\bf{Range}` = "\\bf{Range}", `\\bf{Median}` = "\\bf{Median}", 
    .before = 33) %>% 
  kbl(., booktabs = T, align = c("l", "l", "r", "r"), escape = F, 
    longtable = F, linesep = "", 
    caption = glue::glue("Descriptive statistics of teams, acoustic analyses, and statistical analyses included in the meta-analysis. The data set included analyses from {pull_from_tib(team_desc_df, stage, 'Final', n_teams)} teams and  {pull_from_tib(team_desc_df, stage, 'Final', n_analysts)} analysts."), 
    label = "descriptives-table") %>%
  kable_styling(full_width = F, latex_options = "scale_down") %>% 
  add_indent(c(9, 15, 21, 26, 29, 34)) %>% 
  add_indent(c(37, 38), all_cols = T) %>% 
  row_spec(8, hline_after = T) %>% 
  row_spec(25, hline_after = T)
```
<!-- END TABLE --> 

### Review ratings

Teams reviewed each others' acoustic and statistical analyses.
The mean rating of the acoustic analyses, on a scale from 0 to 100, is `r round(review_mean$mean_prating,1)` (SD = `r round(review_mean$sd_prating,1)`).
The mean rating of the statistical analysis is `r round(review_mean$mean_srating,1)` (SD = `r round(review_mean$sd_srating,1)`).
For reference, as mentioned in the Methods section, a score of 75 was defined as "an imperfect analysis but the needed changes are unlikely to dramatically alter the final interpretation", indicating that on average reviewers judged the provided analyses to be appropriate, although "imperfect".

## Meta-analytic estimation {#meta-est-2}

This section deals with the meta-analytic analysis of the results submitted by the teams.
As discussed above, the analyses of only 30 teams out of all the submitted analysis were included in the meta-analytic model discussed here.
First, we report on the between-team variability estimate (i.e. the meta-analytic group-level standard deviation $\sigma_{\alpha_{\text{t}}}$), which is the focus of this study, followed by the meta-analytic estimate (i.e. the intercept of the meta-analytic model, in other words, the estimated effect of typicality on the acoustic production of adjective-noun combinations).

```{r load-meta-analysis}
# Preregistered ----
meta_bm_prereg <- readRDS(here("./data/meta_analysis/meta_bm_prereg.rds"))

meta_intercept_prereg <- fixef(meta_bm_prereg)
meta_random_prereg <- ranef(meta_bm_prereg)
meta_bm_prereg_tidy <- tidy(meta_bm_prereg)

# wrangle by hand 
post_meta_prereg <- meta_bm_prereg %>%
  spread_draws(b_Intercept, r_animal[team,]) %>%
  mutate(team_mean = b_Intercept + r_animal) %>% 
  group_by(team) %>% 
  dplyr::summarise(post_mean = mean(team_mean),
            lower95 = quantile(team_mean, probs = .025),
            higher95 = quantile(team_mean, probs = .975)
            ) %>% 
  # this basically needs to be fed by whether or not the authors considered there to be an effect
  mutate(compelling = as.factor(case_when(lower95 > 0 ~ "negative",
                                          higher95 < 0 ~ "positive",
                                          TRUE ~ "not compelling"))) 

compelling_results_prereg <- post_meta_prereg %>% 
  count(compelling) %>% 
  summarise(prop = n / sum(n))

compelling_count_prereg <- table(post_meta_prereg$compelling)

# Model ID ----

meta_bm <- readRDS(here("./data/meta_analysis/meta_bm.rds"))

meta_intercept <- fixef(meta_bm)
meta_random <- ranef(meta_bm)
meta_bm_tidy <- tidy(meta_bm)

# wrangle by hand 
post_meta <- meta_bm %>%
  spread_draws(b_Intercept, r_model_id[model,]) %>%
  mutate(model_mean = b_Intercept + r_model_id) %>% 
  group_by(model) %>% 
  dplyr::summarise(post_mean = mean(model_mean),
            lower95 = quantile(model_mean, probs = .025),
            higher95 = quantile(model_mean, probs = .975)
            ) %>% 
  # this basically needs to be fed by whether or not the authors considered there to be an effect
  mutate(compelling = as.factor(case_when(lower95 > 0 ~ "negative",
                                          higher95 < 0 ~ "positive",
                                          TRUE ~ "not compelling"))) 

compelling_results <- post_meta %>% 
  count(compelling) %>% 
  summarise(prop = n / sum(n))

compelling_count <- table(post_meta$compelling)

# Nested ----

meta_bm_nest <- readRDS(here("./data/meta_analysis/meta_bm_nest.rds"))

meta_intercept_nest <- fixef(meta_bm_nest)
meta_random_nest <- ranef(meta_bm_nest)
meta_bm_nest_tidy <- tidy(meta_bm_nest)

# wrangle by hand 
post_meta_nest <- meta_bm_nest %>%
  spread_draws(b_Intercept, `r_animal:model_id`[model,]) %>%
  mutate(model_mean = b_Intercept + `r_animal:model_id`) %>% 
  group_by(model) %>% 
  dplyr::summarise(post_mean = mean(model_mean),
            lower95 = quantile(model_mean, probs = .025),
            higher95 = quantile(model_mean, probs = .975)
            ) %>% 
  # this basically needs to be fed by whether or not the authors considered there to be an effect
  mutate(compelling = as.factor(case_when(lower95 > 0 ~ "negative",
                                          higher95 < 0 ~ "positive",
                                          TRUE ~ "not compelling"))) 

compelling_results_nest <- post_meta_nest %>% 
  count(compelling) %>% 
  summarise(prop = n / sum(n))

compelling_count_nest <- table(post_meta_nest$compelling)


# Load MA with predictors
# Generate tibble with estimate for printing to prose
predictors_bm_rintercepts <- 
  readRDS("./data/meta_analysis/predictors_bm_rintercepts.rds") %>%  
  as_draws_df() %>% 
  select(contains("b_")) %>% 
  pivot_longer(cols = everything(), names_to = "param", values_to = "estimate") %>% 
  group_by(param) %>% 
  mean_qi(.width = 0.95) %>% 
  transmute(param, estimate, lo = .lower, hi = .upper) %>% 
  mutate_if(is.numeric, specify_decimal, 2) %>% 
  mutate(param = str_remove_all(param, "b_"), 
    param = str_remove_all(param, "outcome."), 
    param = str_remove_all(param, "Mintercept"), 
    param = str_remove_all(param, "temporal_window."), 
    ci = glue::glue("[{lo}, {hi}]")
    ) 

# report_posterior(predictors_bm_rintercepts, "segment")
```

### Between-team variability

The primary aim of this analysis is to assess the degree of between-team variability.
As a measure of between-team variability, we chose to use the meta-analytic group-level standard deviation ($\sigma_{\alpha_{\text{t}}}$).

According to the preregistered meta-analytic model, the group-level standard deviation for teams is between `r round(meta_bm_prereg_tidy$conf.low[["sd_animal__Intercept"]], 2)` and `r round(meta_bm_prereg_tidy$conf.high[["sd_animal__Intercept"]], 2)` standard units at 95% credibility.
In other words, the estimated range of variation across teams lies somewhere between `r pm(round((meta_bm_prereg_tidy$conf.low[['sd_animal__Intercept']] * 1.96), 2))` (0.03 * 1.96) and `r pm(round((meta_bm_prereg_tidy$conf.high[['sd_animal__Intercept']] * 1.96), 2))` (0.07 * 1.96) with 95% credibility.”

**Non-preregistered**. However, in our preregistration we did not take into account that teams might submit multiple analyses/models which, if unaccounted for, violates the independence assumption. 
Teams were explicitly instructed to only submit one effect size without enforcing it. 
As a result, some teams followed the instruction and submitted only one model while others submitted multiple models.
To account for this added layer of dependency, we have run a model with team and model ID nested within team as group-level effects (`(1|team) + (1|team:model_id)`), which allows us to estimate both the between-team variation and the between-analysis variation.
This analysis was not preregistered and should thus be interpreted with caution.^[Note that before fitting this model, we fit a separate one in which model ID was the only (non-nested) group-level effect. 
The estimated group-level effect of model ID is identical to that of the nested model, so we will not discuss it further.]

<!-- Based on the first non-preregistered model, the group-level standard deviation for model ID is between `r round(meta_bm_tidy$conf.low[["sd_model_id__Intercept"]], 2)` and `r round(meta_bm_tidy$conf.high[["sd_model_id__Intercept"]], 2)` standard units, at 95% credibility.
This means that the deviations of any individual analysis model from the meta-analytic effect estimate range between ±0.22 to ±0.3 (0.11 * 2, 0.15 * 2) in standard units.-->

The nested model yields a posterior 95% CrI for between-team variability of `r round(meta_bm_nest_tidy$conf.low[["sd_animal__Intercept"]], 2)` to `r round(meta_bm_nest_tidy$conf.high[["sd_animal__Intercept"]], 2)` standard units ($\beta$ = 0.02, SD = 0.01), corresponding to a mean deviation range of about `r pm(x = 0)` to `r pm(x = 0.1)` standard units and 95% probability.
The posterior 95% CrI for between-analysis variability (nested within teams) is `r round(meta_bm_nest_tidy$conf.low[["sd_animal:model_id__Intercept"]], 2)` to `r round(meta_bm_nest_tidy$conf.high[["sd_animal:model_id__Intercept"]], 2)` standard units ($\beta$ = 0.132, SD = 0.01).
For the sake of illustration, these would correspond to an estimate of between-model variability in milliseconds (if looking for example at segment duration) that ranges between `r round(3.8*1.96, 1)` and `r round(7.7*1.96, 1)` ms at 95% credibility.
We interpret these values in more details in the Discussion section.

Taken together, the models suggest that the variability of reported effects between any model (within team or across) is substantially larger than the variability across individual teams.
We return to this important observation later.

### Meta-analytic intercept

After assessing the variation between teams and analyses, we now turn to the meta-analytic estimate of the effect of typicality on the acoustic realization of sentences with adjective-noun combinations.
The meta-analytic model estimates the range of probable values of the standardized effect size to be between `r round(meta_intercept_prereg[3],3)` and `r round(meta_intercept_prereg[4],3)` standard units (95\% CrI, mean = `r round(meta_intercept_prereg[1],3)`).
In other words, our best guess is that speakers might not encode typicality in the acoustic signal (e.g. by duration, f0, etc,) or, if they do, they do so by a maximum of `r pm(x = 0.02)` standard units.

**Non-preregistered**. As mentioned in the previous section, we have run an additional model, using team and model ID nested within team as group-level effects.
In this non-preregistered model, the meta-analytic intercept estimate is between `r round(meta_intercept_nest[3],3)` and `r round(meta_intercept_nest[4],3)` standard units (95\% CrI, $\beta$ = `r round(meta_intercept_nest[1],3)`).
This suggests that the acoustic measures of typical word combinations are 0.01 standard units lower to 0.04 standard units higher than the measures of atypical word combinations, at 95% confidence.
Relative to the preregistered model, this model suggests a somewhat positive effect of typicality (although small negative effects are also possible).

The meta-analytic intercept conflates estimates from a variety of responses taken from very different places in the utterance (nouns, adjectives, determiners, entire phrases or sentences, etc).
This means that some of the effects on a particular response as observed in a specific location within the utterance might naturally be positive, while other negative, resulting in a meta-analytic intercept of about zero.
We want to stress, however, that our focus is not on the meta-analytic intercept per se, but on the fact that a seemingly straightforward research question led to so many possible outcomes.
More on this in the Discussion section.

Figure \@ref(fig:plot-meta1) illustrates the individual intercepts for critical typicality coefficients across models and teams, sorted in ascending order based on their mean.
Given the nature and wide variety of acoustic operationalizations, there is no natural interpretation of the scale, so we cannot interpret the direction of estimates.
When looking at the raw estimates and their variance (grey triangles and lines), it is striking how much estimates differed. 
Estimates ranged from `r round(min(msa_models$estimate),2)` to `r round(max(msa_models$estimate),2)` standard units. 

While the majority of model estimates and their uncertainty after shrinkage (in black) yields inconclusive results (i.e. are compatible with a point null hypothesis), there are `r compelling_count_nest[[1]] + compelling_count_nest[[3]]` model estimates for which the 95\% credible interval does not contain zero (in blue, `r round((compelling_results_nest[[1,1]] + compelling_results_nest[[3,1]]), 2) * 100`\%).

<!-- FIGURE -->
(ref:plot-meta1) Standardized effect sizes across all critical coefficients provided by the teams. Raw estimates are displayed in grey. Estimates after shrinkage as provided by the meta-analytic model are displayed in black/orange.

```{r plot-meta1, echo = FALSE, fig.cap="(ref:plot-meta1)", out.width="100%"}
knitr::include_graphics(here::here("./figs/meta_plot1_shrinkage.png"), error = FALSE)
```

## Analytic and researcher-related predictors

After assessing the variability across teams and models, we now turn to estimating the impact of a series of predictors on the reported standardized effects.
There is a large amount of variation between and within teams, raising the question as to whether we can explain some of this variation or whether it is purely idiosyncratic [@breznau2021observing].

We have run a model as described in Section Analytic and researcher-related predictors affecting effect sizes above.
Figure \@ref(fig:plot-meta2), panel C, displays the coefficients for all predictors alongside their 80% and 95% credible intervals.
The model suggests that most team-specific predictors yield very small deviations from the meta-analytic estimate and their 95% credible intervals include zero, leaving us highly uncertain about their direction. 
Neither analysts' prior beliefs in the phenomenon `r report_posterior(predictors_bm_rintercepts, "prior_belief_s")`, nor their seniority in terms of years after completing their PhD `r report_posterior(predictors_bm_rintercepts, "years_from_phd_s")` seem to affect model estimates.
Similarly, the evaluation of the quality of the analysis from their peers yielded a rather small effect magnitude, again characterized by large uncertainty `r report_posterior(predictors_bm_rintercepts, "all_rating_s")`. 
Interestingly, the model uniqueness, i.e. how unique the choice and combination of predictors are, affects the analysts' estimate, with more unique models producing higher positive estimates `r report_posterior(predictors_bm_rintercepts, "pop_sdi_s")`. 

Looking at the most important choices during measurement, both the acoustic parameter under investigation (e.g. f0 or duration) and the choice of measurement window affected the results. 
Panels A and B of Figure \@ref(fig:plot-meta2) display the posterior estimates for the measurement outcome (i.e. what acoustic dimension was measured, panel A) and measurement window (i.e. what is the unit over which the outcome was measured, panel B). 
If, on one hand, an acoustic dimension related to f0 was measured, estimates are lower than the meta-analytic estimate.
If, on the other hand, duration was measured, estimates are higher than the meta-analytic estimate.
Similarly, if acoustic parameters were measured across the entire sentence, estimates are lower than the meta-analytic estimate.
In other words, depending on the choice of measurement and the measurement window, analysts might have arrived at different conclusions about how and if typicality is expressed acoustically. 

It is due of the latter patterns that we need to interpret the results of the model with great caution. 
Since there are combinations of analytic choices that appear to systematically result in lower or higher estimates and the fact that predictors are not fully crossed (i.e. we do not have the same amount of data for all combinations of e.g. outcome and measurement window), the estimates for certain predictors might be biased if predictors are collinear. 
This bias might be amplified by the fact that the scale has no natural way of being interpreted across all teams with different measurements cancelling each other out. 
We checked correlations between predictors and while predictors do not seem to be highly collinear, the estimates might still be biased.  

<!-- Figure --> 
(ref:plot-meta2) The effects of analytic and researcher-related predictors on the reported standardized effect sizes. (A) Posterior samples for the four most frequent outcome variables; (B) Posterior samples for the four most frequent temporal windows: Black points indicate medians; shaded areas represents 50/80/95% highest density intervals. (C) Mean posterior samples (white circles) and 80/95% credible intervals for all predictors.

```{r plot-meta2, echo = FALSE, fig.cap="(ref:plot-meta2)", out.width="100%"}
knitr::include_graphics(here::here("./figs/alltogether.png"), error = FALSE)
```

# Discussion

## Summary

We gave `r pull_from_tib(team_desc_df, stage, "Submitted", n_teams)` analyst teams the same speech data set to answer the same research question: *Do 
speakers acoustically modify utterances to signal atypical word combinations?*
In order to answer this question, teams had to interpret the research question by
operationalizing constructs within multidimensional signals, operationalizing and choosing appropriate model predictors, and constructing appropriate statistical models.
This complex process has led to a vast "garden of forking paths", i.e. to a wide range of combinations of possible analytic decisions.
The submitted analyses exhibited at least `r nrow(unique_model_specs)` unique ways of operationalizing the acoustic signal alongside `r nrow(unique_acou_oper)` unique ways of constructing the statistical model.
By multiplying the numbers of acoustic and model specifications, there are in principle `r nrow(unique_model_specs) * nrow(unique_acou_oper)` possible unique combinations. 
Note that this is a conservative estimate of the number of possible analytic choices for our research question, ignoring many other degrees of freedom like e.g. acoustic parameter extraction, outlier treatment, and transformations, all of which might have an impact on the final results [@breznau2021observing].

Different analysis paths led to different categorical conclusions with `r perc_atleast_1`% of teams reported to have found at least one statistically reliable effect.
To gain a better understanding of whether the observed quantitative variability can result in theoretically different claims, we will contextualize them in actual acoustic measures.
We calculated the standard deviation of a selection of acoustic measurements, as submitted by the analysis teams: duration, f0 and intensity, taken from different time windows. 
These standard deviations can be considered as a coarse indication of the variability in the obtained acoustic measures.
We can now use these values to interpret the meta-analytic estimates, which are in standardized units, by transforming the standardized units to measures of duration, f0 and intensity.^[Note that these categories necessarily refer to a variegated set of measures, for example the domain "word" includes words that differed along several dimensions, including their length and their metrical structure.]

For example, for those analyses that investigated the duration of vowels (e.g. the duration of the stressed vowel in *Banane*), the reported duration measures exhibit standard deviations that range from 33.4 to 51.4 ms.
These standard deviations allow us to convert the meta analytic estimates into milliseconds by multiplying those values with the standard units.
The reported effect estimates from teams varied between `r round(min(msa_models$estimate),2)` and `r round(max(msa_models$estimate),2)` standard units, which corresponds to duration values ranging from `r round(min(msa_models$estimate) * 33.4,2) ` to `r round(max(msa_models$estimate) * 51.4,2)` ms.
A more conservative approach is to convert the meta-analytic estimates of between-model variation, thus obtaining an estimate of between-model variability in milliseconds that ranges between `r round(3.8*1.96, 1)` and `r round(7.7*1.96, 1)` ms at 95% credibility.^[The calculation is thus: the minimum standard deviation of duration multiplied by the lower limit of the 95% CrI of the between-models variability estimate, times 1.96 to obtain a 95% CrI: $33.4 * 0.11 = 3.8$, $3.8 * 1.96 = 7.4$ ms; the maximum standard deviation of duration multiplied by the upper limit of the 95% CrI of the between-models variability estimate, times 1.96: $51.4 * 0.15 = 7.7$, $7.7 * 1.96 = 15.1$ ms.]

While this might not immediately strike one as highly variable, it crosses several theoretically relevant thresholds for perception and articulation:
for example, the widely studied phenomenon of incomplete neutralization involves vowel duration effects ranging from 7 to 15 ms [@nicenboim2018using]. 
This particular phenomenon has sparked long-lasting methodological and theoretical debates about the very nature of linguistic representations [@port2005against] and has been replicated several times in both production and perception.
Vowel duration differences within this range have also been reported across phenomena associated with segmental contrasts [@coretta2019], reduction phenomena [@nowak2006], and biomechanical reflexes of prominence [@mucke2014effect]. 
Thus, variation between different analyst teams of `r round(3.8*1.96)` to `r round(7.7*1.96)` ms in one or the other direction can be theoretically relevant and might lead to opposing theoretical conclusions.

```{r msa-sd}
# MSA SD ----

msa_mean_sd_df <- read_csv("data/analyses/msa_mean_sd_df.csv") %>%
  filter(transformation == "none", mean > 10, temporal_window != "sentence")

meta_sd_clo <- meta_bm_tidy$conf.low[["sd_model_id__Intercept"]] 
meta_sd_chi <- meta_bm_tidy$conf.high[["sd_model_id__Intercept"]] 

msa_sd <- msa_mean_sd_df %>% group_by(outcome, temporal_window) %>%
  summarise(
    sd_min = min(sd),
    # sd_mean = mean(sd),
    sd_max = max(sd),
    .groups = "drop"
  )

msa_sd_sd_lo <- msa_sd %>%
  mutate(
    across(starts_with("sd_"), ~.x * meta_sd_clo * 1.96),
    ci = "lower"
  ) %>%
  pivot_longer(sd_min:sd_max, names_to = "sd")
msa_sd_sd_hi <- msa_sd %>%
  mutate(
    across(starts_with("sd_"), ~.x * meta_sd_chi * 1.96),
    ci = "upper"
  ) %>%
  pivot_longer(sd_min:sd_max, names_to = "sd")

msa_sd_sd <- bind_rows(msa_sd_sd_lo, msa_sd_sd_hi) %>%
  mutate(value = round(value, 1)) %>%
  pivot_wider(names_from = c(ci, sd), values_from = value) %>%
  unite("lower", lower_sd_min, lower_sd_max, sep = "-") %>%
  unite("upper", upper_sd_min, upper_sd_max, sep = "-") %>%
  mutate(
    unit = c("ms", "ms", "hz", "hz", "dB", "dB")
  ) %>%
  rename("temporal window" = "temporal_window")
```

```{r}
#| label: msa-sd-table
#| results: 'asis'
msa_sd_sd %>% 
  mutate(outcome = case_when(
    outcome != "f0" ~ str_to_sentence(outcome), 
    TRUE ~ outcome), 
    `temporal window` = str_to_sentence(`temporal window`)) %>% 
  janitor::clean_names(case = "sentence") %>% 
  knitr::kable(format = "latex", booktabs = T, escape = F, 
    linesep = "", 
    caption = "Estimated 95 percent CrIs of deviation from the meta-analytic effect in acoustic measures, based on the lower and upper limits of the between-model variation.",
    label = "msa-sd-table"
  ) %>% 
kableExtra::kable_styling(., font_size = 11)
```

While one might find it obvious that measuring different parts of the speech signal can lead to different results, the fact that analysts (and reviewer alike) considered all these data analytic pipelines valid ways of answering the same research question points to a lack of theoretical consensus on what parts of the speech signal correspond to what types of communicative functions. 
Importantly, even if analysts choose to measure more or less the same acoustic property within the same measurement window, they arrive at different estimates: 
For example, six teams measured f0 in the adjective and predicted f0 based on typicality as a categorical predictor. Their standardized effect estimates ranged from -0.11 to 0.38 standard deviations. 
While these teams in principle measured the same thing, they differed in analytical details of how f0 was operationalized (i.e. mean, mininum, maximum, or range) and how their statistical model was constructed (i.e. the number of predictors ranged from 1-3 and the number of random effect terms ranged from 1-4). 
As shown by Breznau et al.[-@breznau2021observing], even seemingly inconsequential analytical choices can affect conclusions in non-trivial ways.

The observed variation does not seem to be systematic.
For example, variation between teams was not predicted by the analysts' prior expectations about the phenomenon.
In fact, teams on average rated the plausibility of the effect as rather high before receiving access to the data.
The observed variation was neither predicted by the analysts' experience in the field nor by the perceived quality of the analysis as judged by other teams. 
Analyses received overall high peer-ratings for both the acoustic and the statistical analysis, suggesting that reviewers were generally satisfied with the other teams' approaches.

These findings are very much in line with previous crowd-sourced projects that suggest variation between teams is neither driven by perceived quality of the analysis nor by analysts' biases or experience [e.g., @silberzahn2018many; @breznau2021observing].
Following @breznau2021observing [p. 9], we are bound to conclude that "[...] idiosyncratic uncertainty is a fundamental feature of the scientific process that is not easily explained by typically observed researcher characteristics or analytic decisions".
Idiosyncratic variation across researchers might be a fact of life which we have to acknowledge and integrate into how we evaluate and present evidence.

While properties of the teams did not seem to systematically affect the results, teams' estimates seem to highly depend on certain measurement choices. 
Human speech entails complex multidimensional signals.
Researchers need to make choices about what to measure, how to measure it and which temporal unit to measure it in. 
Some of these choices seem to result in estimates in one direction while others seem to result in estimates into another. 
For example, measurements related to f0 tended to result in lower estimates while measurements related to duration tended to yield higher estimates.

The asymmetry observed in the effect direction of different measurements can have several causes.
First, there could be a true underlying relationship between typicality and the speech signal that manifests itself in some measures but not others and/or manifests itself negatively in one acoustic measure but positively in another.

Secondly and orthogonal to a possible true relationship, certain measurement choices might be associated with stronger expectations relative to the research question, which might lead to stronger researcher biases.
Many analysts targeted measures related to f0, likely because similar functional relationships like information structure and predictability can be expressed by f0 [e.g. @grice2017integrating; @turnbull2017role]. 
Moreover, prior work has actually suggested a relationship between typicality and f0 [e.g. @dimitrova2008prosodic; @dimitrova2009did]. 
Participating analysts could have been aware of those findings, which might have, subconsciously or otherwise, nudged their choices into one particular direction.

Regardless of the cause of these systematic effects, we have to conclude that depending on the choice of how the speech signal is operationalized, researchers might find evidence for or against a theoretically relevant prediction.
This conclusion is further supported by the fact that between-team variability was lower than between-model variability.
This is an important observation when put into context of the fact that most teams submitted many different models.
Teams submitted up to 16 different models to test for a possible relationship between typicality and the speech signal.
The complexity of the speech signal lends itself to multiple approaches, but this plurality of hypothesis tests invites bias and can dramatically increase the rate of falsely claiming the presence of an effect [@roettger2019researcher; @simmons2011false].
We of course are not arguing that exploratory analyses should not be employed.
Rather, we simply want to point out that if the theoretical underpinnings of the field were much clearer, different teams would have converged towards a limited set of analyses despite of a less specific research question.

In relation to this aspect, one team coordinator decided to drop out of the project because of its approach being too top-down.
The coordinator also expressed a preference to be able to explore and run a variety of descriptive analyses followed up with inferential statistics.
We find that this attitude speaks to the main objective of the current study: investigate researchers' degrees of freedom in the speech sciences. 
Based on our personal experience with research in the field, it is common practice to test many different types of models, using many different types of measurements, to answer one research hypothesis.
While this is a valid way to explore data and generate new hypotheses, it is not suitable for hypothesis testing.
When operating within the frequentist inferential framework, testing the same hypothesis with different dependent variables is known to increase the false-positive (Type-I error) rate. 
The well-established solution to this problem is to apply a correction for family-wise error (i.e., alpha correction). 
However, less clear-cut degrees of freedom such as observed in the present study can not be corrected for in a straightforward way. 
If uncorrected for, these degrees of freedom can nevertheless drastically inflate the false positive rate, even if different choices are highly correlated [@roettger2019researcher]. 
Another possible outcome of analytic flexibility as seen in this study is selective reporting of those tests that yield a desirable outcome [@kerr1998harking; @john2012measuring; @simmons2011false], while null results remain unreported [@sterling1959publication; @rosenthal1979file]. 
Fields such as the speech sciences that make theoretical advances based on multidimensional data should be aware of this flexibility and calibrate their confidence in empirical claims accordingly. 

Looking at our results, one might argue (and this interpretation has been articulated by several teams during the collaborative write-up) that our sample of speech scientists actually converged on a qualitative conclusion, i.e. there is no evidence for a relationship. 
However, if there truly was no underlying relationship, our results would suggest a concerning false positive rate with `r perc_atleast_1`% of teams reported to have found at least one statistically reliable effect. 
This rate is substantially higher than the conventionally accepted 5% false positive rate in for example null hypothesis significance testing frameworks.
If, on the other hand, there actually was an underlying relationship, our results would suggest a concerning false negative rate of `r 1-perc_atleast_1`%, with the majority of teams not detecting the effect. 
If the latter was true, the fact that the majority of teams arrived at a null result might also simply be a consequence of the sample size in the data set being too small to reliably detect an effect (which is unknown to us). 
Thus, we do not think that our study provides convincing evidence that speech researchers converged on the same qualitative answer to a broad research question.

## Lessons for the methodological reform movement

The current results point to important barriers to the successful accumulation of knowledge.
The replication crisis has brought attention to scientific practices that lead to unreliable and biased claims in the literature [@vazire2017quality; @fidler2018reproducibility].
One of the suggested paths forward is for researchers to directly replicate previous studies more often [@open2015estimating; @camerer2018evaluating].
While we agree with the importance of direct replications, our study (and similar crowd-sourced analyses before us) suggest that replicating more is simply not enough.
There is only limited value in learning that a particular procedure is replicable if the idiosyncratic nature of the procedure itself might not yield a representative result relative to all possible procedures that could have been applied to the research question.
Beyond a replication crisis, quantitative disciplines are going through what has been called an "inference crisis" [@rotello2015more; @starns2019assessing].
As shown by the peer-ratings of the analyses reported in this study, well-trained and experienced speech researchers not only applied completely different approaches to the same research question, but also considered most of these alternative approaches acceptable.
Being aware of this idiosyncratic variation between analysts should lead to more nuanced claims and a certain level of epistemic humility [see @campbell1975 for an overview of the concept].

A desired outcome of knowing that different but reasonable measurement choices or statistical approaches might lead to different interpretations of research data is to calibrate our (un)certainty in the strength of the collected evidence and, in turn, communicate that (un)certainty appropriately.
The fact that the choice of measurement, measurement window, and predictor choice affect the answer to the research question further suggests that research assumptions and hypotheses should be formulated in much greater detail, particularly so in regards to how measurement systems (here, the acoustic signal) and underlying conceptual constructs (here, the phonetic expression of typicality) relate to each other.

We should ideally specify the link between conceptual construct and quantitative system---the "derivation chain" [@dubin1970theory; @meehl1990summaries]---prior to data collection and analysis, including defining constructs and their relationship within the quantitative system, specifying auxiliary assumptions and boundary conditions, and defining target measurements, statistical expectations and possible (and impossible) effect magnitudes.
Without well-defined derivation chains, we "are not even wrong" [@scheel2022most] because falsified expectations cannot tell us much about the conceptual constructs they are based on when the relationship between the two is underspecified.
Some of the analysis teams explicitly recognized and acknowledged the need to formulate a more precise version of the research question by preregistering their planned data analysis pipeline.  

In light of the observed analytic flexibility, there are a few things that researchers can do to appropriately calibrate confidence in their claims.
First of all, through sharing of materials, data and statistical protocols, we can make our idiosyncratic choices transparent to others [@munafo2017manifesto; @vazire2017quality].
Sharing further enables the evaluation and verification of underlying claims and allows for the evaluation of empirical, computational and statistical reproducibility [@lebel2018unified]. 
It allows for alternative analyses to establish analytic robustness [@steegen2016increasing] and strengthens attempts to synthesize evidence via meta-analyses [e.g., @nicenboim2018using].
Given that minor procedural changes can sometimes drastically affect the final interpretation of the results [@breznau2021observing], we should ideally share a detailed documentation of the data collection procedure, the measurement choices, the data extraction, and statistical analyses.
Within fields that deal with speech data, open source software that permits the extraction of acoustic parameters via reproducible scripts can help other researchers to trace back seemingly inconsequential choices during the measurement process [e.g., Praat: @boersma2021praat; EMU: @winkelmann2017emu; the Montreal Forced Aligner: @mcauliffe2017].

Second, making analytic pathways completely re-traceable does not change the fact that analysts apply different analytic approaches.
Crowd-sourced projects such as the current one can shed light on the range of degrees of freedom during analysis and could possibly help produce a consensual estimated effect if the research hypothesis is specific enough. 
Crowd-sourcing analyses is obviously not always feasible in terms of required resources and time, but could be a consideration for claims that have large epistemological or practical consequences.

Third, if we develop a good understanding of relevant analytic degrees of freedom, we could apply all conceivable analytic strategies and compare the results across all combinations of these choices.
Such an analysis can provide insight into how much the conclusions change due to analytic choices as well as which choices have neglible or large impact on the result. 
This approach is called a "multiverse analysis" [e.g, @steegen2016increasing; @harder2020multiverse] and has recently gained popularity across disciplines.

Finally, neither crowd-sourcing nor multiverse analyses will guarantee that all relevant pathways are explored. 
Crowd-sourcing is limited by the sampled analysts and their biases.
Multiverse analyses are limited even further by the group of researchers who define possible analytic pathways.
Eventually, a mature scientific discipline needs to develop a set of detailed quantitative hypotheses of how conceptual constructs manifest themselves in the measured system, i.e. in the present case how communicative pressures of certain functions are expressed in the acoustic signal. 
Possible tools to strengthen theoretical development are relate to mathematically formalizing verbal expectations or using computational models [e.g., @van2020formalizing; @guest2021computational; @scheel2021hypothesis; @devezer2021case]. 

## Caveats
Our study has several limitations that need to be considered when evaluating our results. 

First, while the total number of analyses is larger than most earlier crowd-sourcing projects, it is likely to be too small to reliably estimate the impact of certain predictors.
Since predictors' values were not systematically distributed across teams, our estimates are characterized by large uncertainty.

Second, uncertainty is further inflated by the fact that the research question presented to the teams was vague, despite being of a kind normally found in the speech science literature: *Do speakers acoustically modify utterances to signal atypical word combinations?* 
Interpreting the research question/hypothesis differently in terms of its statistical consequences has recently been shown to explain some variation between analysis teams in many-analyses projects [@auspurg2021has]. 
The analysts might also have tried to answer different specific manifestations of the research question that was given to them, leading to different choices down the line (e.g. Do speakers modify f0 in atypical adjectives?). 
It could be argued that some teams would have not specified such a vague research question to begin with which would have reduced the possible degrees of freedom substantially. 
However, this very underspecification of research hypotheses in the field of speech science [and beyond, see @scheel2022most] is very common.
For example, researchers seem to have not yet agreed on how to acoustically measure cross-linguistically common phenomena such as word stress [e.g. @gordon2017acoustic].
Research on acoustic markers of clinical conditions such as depression and schizophrenia are often difficult to compare due to the wide variety of different acoustic measures employed [e.g. @cummins2015review; @parola2022speech]. 

Third, the design of this crowd-sourced study has artificially inflated the variability between teams by encouraging anti-coordination strategies. 
Teams knew that there will other analyst teams and therefore might have chosen a "less canonical" analysis.
Since analysts were guaranteed to become co-authors of a (in principle) guaranteed publication, such an anti-coordination approach was not explicitly disincentivized. 

Forth, our sample is an opportunity sample. 
We have advertised the project through online platforms which might have led to the exclusion of certain potential researcher groups. 
The sampling strategy also might have given access to researchers who were less experienced in particular aspects of the data analysis, possibly introducing uncommon analytic choices or poor quality analyses.
However, to our knowledge, neither the peer review among teams nor the information gathered through our questionnaires indicated any obvious cases of what one might consider incompetent analyses. 

In light of both the observed large variability between teams, and possible sources of bias, a field can benefit from explicit positionality statements [e.g., @jafar2018; @darwin2020; @fox2021open].
Researchers do not analyze data in a vacuum. 
It is important to recognize and disclose one's positionality, i.e., a reflection about how educational background, social identity, power, experience and context might influence researchers' approaches and interpretations.
For example, the coordinating authors have engaged with meta-scientific research before and have been actively involved in methodological debates about scientific practices including transparency and statistical methods.
They have in the past used the lack of standardized analytic approaches as an argument for proposing behavior and policy changes in the field. 
This might have biased their own judgement during the analysis which itself came with many researcher degrees of freedom.
We hope we were able to make these degrees of freedom as well as the timing and reasoning of these analytic choices at least detectable and we invite other researchers to re-analyze our data and try to replicate our results using a different research question.

Finally, the present study focused on a particular phenomenon within the speech sciences using a speech production data set with very specific properties.
The generalizability of our findings to other disciplines, as well as to other sub-disciplines of the language sciences specifically, is, of course, limited.
We focused on quantitative analyses that require the operationalization of a multidimensional signal in an artificial elicitation situation (laboratory speech). 
While we do believe that our qualitative conclusions hold across fields exhibiting similar methodologies, the detailed quantitative results will only be able to directly inform similar disciplines that work with speech or audio/video signals. 
This is an important point to make because cognitive sciences in general, and the language sciences in particular, have many research areas that are based on qualitative methods [@haven2019].
It is conceivable that the discussed issues apply differently or not at all to qualitative data analyses.

# Conclusion

In recent efforts, several studies have highlighted the large degree of analytic flexibility in data analysis. 
When many different analysts have to analyze the same data set to answer the same research question, analysts differ in how they approach this task, leading to both different qualitative answers (i.e. is there evidence for a relationship or not) and different effect magnitudes. 
This is concerning, as it can lead to substantially different conclusions based on the same data set, a state of affairs that can generate biased inferential decisions and might weaken confidence in the published literature.
More specifically, what we find of particular relevance is the fact that commonly research proceeds based on publications from one research team at a time.
If we imagine a situation where any of the 46 teams could have been *the* team publishing a study on this topic, it is immediately clear that that single study is just a very limited view.
In light of this we want to stress that the field has to quickly move from one-off studies to collaborative approaches like the one employed here and to more frequent replication attempts for example by incentivizing replication through dedicated funding and editorial policies, among others.

Going beyond previous empirical studies, the current paper looked at many analyses of speech data.
Speech is a multidimensional signal that allows for great flexibility because it lends itself to a variety of possible operationalizations.
In this study, 46 teams of speech scientists analyzed the same data set.
Analytic approaches differed vastly in terms of their operationalization of key constructs, as well as their statistical analyses.
Given the observed variability, conservative estimates of the sheer number of possible analytic paths for this research question lies in the thousands.
Quantitatively, the between-team and between-model variation of estimates crosses important theoretical thresholds as to what constitutes communicative, cognitive, or bio-mechanical values.

In line with previous findings, neither the perceived quality of analyses, nor the experience or prior beliefs of teams explained the observed variation.
Importantly however, we found some evidence for systematic effects on teams' estimates based on what and how they measured the speech signal. 
This result, taken together with the meaningful between-model variation and the tendency to test the research question on multiple outcome variables, suggests that a vast plurality of acceptable approaches is expected to frequently lead to different conclusions.
We suggest that fields that use multidimensional data need to acknowledge these degrees of freedom, consider crowd-sourcing and multiverse analyses when evaluating epistemologically or practically important phenomena, and strengthen the link between theoretical predictions and the measurement system by means of mathematical formalization and computational modeling. 

# Author contributions

See <https://github.com/many-speech-analyses/many_analyses/blob/main/figs/credit-taxonomy.png>.

# Conflicts of interest

We have no conflicts of interest to disclose.

\appendix

# Glossary

- **Analysis team**: team of analysts or single analyst.
- **Reported effect sizes**: effect sizes reported by each analysis team.
- **Standardized model**: Bayesian refit of the team's model.
- **Standardized effect sizes**: ($\eta_i$) effect sizes returned by the standardized models.
- **Standardized standard error**: ($\text{se}_i$) standard deviation of the standardized effect sizes.
- **Bayesian random-effects meta-analysis** and **meta-analytic model**: multilevel intercept-only regression model for meta-analysis.
- **Meta-analytic group-level standard deviation**: ($\sigma_{\alpha_{\text{t}}}$) standard deviation of the group-level effect of team returned by the meta-analytic model.
- **Analytic and researcher-related predictors**: predictors used in the model that assess the effect of analytic and researcher-related factors on the standardized effects.
