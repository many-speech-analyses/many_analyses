Q3,Q5_2,Q5_3,Q5_4,Q6,Q8,Q9,Q10,Q11,Q12,Q13,Q14,Q15,q6
pervagor_adscensionis,70,85,80,publishable with minor revision.,NA,NA,NA,NA,NA,NA,NA,NA,2
pervagor_adscensionis,70,85,80,publishable with major revision.,"The acoustic analysis was robust; however, there was minimal phonetic analysis. It is clear from the data provided that a lot of work went into preparation and analysis, but it's difficult to evaluate without any guidance from the authors. For example, they say they didn't have time for feature selection, but still included a /Features subfolder with multiple scripts inside. Some information in the scripts didn't appear in the report (e.g., that the speaker utterances were chunked into 5 per utterance for the GMMs). We concluded the statistical analysis was robust based on the comments and the final report; however, we can't replicate the analysis in its current format.",The team used Linear mixed effects models (lmers) for their statistical analysis. Their choice of statistical analysis is suitable for the current study.,"The team derived the Kullback-Leibler divergence for Mel-frequency cepstrum coefficients (MFCC) and fundamental frequency (f0) between pairs of utterances and in adjective and z-scored within speaker. These MFCC and f0 distance measures were the dependent variables and a typicality score by subtracting the median typicality of A from the typicality of B from norming data. The team did not explain A or B. The derived median typicality score was the independent variable. However, we conclude their choice of variable selection was suitable for the current study. They noted they wanted to do more feature selection on other elements of the utterance, but were unable to due to time limitations.","The team did not justify their choice to use MFCC and f0 distance in their methodology. They did not justify their choice of using the derived median typicality score as opposed to the typicality categories; however, we concluded the use of the typicality score from the norming data was justified within the context of the statistical model.","The structure of the statistical models were suitable for the purposes of the current analysis. The adjective and the speaker were included as random effects. The initial lmer model had speaker by adjective as random slopes; however, this failed to converge. There were no interactions in their statistical models. Models with p-value > 0.05 were significant; however, residual plots were used to evaluate the interpretability of models. We note that the final models were not selected using a stepwise regression process and explanatory power between models were not evaluated.","The team filtered the analytical data set and removed observations with errors and included only observations in the noun-focused conditions and where the adjectives were the same across pairs (A and B). Their choice to remove observations with errors is justified; however, they did not provide summary statistics for the final analytical data set.","The team did not transform the final analytical dataset; however, they did mention in their questionnaire they would’ve removed outlier when they were putting together their analytical data set and they would've normalised the MFCC and f0 distance measures. There was some contradiction between what was reported (that no outliers were removed) and the statistical analysis (where outliers were removed in the final step) provided, but it did not have a major effect on the outcome.",The authors didn't relate the findings back to the original research question.,1
pervagor_adscensionis,75,60,68,publishable with major revision.,"It is not clear how the final models were chosen, other than for reasons involving convergence issues with more elaborated models. The analysis of adjectives, but not nouns or noun phrases is not motivated.","Linear Mixed Effects models are suitable, but Bayesian Mixed Models might have been more informative and a more appropriate approach, given the exploratory nature of the analysis. In particular, since the team state at the outset that ""we had no strong hypothesis about the results we expected to find"", and since no hypotheses are developed over the course of the analysis, the choice of NHST does not seem to be well-motivated.",The reasons for the choice of variables in the final model are not clearly explained. The main factor informing model structuring and refinement appears to data sparsity and model convergence issues. This would be OK if more information had been provided to motivate the random variables selected in the final model.,"The main effects of typicality and the choice of continuous variables are well chosen and clearly motivated, but it is not clear why random intercepts are included only for adjectives.","It is not actually clear which final model was used: 'MFCC_all' is reported as having good residuals in 'stats.R', but earlier, the model is defined as follows:
MFCC_all = lmer(mfcc_dist ~ typ_diff + (1|adjA) + (1|subjA) + (1|noun_pair), all) # singular fit if 1|noun_pair is included

... so what was the final model run for the analysis? Presumably, the formula was modified to drop the final random effect, but the analysis is not reproducible from the scripts provided.",It appears that all numbered trials in the dataset provided were included in this analysis.,"The use of MFCCs, GMMs, and z-scored KL-divergences is insightful; these are well-established methods of data parameterization for speaker ID analysis and utterance comparisons of this type. This team have extensive experience in the application of these methods, and the data processing transformation and analysis are appropriate to the dataset and research question.","This team have used a novel approach to examine differences between utterances, and the application of methods more commonly used in forensic analysis and speaker recognition potentially offers important insights into the phonetic properties of the dataset. Unfortunately, it is difficult to interpret this approach because the tools are not deployed in service of a hypothesis, and the discussion is not sufficiently detailed to explain exactly what was found and what it might mean. In particular, the sub-division of the utterance, and the choice of models and how they related to these sub-intervals is not explained. For example, why are the MFCCs of adjectives produced in different typicality conditions examined, but not the entire target NP, the noun itself, or other constituents associated with the referent? The division of utterances into ‘carrier-pre’, ‘article’, ‘noun’, ‘adjective’, and ‘carrier-post’ seems like a good approach, but then it is not clear how the interval of interest (between ‘carrier-pre’ and ‘carrier-post’) is analyzed and modeled, other than the adjective. From the script 'stats.R', it appears that convergence issues are the main reason the linear models didn't include the noun, but if so, why include random intercepts only for the adjective rather than the actual target noun? Is this purely a model fit/data sparsity issue, or is there a motivation for including random effects of adjective, but not nouns? What about the determiner, and the whole DP/NP?

Another issue not explained is how timing differences were dealt with in this analysis: are duration differences captured/bypassed by the Gaussians/MFCCs, or were the data time-normalized, or were temporal differences disregarded? Presumably the Kullback-Leibler metrics were used to compare utterances differing in the number of frames/dimensionality of the vectors, but how does this affect the quantified divergence, or doesn't this matter for this analysis? Figures illustrating the intervals of analysis would help to explain these issues, as would a more detailed report and more complete documentation of the statistical modelling used.

Materials on OSF repository should be anonymized if blind review was required, but scripts and code have all been signed by this team.",1
pervagor_adscensionis,95,75,80,publishable with minor revision.,"We didn't find any issues with the overall methods, but found the results difficult to interpret",The use of discriminative techniques means that the question is answered in a more indirect way - so acoustic effects are inferred from classification rates.,"Choice of variables seemed good, but structure of statistical model doesn't directly answer the research question (but does address it in an interesting way).","A very wide range of acoustic features included, but unclear what each of the features actually represented. We also found it difficult to interpret the final result when referring only to DCT coefficients.",We're not familiar enough with this type of statistical modelling to comment on the specific structure of the model,Appropriate,"Lots of transformations applied to the data which in principle make sense, but in practice make it difficult to interpret the results",Beautiful R Markdown write up,2
pervagor_adscensionis,31,37,29,publishable with major revision.,"The authors firstly acknowledge the non-normality of the residuals and their measures but do not take any measures to correct for this. Secondly, the entire data is analyzed for typicality, when typicality was only manipulated in the NF condition, so it is unclear how the analyses reflect these differences.",We are not familiar with the type of analyses used in this report (the distance measures) so do not have comments on the technical soundness of the analyses. The lmer analyses are not appropriate for the measures chosen because of the non-normality of residuals.,We are not familiar with MFCCs at all so cannot comment on this - but it seems that calculating for the full dataset when the typicality manipulation applied only to NF condition is weird.,NA,NA,All the data was used but no indication of how the absence of typicality scores was handled.,NA,We did not fully follow this team's analysis as it is not common practice in our field.,1
