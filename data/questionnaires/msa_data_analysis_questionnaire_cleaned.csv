StartDate,Q17,Q16,Q2,Q3,Q13,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11,Q12
2022-03-18 08:47:52,lasionycteris_altavela,https://osf.io/aes8n/?view_only=232b2de4e8254af5832d63baa2a9450a,"We used a combination of GitHub and OSF as our research workflow. Each member pushed and pulled from the Github (https://github.com/doomlab/many_speeches) which automatically synced with our OSF account (https://osf.io/ycn9r/). Further,  we set up bi-weekly meetings  (as needed) and discussed the selection of the dependent variable, the structure of the analyses, and how to prepare the information for sharing the final product.","The processing of the recordings and the extraction of acoustic measurements were done with Praat praat6209, R version 4.1.1, and rPraat v. 1.3.2.1 package.",Jason: Very familiar Erin: Super Duper Familiar Ryan: Slightly familiar,"We used intensity at the segmental level as our DV. Fundamental frequency, duration, and intensity are all commonly cited correlates of stress in languages such as English and German (Baumann et al., 2007; Braun, 2006; Braun & Ladd, 2003; Katz & Selkirk, 2011; Mixdorff et al., 2015; Sityaev & House, 2003; for a review, see Gordon & Roettger, 2017). We chose intensity both because it is a reliable indicator of 
stress and contrastive focus, and because of the difficulty extracting durational information from the raw recordings. We extracted peak intensity values from each sentence under the hypothesis that the target word would be subject to contrastive focus and likely show increased duration and intensity, as well as elevated F0.","The only IV we included was typicality (as a continuous measure). This was based on the question of interest (i.e., Do speakers phonetically modulate utterances to signal atypical word combinations?). We chose to use the continuous measure of typicality, over the categorical option, because we believed it provided more information in the continuous form to explore information about the nature of typicality across the spectrum. Further, this provided one parameter estimate (b) for typicality predicting our DV, rather than categorical comparisons between each level of typicality. We also included the random intercept of participants (filename below) to control for the repeated measurement of participants which was cross classified with the item target name. Intensity ~ typ_mean + (1|target_name) + (1|filename) + (1|filename:target_name)","IV: Typicality was treated as a continuous measure, as provided in the dataset (0 to 100 typical features of the object). 

DV: We extracted intensity in dB from the segment level. We broke down each segment into (approximately) milliseconds to find the maximum intensity for the segment. 
",We did not transform any of our variables.,"We excluded any trials that were marked as ""error""","We used p < .05 as our criteria. 
","Zero 

< .001 [.000, .999]
",Pseudo-R2 < .000,"We originally discussed using typicality as a categorical variable. This model was analyzed, but then we figured out the typicality mean was included in the data, and we switched to the more descriptive version of typicality. Additionally, we discussed that the team likely wanted one parameter estimate, but the categorical option would create multiple parameter estimates (i.e., 3 b values if you compared all versions of typicality)."
2022-03-27 02:31:39,petauroides_fistulator,https://osf.io/k5bxr/?view_only=8d16018b339f4db58584bec4a89ccd0e,"Praat: multiple splitting of TextGrid and audio files; extraction of pitch information in Hz at 10 ms intervals
R: extraction of the TextGrid ‚Äòword‚Äô tier entries; creation of utterances as text files
webMAUS: automated transcription and segmentation of utterances (as created by R)",We used a Gaussian generalised additive mixed model as statistical technique.,"Fairly familiar, spent some years using it","We chose fundamental pitch, F0. The rational of this choice was that if intonation was different due to differences in typicality, this should become visible in the overall pitch contour.","We included a number of independent variables:
-	as parametric predictor: TYPICALITY
-	as smooth term predictor: ID
-	as random smooth terms: TRIAL, SPEAKER, TARGET_NAME, TARGET_COLOUR, COMPETITOR_NAME, COMPETITOR_COLOUR, DISTRACTOR1_NAME, DISTRACTOR1_COLOUR, DISTRACTOR2_NAME, and DISTRACTOR2_COLOUR

We chose to include as many variables as possible to account for potential effects of these variables. Including especially names and colours of non-target objects, we took into account effects which might otherwise stayed hidden. Including ID as smooth term, we accounted for the natural order of the pitch values we extracted. TYPICALITY, finally, was the predictor of interest.","Dependent variable: F0 was extracted in Hz for each target word (i.e. the noun) at intervals of 10 ms. Before the statistical analysis, F0 was transformed to semitones with a baseline frequency of 50 Hz. For each target word, F0 was measured at 10 ms intervals, starting at the onset of the word. Thus, there are multiple data points per target word.

Independent variables: ID was included to account for the natural order of the pitch values we extracted.

All other variables (as mentioned in Q5) were included in the way they are provided in the original materials. No changes have been made as we found that no changes were necessary.","Pitch in Hz as extracted by the Praat script was transformed into semitones using the hqmisc (Quen√©, 2014) package‚Äôs f2st function in R, with the default baseline frequency of 50 Hz. The pitch variable in Hz is given as ‚Äòpitch‚Äô, the pitch variable in semitones is given as ‚Äòpitch_st‚Äô.

Other variables were not transformed.
",All observations of the ‚ÄòNF‚Äô condition were included.,We relied on p-values as obtained by multiple comparisons of the variable of interest (TYPICALITY) as contained in our GAMM. We aimed at a conservative alpha-level of 0.001 (instead of the common 0.05) to speak of significance.,"We are not aware of a widely accepted concept of effect size in GAMS, or an appropriate method thereof. We report here the estimated means and their CIs of the TYPICALITY variable‚Äôs levels:
typical: 20.39079 [19.43039, 21.35119]
medium: 21.01482 [20.05419, 21.97545]
atypical: 18.80542 [17.95549, 19.65535]","Connected to Q10, our values are given in (estimated) semitones.","To our own surprise, no roadblocks occurred."
2022-03-30 18:55:29,pervagor_meeki,https://osf.io/sfhn5/?view_only=a8cec2e3e26043f2ab0218c7a02694f2,"We force-aligned the TextGrid word boundaries with the Montreal Forced Aligner (MFA, v1) (McAuliffe et al., 2017) with the pre-trained acoustic Prosody Lab German model and the ProsodyLab German Dictionary. 

We then wrote a custom Praat code to recombine the force-aligned TextGrid output with the original TextGrid (indicating Condition, Sentence Number; replacing the erroneous phone/word tiers with those from the MFA output). We also wrote a custom script to slow down the wavfile and TextGrid for one particularly fast talker (Subject ‚ÄúAL‚Äù); after running the slowed soundfile/TextGrid through MFA, we wrote another custom script to speed up the MFA TextGrid to match the original for the speaker. This speed manipulation was only needed for the one subject (AL).

Average acoustic measurements over each word were acquired through Praat, using a custom analysis script.
","Each acoustic feature was separately analyzed through linear mixed effects models, using the lme4 and lmerTest packages (Bates et al., 2015; Kuznetsova, Brockhoff, & Christensen, 2017) in R (version 4.1.2, R Core Team, 2021). Separate models compared the acoustic features of the noun and the color adjective of the NF trials.

The models on the nouns included a fixed effect of Typicality (Typical, Medium, Atypical; treatment coded, reference = Typical) and by-Speaker random intercepts and slopes for Typicality. The models on the adjectives additionally included by-Word random intercepts and slopes for Typicality. In case of non-convergence or singular fit, the random effects structure was simplified by iteratively removing the random effects associated with the least amount of variance (Barr et al., 2013). Each model was run twice (reference = Typical or reference = Medium) in order to produce estimates for all contrasts.
",Very familiar (primary method for analysis),"The dependent variables were mean and max f0, mean intensity and duration for each word (nouns and adjectives in NF), as all of these acoustic features have been found to correlate with focus (Breen et al., 2010; Lam & Watson, 2010; Roettger et al., 2019). 
","The only predictor we used was the Typicality category assigned to each color-noun pair, as this was the primary manipulation in the study. 

We originally intended to include the focus condition (NF vs AF) as an additional predictor, but decided not to due to limitations of the study design (see answer to question 11).
","Dependent variables: 
Average mean and max f0, intensity (in dB) and duration were extracted at the word-level, from the nouns and color adjectives in the NF trials. Similarly to previous studies (e.g., Lam & Watson, 2010) we used a single average acoustic measurement over each word, rather than over individual vowels, because different target words in this experiment (the focused nouns) contain unequal numbers of vowels. 

Only pitch measurements within the plausible range of 78 to 300 Hz were used to calculate mean and max f0 over each word.

Independent variable:
Typicality included three levels: Typical, Medium and Atypical. Each color-noun pair in the NF condition had been already categorized as part of one of the three levels by the study coordinators based on the norming study. Typicality was treatment coded, with the reference level = ‚ÄòTypical‚Äô.
","Pitch was converted to semitones (ST; relative to 75 Hz) with the hqmisc R package (Quen√©, 2014), so that f0 would be on a linear scale. 

In our dataset, non-transformed f0 measurements (in Hz) are found under the column names ‚ÄúMean_f0‚Äù and ‚ÄúMax_f0‚Äù. Transformed f0 measurements (in ST) are found under the column names ‚ÄúMean_f0_st‚Äù and ‚ÄúMax_f0_st‚Äù.
","Only sentences in the critical Noun Focus condition were analyzed. Sentences that had been marked as containing a speech or structure error, noise or hesitation were excluded from the analyses.

Acoustic measurements exceeding 3 standard deviations from each speaker‚Äôs mean were excluded from the analyses.
","The decision was based on beta coefficients from the mixed effects models (which also represent simple effect sizes) and the corresponding p-values, for each contrast among the three Typicality levels.
","NOUNS
Measure	Contrast	Effect size (beta)	95% CI
intensity	atypical - typical	-1.06	[-1.57, -0.55]
intensity	medium - typical	-1.54	[-2.05, -1.03]
intensity	atypical - medium	0.48	[-0.04, 0.99]
duration	atypical - typical	-80.67	[-98.54, -62.79]
duration	medium - typical	33.02	[15.26, 50.78]
duration	atypical - medium	-113.69[-131.59, -95.8]
meanf0	atypical - typical	0.94	[0.51, 1.36]
meanf0	medium - typical	0.41	[0.02, 0.81]
meanf0	atypical - medium	0.53	[0.11, 0.94]
maxf0		atypical - typical	0.77	[0.43, 1.1]
maxf0		medium - typical	0.77	[0.44, 1.1]
maxf0		atypical - medium	-0.01	[-0.34, 0.33]


ADJECTIVES:
Measure	Contrast	Effect size (beta)	95% CI
intensity	atypical - typical	-0.01	[-1.3, 1.28]
intensity	medium - typical	-0.48	[-1.3, 0.35]
intensity	atypical - medium	0.47	[-0.7, 1.64]
duration	atypical - typical	3.98	[-45.55, 53.51]
duration	medium - typical	-9.65	[-52.58, 33.28]
duration	atypical - medium	13.63	[-23.73, 51]
meanf0	atypical - typical	0.16	[-0.14, 0.45]
meanf0	medium - typical	0	[-0.3, 0.29]
meanf0	atypical - medium	0.16	[-0.14, 0.46]
maxf0		atypical - typical	0.27	[0.04, 0.5]
maxf0		medium - typical	0.06	[-0.17, 0.29]
maxf0		atypical - medium	0.21	[-0.02, 0.45]
","We chose to report simple effect sizes rather than a standardized measure of effect size (e.g., eta squared), following Baguley (2009). Simple effect sizes (i.e. unstandardized regression coefficients) are easier to interpret in relation to each dependent measure of interest, and are automatically computed as the estimated beta coefficient of treatment-coded contrasts in mixed effects models. Thus, the effect sizes are in the units of each acoustic measure (Intensity: dB; Duration: ms; Mean/Max f0: ST).

This method allowed us to report an effect size for each contrast of interest (e.g., Atypical vs. Typical), while common methods to acquire standardized effect sizes (e.g., eta squared using the R package effectsize; Ben-Shachar et al., 2020) only lead to a single, less interpretable value for the effect of Typicality across all contrasts. The 95% confidence intervals for the simple effect sizes were calculated using the R package emmeans (Lenth et al., 2019).","We had to make two changes to our planned analyses, resulting from limitations of the production study design.

First, we initially intended to include by-Word random effects for the noun models to account for differences in baseline acoustic measurements (e.g., certain words being overall longer than others). However, we did not ultimately include this random effect for the noun models because, contrary to the color adjectives, target nouns were not fully crossed with Typicality (each noun was only associated with one Typicality level). 

Second, we originally intended to compare Noun Focus (NF) and Adjective Focus (AF) conditions to determine whether the effect of typicality on the noun and adjective varied as a function of whether the noun or the adjective were in focus. However, in the end we only analyzed NF trials because the data were not balanced. In the AF trials, the nouns were not counterbalanced across the 3 Typicality categories (typical, medium, atypical), but rather only contained atypical food color-noun combinations and some medium-typicality non-food nouns.

We suggest that in the final paper resulting from this study, the study description in the method should be revised to more clearly differentiate between the designs of the NF, AF and ANF conditions, as the current study description and examples used (e.g., ‚Äúyellow banana‚Äù vs ‚Äúblue banana‚Äù for the AF condition, which were not actual items used in AF) led us to misinterpret the design.
"
2022-04-06 04:22:09,aracana_bitatawa,https://osf.io/u5sx6/?view_only=dc3af8fe09eb49d1b90405bb9a73f260,"The extraction of acoustic measurements (F0, duration) was accomplished with Praat.",We run Quantile Generalized Additive Mixed Effects Models.,Very familiar,"We used 
*duration of the adjective and
* F0 
as the dependent variables. Reasoning: these have been mostly found to correlate with lexical predictors such as frequency or probability -- measues that can be regarded to gauge typicality. 

We did not use other dependent variables such as formants of the stressed vowel because the adjectives were not controlled for phonetic characteristics (which is technically also not possible).","The predictor of interest was typicality -- the target of this analysis. 
We also controlled for and tested  
* frequency of occurrence of the adjective-noun phrase -- which can be regarded to gauge typicality
* number of syllables of the following noun -- to account for potential duration variation in an articulatory frame
* speaking rate -- to account for contextual velocity changes affecting both, F0 and the duration of the adjective. 
* when testing F0, we also controlled for the duration of the adjective -- to account for effects of duration onto F0","dependent variables
-----------------------------
* duration - duration of the adjective, gradient variable
* F0 - extracted as a trajectory along the entire adjective, gradient variable, z-transformed across participants

independent variables
--------------------------------
* frequency of occurrence -- google count, gradient variable, log transformed
* number of syllables of the following noun -- numeric
* speaking rate -- number of syllables in entire phrase divided by the duration of the phrase in seconds","* frequency of occurrence (adjective + noun) was log transformed --> column L.Freq
* F0 was  z-scaled --> column c.F0",We used all observations.,"We used z-values provided by the qgam analysis, with absolute 2 as the significance threshold.","We have performed an analysis that gives an effect for a continuum of percentiles of the dependent variable. This is why we obtained a set of effect differences. On average, we our effect size is negative and smaller than 0.002 seconds (standard error of 0.01).",Seconds.,No.
2022-04-06 10:05:21,hoplostethus_macrosteus,https://osf.io/csujh/?view_only=3e4c9e83560d4811940b8fc4d1f65fef,"First, the target (stressed) vowels were segmented by hand in Praat. Then, a Praat script was used to extract the acoustic measurements of duration and intensity of target vowel.",We ran linear mixed-effects regression models in R using duration (in milliseconds) and maximum intensity (in decibels) as our continuous dependent variables.,Very familiar,"Our dependent variables were duration and intensity of the target vowel. We chose these variables because they are two of the few vowel variables that do not differ between male and female speakers. Since we were not provided with the gender of the participants, we did not feel we could use f0 or vowel quality, as both of these variables (and their measurements) depend heavily on vocal tract size. 
","We included ‚Äòtypicality‚Äô, ‚Äòcolor‚Äô, and ‚Äòobservation‚Äô as independent variables (fixed effects) and ‚Äòparticipant‚Äô as a random effect. We included ‚Äòtypicality‚Äô to respond to the research question of whether speakers acoustically modify their speech with different color + noun typicality combinations. We included color in order to examine the differences in acoustic realization between the five colors. We also included Observation, a numeric variable assigned during data collection, as a possible task effect of where in the experiment the token appeared. Given what we know about variation in duration based on vowel identity, we tested models that included an interaction between color and typicality, so we could evaluate each color‚Äôs typicality separately. Finally, we included participants as a random effect to account for interspeaker variation.","Because the stressed syllable of the adjective started with either a liquid (gr√ºn, roten, braun, orangen) or a stop (gelb), we determined that measuring consonantal features such as VOT would be inappropriate. Therefore, we chose to segment the stressed vowel of each adjective. The target nuclear vowel (that of the stressed syllable in each color word) was segmented by hand using the following criteria for onset and offset of the vowel, primarily relying on oral consonantal constriction events (as recommended by Turk et al. 2006:3).
Onset:
In the case of vowel nuclei preceded by a liquid, we relied on a change in waveform amplitude between the preceding voiced liquid and the vowel nucleus, as well as clear onset of energy in the vowel‚Äôs second formant. For vowels that followed stops, we again relied on an increase in waveform amplitude, periodicity in the waveform, and clear formant energy to mark the onset of the segment (the stop burst was not included in the vowel nuclei segment). In the few instances of mismatch among these three parameters, we particularly relied on the onset of the second formant (following Turk et al. 2006) to identify oral stop closure releases. 

Offset: 
To determine the offset of the vowel nuclei, we utilized both the waveform and spectrogram and placed the offset of the vowel at the point at which amplitude in the waveform decreased accompanied by cessation of formant energy (relying specifically on the second and third formants (following Turk et al. 2006:7) when there were instances of formants offsetting at different times). 

Following nasal (grun, braun, orangen): The nasals were often accompanied by abrupt spectral changes at closure onset (cf. Turk et al. 2006:12), particularly in the middle and higher-frequency regions due to nasal antiformants, and a decrease in waveform amplitude.
Following stop (roten): The offset of the vowel was marked at the oral stop closure, indicated by a cessation of formant energy and a decrease in overall amplitude.
Following liquid (gelb): Many times there was no clear decrease in waveform amplitude, signaling a deleted or coarticulated /l/ in the word ‚Äògelben‚Äô. In these instances, the offset of the vowel was marked at the oral stop closure [b].

Duration: The Praat script measured the duration of the segmented target vowel.
Intensity: The Praat script measured the maximum intensity of the target vowel. 
Typicality: Typical, medium, atypical
Color: Gr√ºn, Gelb, Ro:ten, Braun, Orangen 
","We centered and scaled the duration and intensity_maximum variables to ensure normality of our data distribution. 
","During our segmentation process, we noted that some of the trials were marked with ‚ÄúError‚Äù codes on the textgrid and excluded these observations from our analysis. In addition, there were some trials that were marked with ‚Äúhesitation break‚Äù. If the hesitation break affected the target color + noun combination, we excluded the observation, and if it did not affect the target, we included the observation. 
Additionally, since we had normalized the duration and the intensity measures, we excluded any values that were +/- 2 standard deviations from the mean (in the case of duration, this excluded N=30 observations, leaving 830 observations, and in the case of intensity, N=33 observations were excluded, leaving 827 observations for the analysis. 
","We used p-values (alpha value of 0.05) to determine whether there was a significant effect of the independent variables on duration or intensity. 
We compared the AIC values of the models to determine the model with the best fit. 
Additionally, we calculated the marginal and conditional R-squared values of each of the models (from Nakagawa and Schielzeth (2012), using the performance R package (L√ºdecke et al. 2021)). The ‚Äúwinning‚Äù model in each case was the model with the lowest AIC and the largest marginal and conditional R-squared values.",We found no significant main effects.,n/a,"We determined early on that we would not be able to use f0 or vowel quality, two of the predictors we thought would be most influenced by typicality, without making assumptions about the gender of the participants. We decided that it was not a sound decision to make this assumption and thus were not able to include these predictors. 
We encountered a couple of issues with segmenting which were resolved with our inter-rater reliability checks: 
At times the vowel was nasalized. 
Because of the variation in liquid realization, segmenting words such as ‚Äúbraunen‚Äù, ‚Äúorangen‚Äù, ‚Äúrot‚Äù, and ‚Äúgelb‚Äù proved challenging at times. 
"
2022-04-08 14:37:53,psittacula_scabriculus,https://osf.io/ge7fm/?view_only=5bbf60695804487d9c6862f4798ebe7b,Segmentation of the data was conducted in Praat. The team member created an additional interval tier in each of the received text grid files and segmented and marked on it the vowels of the nouns in the NF trials.,We run Bayesian mixed-effect models using a Gaussian likelihood as the distribution of the outcome variable,Novice,"Vowel duration. The reason for choosing vowels for the investigation of the effects of typically was based on previous research (Bell et al., 2002; Bell et al., 2009; Bybee & De Souza, 2019; Jurafsky et al., 2000; Seyfarth, 2014). that predictability has an influence on vowels' qualitative and quantitative characteristics.","Typicality. Speakers are often overinformative, i.e. they use referring expressions that are more specific than necessary. This redundancy has been argued to facilitate object identification and more efficient communication (Arts et al. 2011, Paraboni et al. 2007, Rubio-Fernandez 2016). For example, Degen et al. (2020) show that modifiers that are less typical for a given referent (e.g. a blue banana) are more likely to be used in an overinformative scenario (e.g. when there is just one banana). As a result, atypical objects would be used in a more overinformative scenario than typical objects.","Average vowel duration was computed by averaging individual vowel durations per word for that specific speaker in that condition. For instance, <tomate> in an atypical condition has three vowels 'o', 'a' and 'e' with the duration being 0.05, 0.06, 0.05, so the average duration would be 0.053.
Typicality was operationalised as a three-level factor: typical, medium and atypical.",No transformations were conducted.,No data exclusion,The decision was conducted based on credible intervals and if it crossed or overlapped with 0.,Atypical vs typical: 0.00[-0.04 0.05],Cohen's d,No
2022-04-08 12:56:27,procambarus_mahogoni,https://osf.io/52n8b/?view_only=9d22b89ad26f40288e115830e8718f79,The processing of recordings and the extraction of acoustic measurements were done with Praat.,We fitted 3 linear mixed-effects models using the lme4 package in R.,We are all very familiar with the lme4 package in R.,"The dependent variables were duration, pitch, and intensity. This was done because those three measurements have been shown to be correlates of word stress (Gordon & Roettger, 2017).",The independent variable was typicality median. We chose this because of the research question that asked if speakers phonetically modulate utterances to signal atypical word combinations.,"For the independent variable, we treated it as a continuous measure as was provided in the dataset. For the dependent variable, we extracted duration in milliseconds, intensity in dB, and pitch in Hz.","We standardized duration, intensity, and pitch values.",N/A,We used p-values below 0.05 as a means of answering the research question.,"None of the models revealed significant p-values nor large or medium effect sizes. From duration, pitch, and intensity on typicality all effect sizes were small.",Cohen's D,N/A
2022-04-11 01:47:25,stygobromus_tyraica,https://osf.io/4jzgd/?view_only=8730bd46a96a4626a54e23f023cffa54,"Word- and phone-level forced alignment was performed using Montreal Forced Aligner v2.0, using the pretrained (Prosodylab) German acoustic model and accompanying pronunciation dictionary (edited to include alternative transliterations in the TextGrids, e.g. gr√ºnen/gruenen).

Further trimming of recordings was accomplished in Praat.

Acoustic features were extracted using the openSMILE 3.0 toolkit and the ‚Äúextended Geneva Minimalistic Acoustic Parameter Set‚Äù (eGEMAPS) v02, which comprises 25 acoustic features measured at 10 ms intervals. These features were extracted using the reticulate R package interface to the openSMILE Python package.","We trained an extreme gradient boosting model to perform a multiclass classification task, using the XGBoost R package. The acoustic features were used as independent variables and a three-class representation of typicality for the outcome variable.","With the general approach (supervised machine learning classification task), quite familiar: 8/10. With the specific approach (extreme gradient boosting), moderately familiar: 6/10.","Typicality (categorical).

Rationale: If speakers do systematically modulate utterances, then there will be phonetic and acoustic consequences of this modulation. If there are acoustic consequences, then using relevant acoustic parameters to classify and predict typicality category should produce a greater-than-chance outcome. Using typicality as the outcome variable allowed us to incorporate multiple input features in our investigation of how the acoustic signal may be modulated, while maintaining a single effect size output.","We chose to incorporate all 25 low-level acoustic features from the openSMILE ‚Äúextended Geneva Minimalistic Acoustic Parameter Set‚Äù (eGEMAPS) v02 feature set, as well as duration, aimed at making use of the frequency, energy, spectral and temporal dimensions to capture acoustic patterns in relation to stress, accent and aspects of voice quality.","Categorical dependent variable (typicality: ‚Äúatypical‚Äù, ‚Äúmedium‚Äù, ‚Äútypical‚Äù): operationalized as zero-based ordinal factor.

All acoustic independent variables: extracted at the word level, at 10 ms intervals across the duration of the modifier adjective.

Duration (independent variable): extracted at the word level.

Acoustic features were included for only the adjectives (i.e. the color words), since these words are the same across the three typicality conditions whereas the nouns are not.","We performed discrete cosine transform (DCT) on all acoustic variables except duration, over the interval of the entire word, and retained the first four coefficients (named with the suffixes x0-x3). This resulted in 101 features for each speaker (4 DCT coefficients x 25 openSMILE features, + duration). These 101 features were centered and scaled (i.e. z-score transformed) for each speaker.","We excluded trials with non-empty ‚Äúnotes‚Äù in the provided TextGrids (indicating errors, hesitations, etc.). No other observations (e.g., acoustic feature outliers) were removed.",p-value from one-tailed binomial test of whether model accuracy is higher than the no-information rate.,"15.37% [11.95%, 18.79%].",(Percentage) accuracy above chance level (i.e. the no-information rate).,"Initially, we used random sampling for tuning the values of the XGBoost hyperparameters. After further consideration, we came to the conclusion that this approach is anti-conservative. We then changed the approach to use random speaker-wise folds, in the same manner that was applied to building the final models."
2022-04-11 09:13:53,eriphia_laterispinis,https://osf.io/q8ru2/?view_only=e0ddb34a25ef42c5bfb5a0068f2b99cf,"The acoustic measurements were extracted using the PraatR, rPraat, and phonfieldwork packages (Albin, 2014; Bo≈ôil & Skarnitzl, 2016; Moroz, 2020) in R version 4.1.2 (R Core Team, 2020).","We fitted a multivariate (i.e., with multiple outcomes) multilevel Bayesian regression model using the brms package (B√ºrkner, 2017).",Quite familiar (wrote some tutorials about it).,"We chose to focus on three acoustic measurements: the F0, the intensity, and the duration. As the time alignment for the words tier was not available, we computed the average F0, intensity, and duration of the whole sentence.","We only used the typicality categories from the norming study as predictor. Because we were interested in assessing the effects of typicality, we chose to remove medium-typical items to focus on the comparison between highly atypical and highly typical items (included as a binary predictor in the model).","We chose to focus on three acoustic measurements: the F0, the intensity, and the duration. As the time alignment for the words tier was not available, we computed the average F0, intensity, and duration of the whole sentence.","Because speakers have very variable baseline levels of F0 or intensity, our three acoustic measurement were standardised (i.e., subtracting the mean and dividing by the standard deviation) per speaker.",We did not exclude observations.,"We used both the standardised regression coefficient (the slope), its 95% credible interval, a standardised mean difference, and a BF to answer the research question.","Our analyses revealed a negative effect of typicality on the average F0, with atypical items having a higher average F0 than typical items (ùõΩ = -0.469, 95% CrI [-0.626, -0.313], ùõøùë° = -0.479, 95% CrI [-0.642, -0.317]), but a positive effect of typicality on the average intensity (ùõΩ = 0.308, 95% CrI [0.145, 0.469], ùõøùë° = 0.304, 95% CrI [0.142, 0.464]) and average duration
(ùõΩ = 0.274, 95% CrI [0.116, 0.434], ùõøùë° = 0.276, 95% CrI [0.116, 0.436]).","The effect size is a standardised mean difference (computed from the model's estimates, see the RMarkdown document for code details).","Yes. We initially planned on conducting our analyses at the word level, but then realised than the time alignement was not available for words. Because we had not enough time (or skill) to perform the alignement ourselves, we decided to focus on the sentence level."
2022-04-11 13:50:21,trapezia_cantonensis,https://osf.io/8efv2/?view_only=433cd82c91734325aa15d408738ed522,"All acoustic processing was done via Praat. Text was aligned to the audio manually. Acoustic measurements (F1, F2, F3, intensity, duration, F0) were extacted using Fast Track, a Praat plugin.","I ran generalized additive mixed-effects models on F1/F2, intensity, and F0 contours. I ran a linear mixed-effects model on durations.",Pretty familiar. I've used them in multiple publications.,"I analyzed F1, F2, intensity, F0, and duration. 

My hypothesis was that atypical treatments would be realized louder, longer, with more intensity, and with more peripheral vowels than typical treatments. The is consistent with what is often found in stressed syllables (compared to unstressed) syllables in many of the world's languages. I wasn't sure if all four of these differences would be found, but I thought I'd extract all of them and check anyway.","Typicality was the only main predictor. Adjective and speaker were also included as random effects because I expect to find differences across those groups, but I'm not particularly in those differences.","Because I used FastTrack, I was able to extract F1, F2, intensity, and F0 data every 5ms along the duration of each vowel. This resulted in formant, intensity, and fundamental frequency counters. Duration obvious does not apply across time like that, so duration measures were calculated for each vowel token.",No transformations were done.,The only observations that were excluded were in the F0 analysis. Any observation with an F0 of 0 was considered bad data and excluded.,"For the GAMMs, I used model comparison between full and impoverished models. The full model contained typicality as a predictor and the impoverished model did not. The model comparisons consider AIC scores and p-values. In all cases, the p-value was small (as it typically is in GAMMs, I've noticed), so I just examined visualizations of predicted trajectories to check for consistent effects

For the linear mixed-effects models, I used p-values of the model estimates.",I don't know of a way to calculate effect sizes for GAMMs.,unknown,"The only major obstacle was that the data was in German. I am used to using English, so it was not possible to use the tools I normally use for automatic forced-alignment. The manual aligning was easily the most time-consuming step (I spread it out over a month). Once everything was aligned, the formant extraction and most of the analysis was completed in a couple hours."
2022-04-11 09:50:51,trigonias_lachneri,https://osf.io/q9b34/?view_only=22ec25cfefbf4c49bf009846fafc01a6,"Textgrid alignment and phoneme segmentation was done using WebMAUS (link below), splitting the recordings into utterances and acoustic measurements extraction were done using Praat v6.1.39 and Praat v6.2.06.

https://www.clarin.eu/showcase/webmaus-automatic-segmentation-and-labelling-speech-signals-over-web
","We used null hypothesis significance testing, using linear mixed effects models and full random structure, via R Studio (RCore Team, 2019) and the lme4 package (Bates et al., 2015)",Fairly familiar. Most authors have used LMM‚Äôs before and one of the authors has completed a summer school on statistics which specifically addressed LMM‚Äôs using R.,"In the context of the research question ‚ÄúDo speakers phonetically modulate utterances to signal atypical word combinations‚Äù we decided to focus on phonetic prominence as a potentially modulated aspect of the speech signal. This was motivated by the assumption that typicality would be related to predictability in context, and predictability has proven effects on prosodic prominence (e.g. Turnbull 2017).   Phonetic prominence was measured using  Tykalova et al‚Äôs ‚ÄúStress Pattern Index‚Äù (SPI), which is a composite measure of phonetic prominence, taking F0 variation, duration and relative intensity into account. We reasoned that a composite measure was well suited to combine some of the multi-variate effects of prosodic prominence into a single outcome variable.","As the research question focuses on the relationship between typicality and prosodic emphasis overall and only one effect size is of interest, we  used the mean typicality ratings as a continuous predictor and we report the overall relationship between the typicality rating and the prosodic prominence measure.","The DV phonetic prominence was operationalised using the SPI as described in [1] in our own implementation (see Praat script SPI_and_friends4anon.psc) The formula used was:
SPI = (1+ln(F0max/F0min))Œ£En. 
Minimal amplitude over the analysed unit was set to 0 dB.
SPI was originally devised as a word level measure and in accordance to that it was calculated over the total duration of the colour word in the NF condition.

The IV typicality was operationalised as a continuous variable, using the mean typicality ratings from the trial lists.

[1] Tykalov√°, T., Rusz, J., Cmejla, R., Ruzickova, H., & R≈Ø≈æiƒçka, E. (2014). Acoustic investigation of stress patterns in Parkinson‚Äôs disease. Journal of Voice‚ÄØ: Official Journal of the Voice Foundation.

",The SPI log-transforms F0. The composite SPI measure was log-transformed as a Shapiro-Wilk test indicated non-normal distribution.,We excluded all tokens with non-empty comment fields because having inspected the comments it was judged that in all the cases speaker prosody may have been affected by factors other than the experimental condition.),"P-value and 95% CI for the continuous predictor Typicality. P-value was derived using the functions contained in R package lmerTest [1].

[1] Kuznetsova A, Brockhoff PB, Christensen RHB (2017). ‚ÄúlmerTest Package: Tests in Linear Mixed Effects Models.‚Äù Journal of Statistical
Software, 82(13), 1-26.
","The effect size is based on Westfall, Judd, and Kenny (2014), as cited in Brysbaert and Stevens (2018) d =  -0.0003[ -0.002, 0.001]","Standardized mean difference = Mean difference divided by the square root of the sum of the random slopes, intercepts and residual.",We initially anticipated that the focus condition would be another predictor. It was later clarified that the ANF and AF conditions were not balanced so we only focused on the effect of typicality in NF.
2022-04-12 13:12:06,nestor_idahoensis,https://osf.io/923pa/?view_only=7e79bb53fcf7411c89a54f929c0cfbd9,"Study audio was standardized to a left-channel mono at a sampling rate of 44.1 kHz using Praat (Boersma & Weenink, 2019) and the Parselmouth-Praat API (Jadoul, Thompson, & de Boer, 2018) in Python. Utterances were then force-aligned to isolate the stressed vowel in the adjective of interest. Forced alignment was completed using the align function of the Montreal Forced Aligner v 2.0.0rc3 (McAuliffe et al., 2017), using Prosodylab pretrained models and dictionaries for German. The Prosodylab German dictionary was adapted to specifically reflect the combinations of morphemes present in the adjectives of interest. The force-aligned utterances were subset to the stressed phones in the adjectives of interest using Python libraries Parselmouth-Praat, and PraatIO (Mahrt, 2016). Fundamental frequency and formant values were estimated using a Python implementation of the Praat FastTrack plugin (Barreda, 2021), customized for the HTCondor framework (Thain et al, 2005) on the OrangeGrid computing environment at Syracuse University.",We ran one multivariate Bayesian mixed effects model using logarithmic likelihood as the distribution of the outcome variables.,"I would say I am fairly familiar with the statistical technique. There are always more to learn about using Bayesian statistics, such as specifying priors. However, I feel comfortable to run Bayesian mixed effects models with acoustic variables.","We chose four dependent variables which all included acoustic measures of the stressed vowel of the target word: fundamental frequency, duration, first formant frequency, and second formant frequency. We chose fundamental frequency and duration as indicators of changes in prosodic production of the target word. If there is a difference by typicality, then speakers are likely to emphasize the target word using fundamental frequency and duration. We would typically add intensity as a measure as well but we decided not to include it in this analysis since we didn't have information about microphone distance. We also included the first and second formant frequency to determine whether speakers used hyperarticulation to signal typicality.","The only predictor we chose was categorical typicality. We thought about running a separate analysis with the quantitative typicality measure. However, the researchers categorized typicality in the datasheet so we decided to stick with the designated categories. We chose to analyze typicality because the purpose of this analysis was to determine if speakers would alter word production based on typicality. It therefore made sense to include typicality as the predictor of the acoustic production of the target word.","We operationalized our four dependent variables as the stressed vowel of the target word. There is evidence that when speakers alter the production of a word for emphasis, they mainly alter the stressed vowel of that word. Additionally, acoustic measures such as fundamental frequency are more reliably measured from vowels rather than consonants. The first and second formant frequencies were measured from the midpoint of the stressed vowel which reduces the chance of including changes in production due to coarticulation.","All dependent variables were log transformed in the statistical model by specifying a logarithmic distribution for the outcome variables. To do this, I specified the logarithmic distribution in the family function in brms.","Force-aligned phone intervals shorter than 50 ms did not meet our duration heuristic for plausibility and were excluded from further analysis. Candidate formant tracks not meeting heuristic standards (e.g., F1 > 1200 Hz | F2 < 600 Hz) were penalized during the selection step.",We used two criteria for decision making: Bayesian credible intervals and probability of direction. An effect was considered robust if the credible interval excluded zero and if the probability of direction was greater than 95%.,The effect size was a 95% credible interval and 95% probability of direction.,Standardized mean difference.,"There were a few data preprocessing steps we had to add in that we originally did not account for. For example, we realized that to run the forced aligner, we needed a spreadsheet with the filename in one column and the target German words in another column. Extra steps like this had to be delegated and planned out along the way."
2022-04-12 12:37:50,sphyrna_ellioti,https://osf.io/z3jhe/?view_only=bfeb0f65bf594d0885b2d40b30ae826c,"Word-level transcriptions were done by preprocessing the provided TextGrids with Praat scripts and Python and using the german_prosodylab pronunciation dictionary and acoustic model in Montreal Forced Aligner.

Extraction of prosodic cues was done by using a Praat script.","We ran linear mixed effect models, and simplified the random effects structure to aid with convergence and to decrease correlations between terms.","Very familiar, although I do not consider myself an expert.","Our dependent variables were log duration, intensity, and F0 for the critical words (the adjective and noun in the sentence). The rationale behind choosing these words was that these were the manipulated words, and it might be easier to see differences in how something is being said between conditions by narrowing down our analyses to these words. Adjectives and Nouns were both analyzed, as either could carry focus depending on condition. The cues that were selected are cues that are often argued to be acoustic correlates of focus. Duration was log transformed to make duration values more normally distributed. F0 values were extracted at the word onset, midpoint, and offset so that they may serve as a coarse grained approximation for F0 contours for each word.","The independent variables were the continuous typicality score, the experimental condition for each sentence, the part of speech status of the word that is being analyzed, and the interactions between these terms. 

The continuous typicality score was chosen for 2 reasons: firstly, it offers a more detailed measure of typicality than the binned categories, and secondly, not all conditions were equally represented in all binned categories, but they all did have some continuous typicality value. The experimental condition (focus location) was explored because we thought it might interact with the typicality of the phrase. Similarly, the part of speech status was included since the experimental condition should determine which word carries focus and therefore might affect the prosodic cue values for each word depending on condition.","All dependent variables were extracted from the word level of the critical adjective-noun pair. Duration values were log transformed to normalize their distribution. All F0 values were centered on a by-talker basis so that rises and falls might be treated similarly even if talkers had different baselines in pitch. F0 values were extracted from the onset, midpoint, and offset of each of the 2 words in the critical adjective-noun pair. 

For the independent variables, part of speech was a categorical variable that specified whether the analyzed word was the adjective or the noun from the critical adjective-noun word pair. No other words were analyzed. The typicality measure used for the analyses was the mean typicality score for the adjective-noun pair provided in the norms. The experimental condition investigated included 3 levels, depending on what word was meant to carry focus: Adjective Focus, Noun Focus, and Dual Focus.","Duration was log transformed to make values more normally distributed. The transformation was done in R when performing the analyses, but can be found in the ManySpeech_Preprocessed_Data.csv file as LogDur. 

F0 values were centered based on the talker's mean F0 value for all words for each location. These values can be found as cfo1 (onset), cfo2 (midpoint), and cfo3 (offset) in the ManySpeech_Preprocessed_Data.csv file.",Any words that were not part of the critical adjective-noun pair were excluded from analyses. No other exclusions were made.,P values for the main effect of typicality mean score or any interactions that included the typicality mean score.,"Cohen's F for Log duration:
typicality mean	0.33	[0.000, 0.807]
POS * typicality mean	0.477	[0.000, 0.966]
condition * typicality mean	0.168	[0.000, 0.411]
(POS * condition) * typicality mean	0.317	[0.000, 0.581]

Cohen's F for Intensity:
typicality mean	0.429	[0.000, 0.887]
POS * typicality mean	0.283	[0.000, 0.731]
condition * typicality mean	0.223	[0.000, 0.443]
(POS * condition) * typicality mean	0.192	[0.000, 0.407]

Cohen's F for Onset F0:
typicality mean	0.144	[0.000, 0.410]
POS * typicality mean	0.048	[0.000, 0.302]
condition * typicality mean	0.074	[0.000, 0.172]
(POS * condition) * typicality mean	0.038	[0.000, 0.123]

Cohen's F for Midpoint F0:
typicality mean	0.045	[0.000, 0.414]
POS * typicality mean	0.061	[0.000, 0.447]
condition * typicality mean	0.147	[0.000, 0.357]
(POS * condition) * typicality mean	0.084	[0.000, 0.270]

Cohen's F for Offset F0:
typicality mean	0.496	[0.138, 0.847]
POS * typicality mean	0.335	[0.000, 0.676]
condition * typicality mean	0.086	[0.000, 0.214]
(POS * condition) * typicality mean	0.164	[0.000, 0.302]",Cohen's F,"I was initially expecting typicality to have been independently manipulated independent of the condition. I noticed while exploring the data that this wasn't the case. This limited my options as to how I could analyze the data. Although I don't think the analysis is optimal in this case, I still would have had a preference to include the continuous typicality mean score rather than the categorical typicality variable if possible."
2022-04-13 02:37:42,procambarus_maculosus,https://osf.io/sfwk6/?view_only=07668f1993c545018e85d24712b43004,"Praat (script): tier extraction, pre-segmentation; R, emuR package/BAS WebServices: automatic segmentation; wrassp package: f0 calculation; fda package: FPCA analysis; MuMin, emmeans and lmerTest packages: LMER analysis","Functional Principal Components Analysis to decompose the signal in principal components, describing main patterns of variation of the signal's position - i.e. frequency height - and shape, in particular its tilt. The PC-scores were submitted to LMER, Pseudo-R2 were calculated in order to quantify and compare the amount of variance captured by the LMER models.",Quite confident and familiar with these methods (already employed for other analyses and papers).,"F0 was the variable analysed in our analysis, since there are some studies indirectly suggesting that a higher pitch might correlate with atypicality. The PC-scores connected to the PCs (i.e. main f0 variations) were chosen as dependent variables for the LMER models (one score per PC and token analysed in the dataset).","Typicality, Sex as fixed factors, Word and speaker as random factors (in order to filter out variation due to these components).

Typicality referred to the main research question, while Sex is also highly correlated to pitch height, this si why it also had to present so as to give non-biased results.","F0 extracted from the vowel nuclei of the primary stressed syllables of the nuclear accented (i.e. target) words. The F0 trajectories were time-normalised, so that the whole trajectories could be submitted to FPCA. Sex had two levels (M/F). The random factors Word and Speaker had as many levels as the number of speakers/words elicited.","F0 values were extracted by means of the ""ksv"" algorithm (Schaefer-Vincent, 1983). The F0 signals were then submitted to FPCA. This type of functional analysis linearly decomposes the input signals f 0i(t) and returns (i) K time-varying Principal Components PCkf 0 (t) which capture independent modes of variation in the signal shapes, (ii) PC scores sk,i, one per PCk and signal i, which modulate the PCs differently for each
signal, and (iii) the mean f0 signal Œºf 0 (t) . Formally:

f 0i(t) ‚âà Œºf 0 (t) +K‚àëk=1sk,i ¬∑ PCkf 0 (t)","Two audio files had to be excluded from the analysis; one because the target word was produced incorrectly Tirsche instead of Kirsche, and the other one because there was a loud noise during the target word.

The extracted F0 signals were cleaned up as follows: First, zeros in the middle 50\% of the vowel (which we interpreted as measuring errors) were replaced by the previous non-zero F0 value, in order to avoid that these measuring errors had an impact on the analysis. Second, zeros in the first and last 25\% of the F0 signals were removed without replacement. This was because the MAUS segmentation was not always accurate, so that parts of the surrounding voiceless sounds were included by MAUS in the vocalic segment.","Shape variation captured by FPCA; amount of variance explained by LMER models (Pseudo-R2), p-values of fixed factors (and their interactions) in LMER models (i.e. significance probability).",The effect on the s1 score describing height shift of F0 was non-significant. The direction of the change was a raising of the pitch of around 10Hz to max. 15Hz for both males and females.,Pseudo R2 (marginal and conditional) applied to the output of the LMER models,"The main problems were connected to the fact that the database was quite unbalanced, so for instance we did not know if it was the case to consider the vowel types as fixed factors or not. We also thought of analysing just women, in order to filter out male/female differences in pitch (but since the results were quite similar, at the end we decided to include all speakers)."
2022-04-12 02:06:40,gnathosaurus_canadensis,https://osf.io/5as6b/?view_only=529066020c654a1fa6879736be119261,"For acoustic measurement (f0), I used Praat with the following settings: Spectrogram settings left on default (view range 0 - 5000Hz), Window length 0.005 s, dynamic range 55.0 dB, pitch settings for male speakers (i.e., low f0), pitch range 75 - 250 Hz, for female speakers (high f0), pitch range 120 - 450 Hz, unit for both semitones re 100.",Friedman test in RStudio,"3/10 (I have not used the method before, I researched it specifically for this analysis)","Median f0 difference between the primary stressed syllable of the adjective and the primary stressed syllable of the noun as dependent variable. Since German is an intonation language and much of prosodic signaling above the individual word level uses f0-variation, I figured that this was a good variable to start with. I have little experience running statistical tests with multiple dependent variables, so I only chose one.","Degree of typicality (typical, atypical, medium typical). Since the research question was ""Do speakers phonetically modulate utterances to signal atypical word combinations?"", I am not sure what other predictor variable I could have chosen to answer the question. I felt like the groups for gender were too unevenly distributed to also use that as one of the predictor variables.","for the median f0 difference, I picked the main stressed syllable of the adjective and the noun in the NF condition (in the data frame with the data I extracted from Praat directly, MSA_extracteddata, the columns are called f0_MSS_adj and f0_MSS_n. The degree of typicality was assigned as specified in the document about the collection of the data.","I extracted the f0 value in semitones (re 100) directly from Praat and then calculated the median f0 difference from the values I got from Praat (MSS_diff in the data frame MSA_extracteddata). Because I only wanted to look at the difference and the direction of the difference, I added a column where I removed the negative markers (MSS_diff_pos).
Because of the inherent f0 differences of the different syllables in the data set, I calculated the median f0 difference per degree of typicality per speaker (data frame ""typ"", column median_diff_st, these are the values I then used for the statistical analysis).","I did not use any of the utterances that had the ""error"", the ""noise/sound"" or the ""structure"" comment in the textgrids. While there were notable outliers for the f0 differences, these ultimately did not have a huge effect on the result, so I did not remove these outliers.",p-value,"0.04 (small), 95%CI [0.0,1.0]",Kendall's W,"After I was done collecting the data, I realized I had to use a different statistical test from the one I originally wanted to use to one I had never used before. Because I never used this test before I also had to use a method to calculate effect size that I had never done before, which I found rather difficult because most webpages explaining statistical methods do not really tell you how to read the output you get from, e.g., R, and do not specify which packages are needed to run a specific test, or to use specific operators, which makes it basically impossible to use these methods if you are not already familiar with them."
2022-04-13 15:05:23,dermatolepis_aculeatus,https://osf.io/vgj8f/?view_only=7b9c751b452a4726ba7c7ee32a570f60,"The extraction of acoustic measurements (minimum pitch, high pitch, mean pitch) was accomplished with Praat.",We run mixed-effects models.,Very familiar.,"We picked three dependent variables: maximum high pitch of nouns, maximum high pitch of adjectives, and maximum high pitch of the whole phrase. The rationale behind these variables is that we assume that atypical utterances would have the highest maximum pitch values, typical utterances would have the lowest maximum pitch values, and medium typical utterances would fall in between. Thus, we chose the maximum high pitch of the target tokens to examine the data.","As  the Noun Focus condition contained stimuli with all three levels of typicality,  we picked typicality as our independent variables in the subset data created for Noun Focus condition only.","We extracted the high pitch (f0) for nouns, adjectives, and phrases.",We normalized all the values of each dependent variables by scaling them around their means. This was done via scale function in R.,"Thirteen token adjective-noun pairs were identified as containing errors and were excluded. Two contained mispronunciations (e.g., [t]irsche instead of [k]irsche). Eight had false starts, where either the adjective or noun was partially pronounced, and then the speaker stopped and restarted (e.g., roten Gurk- Gurke). Finally, three were excluded for recasting, where the speaker said part of either the adjective or noun, stopped, and then re-started or changed the phrase (e.g. says ‚Äògelben Bohne‚Äô and then corrects to ‚Äògelben Erbsen‚Äô.) 
There were some trials that were already marked as errors on the TextGrids that we chose not to exclude. These included hesitation breaks; although it seems possible that such a delay might signal an atypical word combination, it is irrelevant for the examination of pitch modulations, which was the focus of our analyses. Therefore we did not exclude these trials. We also did not exclude trials in which there was an error but the error did not relate to the pronunciation of the adjective-noun pair, and trials in which there was background noise, but the words were comprehensible and the pitch measurements were unaffected by the noise.",The decision was based on the significant level of the dependent variables in each model.,We did not report it.,We did not report it.,"No, we did not."
2022-04-13 18:46:40,swiftia_ruber,https://osf.io/dgur3/?view_only=46009e06ae7940049ae4e10218c13076,Labeling and extraction of acoustic measurements (time- and frequency-normalized f0 contours) were performed using Praat scripts.,"We ran four Bayesian B-splines in order to model four f0 contours based on the data, one for each combination of typicality-condition possible within the dataset we analyzed: typical-NF, atypical-NF, atypical-AF and typical-ANF. We used the predicted splines for visual comparison.",Somewhat familiar.,We analyzed time- and frequency-normalized f0 contours. f0 was chosen because it is the most robust correlate of intonational contrasts in many languages. Time and frequency normalization were employed to allow for the joint comparison of data from all speakers within the same contour.,"The independent variables were typicality (two levels, typical and atypical) and focus condition (three levels, AF, NF and ANF). Because of the subset of the whole dataset we chose to analyze, it was impossible to look at the interaction between them, once not all focus conditions were present in each typicality level.","F0 was extracted from V-to-V units (intervals between two consecutive vowel onsets on the speech signal) along the test phrase, that is, the Determiner-Adjective-Noun sequence. The test phrases selected for analysis had 5 V-to-V units and 5 equidistant f0 samples were taken within each consecutive V-to-V unit. The 25 f0 values comprising the contour were our dependent variable.","F0 values in Hertz were transformed using the formula ome = log2(Hz / median), where ome is the value in the OMe scale, Hz the f0 value in the Hertz scale and median is the median value of the f0 contour being converted. The transformed values are found in the column ‚Äòf0‚Äô of our data table.",No values were excluded.,"We used 95% credible intervals of the Bayesian B-splines posterior distributions to generate predicted f0 contours, which were visually compared.","Our analysis consisted in the visual analysis of contours generated by the  Bayesian B-splines models, as explained in the previous question. Because of that choice, no effect size was generated.",Not applicable (see previous question).,No analytical decision was revised after the initial data analysis was carried out as planned.
2022-04-14 04:26:24,saron_pictus,https://osf.io/f4wm5/?view_only=b5561241735b4fa8afe34d31c7e50509,"Praat was used to automatically produce a word-level alignment, for hand-correcting the relevant boundaries, and for extracting relevant acoustic measurements from the hand-corrected textgrids.",we analyzed the word duration of the target adjectives produced during the experiment with a Bayesian hierarchical model with a lognormal likelihood,Very familiar,"We chose word duration, as it was thought that the medium and atypical conditions may have been produced more clearly, and with less reduction. Word duration was thought of as a good first step in addressing pronunciation differences.","We included typicality, coded as an indicator variable with typical words being represented by the intercept. We also controlled for trial, as we thought it likely that word durations would be shorter over the course of the experiment, as similar sentences were being repeated.","We chose the duration of the adjective in milliseconds. As the adjective was the only target word that appeared in all three conditions, it was the only suitable comparison that could be make in terms of word duration.","We transformed the trial variable to a normalized variable that spanned between 0 and 1, and we called in 'trial_minmax'.

This is the line of code in R that did this:
d$trial_minmax <- (d$trial - min(d$trial)) / (max(d$trial) - min(d$trial))",We excluded trials that had any sort of issue denoted in the notes column.,We interpreted the results of our Bayesian model using a combination of highest density intervals and regions of practical equivalence.,"0.01[-0.08, 0.09]",the effect size is in log milliseconds,"We initially assumed that the competitor's colour was going to be matched in some trials and mismatched in others, and so we were going to include the competitor's colour in and interaction with typicality. We revised this when we realized that the colours were always congruent."
2022-04-14 08:23:41,chelonia_brummeri,https://osf.io/b5482/?view_only=e953856847a942bdb466fac2d5a76df5,"The transcripts and audio files were force aligned with the Montreal Forced Aligner using the pretrained Prosodylab German acoustic model and corresponding German lexicon (see report for source). 

Acoustic-phonetic measurements for analysis of suprasegmentals were extracted from the aligned TextGrids and audio files using Praat (v 6.1.50). F0 measurements were extracted with a pitch floor of 50 Hz and ceiling of 600 Hz.

Dynamic F1/F2 vowel measurements were extracted using the FastTrack Praat plug-in (Barreda 2021) with the default settings.

Data analysis, visualisation and modelling were performed in R. Bayesian linear mixed-effects models were run using the brms package. Generalised Additive Mixed Models (GAMMs) were run and analysed using the mgcv, itsadug and tidymv packages.",We ran Bayesian linear mixed-effects models using a Gaussian likelihood as the distribution for each outcome variable. Priors on fixed effects were specified with Gaussian distributions; priors on random effects were specified with t-distributions (see report for more details). We ran GAMMs for modelling dynamic formant trajectories.,The team member who ran the models is very familiar with the technique. The team member who ran the GAMMs is somewhat familiar with the technique.,"DVs were: relative duration, relative intensity, max F0 and F0 range. Measurements were taken in the colour adjective only (see report); Relative duration and relative intensity were relativised to relevant properties of each target utterance. 

Relative duration = duration [in milliseconds] of the stressed rhyme of the adjective, as a proportion of the average syllable duration in the target utterance. Average syllable duration in each target utterance was calculated by dividing each utterance duration by the number of syllables in a canonical/citation production of the target utterance.

Relative intensity = mean intensity [in dB] in the stressed rhyme of the adjective as a proportion of mean intensity [in dB] in the whole target utterance. 

Max F0 = max F0 [in semitones relative to the speaker‚Äôs minimum F0] in the adjective. The speaker‚Äôs minimum F0 was the minimum F0 value for that speaker across all target adjectives. 

F0 range = max F0 in the adjective minus min F0 in the adjective [both in semitones relative to the speaker‚Äôs minimum F0]. The speaker‚Äôs minimum F0 was the minimum F0 value for that speaker across all target adjectives.

Rationale: We consider these to be the minimal set of DVs appropriate for estimating suprasegmental acoustic cues to prominence in German. The measures must be relativised to compensate for between speaker and/or item variation in speech rate, amplitude and F0 span/range. 
--
We also analysed F1/F2, using dynamic formant tracking through the stressed vowel in the noun and adjective in each utterance. These measures were taken at regular 2ms intervals throughout the duration of the vowel using the default settings in FastTrack (Barreda 2021). 

We discussed additional potentially relevant measures of F0, to compensate for audible variation in the data in choice of phonological pitch accent across speakers/items. However, manual coding (prosodic annotation) of the data was not possible to complete in the time available, and we would have had to rely on annotation by a single team member, with no scope for checks on annotation (e.g. inter-transcriber agreement).","IVs in the Bayesian models were: typicality (typical, medium, atypical); syllable structure of the stressed syllable in the adjective (CV, CVC, CCV); trial; plus the interaction between typicality and trial.

Rationale: Typicality is the manipulated variable of interest. Trial is included to control for task fatigue as the experiment progressed, which interacts with typicality since each participant produced items in a different pseudorandomized order. Syllable structure of the stressed syllable in the adjective is included because this varies unevenly across target items; syllable structure variation will affect duration of the stressed rhyme directly, and may have indirect effects on intensity and F0 alignment/scaling.","All DVs in the Bayesian models were relativized as set out above (and pasted again below). We did not include speaker sex in the model as we do not have any specific expectations of variation in the DVs by sex or gender. The effect of speaker sex on F0 was controlled by relativizing of F0 measures to a relevant measure for each speaker, as described. For the GAMMs, the effect of speaker sex on F1/F2 was controlled by including random by-speaker smooths in the model (subsuming intercepts and slopes) 
‚Äì
Relative duration = duration [in milliseconds] of the stressed rhyme of the adjective, as a proportion of the average syllable duration in the target utterance. Average syllable duration in each target utterance was calculated by dividing each utterance duration by the number of syllables in a canonical production of the target utterance.

Relative intensity = mean intensity [in dB] in the stressed rhyme of the adjective as a proportion of mean intensity [in dB] in the whole target utterance. 

Max F0 = max F0 [in semitones relative to the speaker‚Äôs minimum F0] in the adjective. The speaker‚Äôs minimum F0 was the minimum F0 value for that speaker across all target adjectives. 

F0 range = max F0 in the adjective minus min F0 in the adjective [both in semitones relative to the speaker‚Äôs minimum F0]. The speaker‚Äôs minimum F0 was the minimum F0 value for that speaker across all target adjectives.","Relative duration: we first extracted values for the duration of the stressed adjective rhyme (d$adj_vdur) and the duration of the target utterance (d$utt_dur) in seconds. These were converted to milliseconds (using the same column names). Then we calculated the average syllable duration (d$avg_syll_dur) by dividing the full utterance duration by the target number of syllables (d$n_syll). Finally, we subtracted the average syllable duration from the duration of the stressed rhyme in the adjective to get the column d$rel_dur. This was then centred on the overall mean in the dataset (d$Nreldur).

Relative intensity: we first extracted the amplitude in pascal for the stressed rhyme of the adjective (d$adj_v_amp) and the entire utterance (d$utt_amp). We transformed each of these from pascal to decibel (dB) using a reference level of 0.00002 pascal. This resulted in the columns d$adj_v_intens and d$utt_intens. Finally, we subtracted the utterance intensity from the adjective rhyme intensity to get the column d$rel_intens. This was then centred on the overall mean in the dataset (d$Nrelintens).

Max F0 and F0 range: we extracted the max F0 (d$adj_maxf0) and min f0 (d$adj_minf0) from the adjective in hertz. We transformed each individual max f0 and min f0 from hertz to semitones using the speaker‚Äôs absolute minimum f0 as the reference level (d$adj_maxf0_st, d$adj_minf0_st). For F0 range, we then subtracted the transformed min F0 from the transformed max F0. The final variable for max F0 is in d$adj_maxf0_st and the final variable for F0 range is in d$f0range_st. These were then centred on the overall mean in the dataset (d$Nadj_maxf0_st, d$Nf0range_st).

The raw F1/F2 values (f1_raw, f2_raw) were centred and scaled using the scale function in R to normalise for between-speaker differences in absolute formant frequencies (f1_norm, f2_norm). Analysis was conducted on both sets of values.","We excluded all tokens with a non-empty ‚Äònotes‚Äô field in the relevant trial list, resulting in 54 exclusions. No further observations were excluded.","Based on the posterior distribution of Bayesian linear mixed effects models, the analysis tests whether the reported effect is reliably different from 0 by examining whether the reported 95% credible interval spans 0. If the reported 95% CI excludes 0, then we conclude that the estimated effect is reliable in its direction and in its deviation from 0. Specifically, for a positive mean estimate, we ask whether p(Œ≤‚â§0| model,data) < 0.025. For a negative mean estimate, we ask whetherp(Œ≤‚â•0| model,data) < 0.025. 

For the dynamic formant analysis, differences between the typicality conditions were diagnosed through a combination of:
- the significance of the parametric and smooth terms for typicality in the model summaries for F1 and F2
- visual inspection of the (overlapping/non-overlapping) confidence intervals in plotted GAMM smooths
- analysis of difference smooths between the typicality conditions within each vowel to reveal which parts of the trajectories significantly differ as a function of typicality
- model comparison with ML estimation between full models containing the typicality predictor and null models containing only the vowel category predictor
","For the suprasegmental variables analysed using Bayesian models, we report the effect of the medium and atypical conditions on each dependent variable as a deviation from the typical condition along with the 95% credible interval. These are the beta coefficients from a Bayesian linear mixed effects model, in which typicality was treatment coded and the typical condition was treated as the baseline.

Relative duration (ms):
beta_medium: 3.33, 95% CI: [-3.18, 10.09]
beta_atypical: 4.24, 95% CI: [-2.01, 10.34]

Relative intensity (dB):
beta_medium: 0.154, 95% CI: [-0.141, 0.436]
beta_atypical: 0.208, 95% CI: [-0.058, 0.473]

Max F0 (st):
beta_medium: 0.269, 95% CI: [-0.029, 0.558]
beta_atypical: -0.015, 95% CI: [-0.291, 0.257]

F0 range (st):
beta_medium: -0.530, 95% CI: [-1.235, 0.176]
beta_atypical: 0.014, 95% CI: [-0.708, 0.726]


For the dynamic formant analysis, we report the effect of the medium and atypical conditions on F1 and F2 as a deviation from the typical condition along with the 95% confidence interval as calculated/reported from the GAMM, in which typicality was treatment coded and the typical condition was treated as the baseline.

F1 (Hz)
beta_medium: 15.766, 95% CI: [1.902, 29.63]
beta_atypical: 2.805, 95% CI: [-6.595, 12.205]

F2 (Hz)
beta_medium: 26.42, 95% CI: [-10.88, 63.72]
beta_atypical: 25.17, 95% CI: [-0.15, 50.49]
","Estimated mean deviation between the reported condition (either medium or atypical) and the typical condition. The units depend on the variable and are reported above (differences in milliseconds, decibels, raw semitones, and formant frequencies).","No. 

We considered additional approaches that we think are probably needed to completely rule out the possibility of meaningful patterns in this data (i.e. patterns consistent with a positive answer to the RQ), such as qualitative prosodic annotation, but were not able to perform them with the time and expertise available."
2022-04-14 10:07:24,aratinga_lugubris,https://osf.io/uvnym/?view_only=894f5252a93944ca938d31ed02970816,"We used a viewer created in R package phonfieldwork.
We did not use any software for extracting acoustic measurements. We analyzed non-phonetic cues and annotated them in Excel tables","We used Bayesian mixed effects ordinal regression models as implemented in the R package brms, which uses the Stan programming language‚Äôs interface with the R statistical programming environment. We used default priors from the brms package","We have never used this type of model before, however we have some experience with Bayesian modeling",The dependent variable was typicality as provided by the organizers (typical/medium/atypical),"We chose the number of speech errors/hesitations/longer pauses and adjective focus as independent variables. We did this on the basis of perceptual analysis of the provided recordings.
The speaker and color variables were used as random intercepts","The ‚Äòspeech errors‚Äô variable was binary: 0 or 1. The ‚Äòadjective emphasis‚Äô variable was also binary: 0 or 1.
Both variables were annotated perceptually",We did not apply any transformations,We did not exclude any observations,We visually analyzed degrees of overlapping of credible intervals,"First model (speech errors vs typicality)
No. 	estimate lower higher typicality speech_errors   
1	0.330 0.293  0.372 atypical   no speech errors
2	0.334 0.304  0.365 medium 	no speech errors
3	0.335 0.296  0.376 typical	no speech errors
4	0.360 0.270  0.461 atypical   speech errors   
5	0.330 0.298  0.362 medium 	speech errors   
6	0.307 0.224  0.404 typical	speech errors

Second model (adjective emphasis vs typicality)
No. 	estimate lower higher typicality annotation_adjective_emphasis
1	0.332 0.294  0.372 atypical   non-emphasized          	 
2	0.334 0.303  0.365 medium 	non-emphasized          	 
3	0.334 0.295  0.373 typical	non-emphasized          	 
4	0.356 0.205  0.547 atypical   emphasized              	 
5	0.325 0.274  0.359 medium 	emphasized              	 
6	0.312 0.171  0.494 typical	emphasized",We didn‚Äôt calculate the effect size,During our analysis we set small and achievable goals and decided on the following steps after a goal was reached. So we did not need to change the course of our analysis
2022-04-14 13:42:39,ctenosaura_limax,https://osf.io/vysej/?view_only=b799203d81ae4e08894258219272cfd4,Textgriding was done with Montreal Forced Aligner and acoustic features were extracted with OpenSmile toolkit.,We treated the analysis as a multi-class classification task and ran multinomial logistic regression with L1 regularization. The model was fitted using the Scikit-learn Python package.,Very familiar.,"The dependent variable is the typicality condition, which consists of three distinct categories: atypical, typical, medium.","All independent variables in our final model are listed below. We chose them because they were commonly used features for speech classification. We selected only the 20 f0 and formant features out of the 88 features in the GeMAPSv2 feature set. However, we found that some features were strongly correlated with each other. While strong correlations do not pose problems for the model and the classification task, they make interpretation impossible. So we manually selected ten features through inspecting the pairwise correlation matrix. These ten features were not strongly correlated. 

F0semitoneFrom27.5Hz_mean: mean F0 on a semitone frequency scale, with semitone 0 starting at 27.5 Hz.
F0semitoneFrom27.5Hz_stddevNorm: standard deviation of F0 on a semitone frequency scale, with semitone 0 starting at 27.5 Hz.
F0semitoneFrom27.5Hz_meanRisingSlope: mean of the slope of rising f0
F0semitoneFrom27.5Hz_meanFallingSlope: mean of the slope of falling f0
H1-H2_mean:  mean ratio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2)
H1-H2_stddevNorm: normalized standard deviation of the ratio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2)
H1-A3_mean: mean ratio of energy of the first F0 harmonic (H1) to the energy of the highest harmonic in the third formant range (A3).
H1-A3_stddevNorm: normalized standard deviation of the ratio of energy of the first F0 harmonic (H1) to the energy of the highest harmonic in the third formant range (A3).
F1amplitudeLogRelF0_mean: the mean ratio of the energy of the spectral harmonic peak at the first, second, third formant‚Äôs centre frequency to the energy of the spectral peak at F0.
F1amplitudeLogRelF0_stddevNorm: normalized standard deviation of the ratio of the energy of the spectral harmonic peak at the first, second, third formant‚Äôs centre frequency to the energy of the spectral peak at F0.","We extracted acoustic features from the NP phrases, whose boundaries were extracted with Montreal Forced Aligner. The features were computed from frame-level features of each acoustic frame for the whole phrase. All independent variables are continuous acoustic features.

The dependent variable is a categorical variable with three levels, atypical, typical, medium. They were one-hot encoded, which is a standard practice for classification tasks.

","For the 88 acoustic features, there were multiple transformations applied to them. The main idea is to extract frequency, amplitude and spectral parameters from each frame and then compute summary statistics from them (mean, variance, etc.).
Details can be found in Section 3.1 and 3.2 of this paper: https://sail.usc.edu/publications/files/eyben-preprinttaffc-2015.pdf.

For the extracted acoustic features, we applied mean-variance normalization (z-score) using the StandardScaler() in Sklearn. 
For the dependent variable, one hot encoding was applied to convert the three categories to numerical categories.",We excluded observations from the distractor conditions as they were not relevant to the research question.,"We ranked the magnitude of regression coefficients to find out the variable that contributes most to the best results. 
The results were obtained through cross validation, that is, training the model on all but one speaker and testing the model on the unseen speaker. This was repeated for all speakers so we fitted 30 models. Then we ranked regression coefficients in all 30 models and manually inspected the results to find out the coefficients with largest magnitude.","The effect size is the magnitude of the coefficients, which was selected by ranking all coefficients in a model. We did this by pooling the results across 30 models.",The effect size is unitless as it is based on the normalized features.,"We initially fitted models with 88 acoustic features that were commonly used for machine learning in speech. However, we found that many variables were not linguistically interpretable (MFCCs), even though they were highly predictive. So we later decided to select a subset of 10 f0 and formant features that were more familiar to linguists as the variables for regression analysis."
2022-04-14 11:57:40,pseudodax_euryzona,https://osf.io/g9bq5/?view_only=727c4b232ed146bd88f5fabdb5cb0fc2,We first manually aligned the words and then extracted the relevant acoustic measurements (f0 and duration) using Praat.,"First we performed a Functional Principal Components Analysis (FPCA) to capture the variation in the data, we modeled it with a linear mixed-effects model and then we performed a post-hoc pairwise comparison.",I attended a course on FPCA and I‚Äôm also currently using this method in a different research project.,"The response variable s2 (the score values to be multiplied by the second principal component) was selected after performing a functional principal component analysis on F0 and visually inspecting the curves of the PCs resulting from it. We determined that the first PC, which varied in the vertical axis, likely corresponded to a residue of inter-speaker variation that survived normalization, and that the second (which looked more like a ‚Äúshifter‚Äù), had likely captured the relevant variation. PC2 corresponded to 20.8% of the variation in the F0-normalized dataset, and the statistical analysis revealed that it effectively captured at least part of the relevant variation present in the data.","We used typicality as the only independent variable, given that this predictor directly corresponds to the proposed research question. Given that the focus condition would also likely elicit significant differences between f0 slopes, we only used the NF (noun focus) condition for the final analysis, which was also the only condition marked for typicality in the provided materials. As typicality only applies to the FOOD subset, the NON-FOOD subset of examples was not used in the analysis.","The variables time and f0 were extracted both by (prep.) phrase and by word. The prepositional phrase was created by combining the data extracted for sequences of four words: the word auf and the article, adjective and noun that followed it (e.g., auf den orangen Kartoffeln). The variable gender was only used for parameter setting in the extraction of f0 values in Praat.","We min-max transformed the time values, which are found under ‚Äútime‚Äù, by subtracting the minimum value to each observation and then dividing it by the range with the function (x - min(x)) / (max(x) - min(x)). F0 was also z-transformed (x‚Äìmean(x)/sd(x)) to account for inter-speaker variation using the function scale(), and the resulting z-scores can be found in column ‚Äòf0.z‚Äô. Normalized Hz values were then reconstructed from the z-scores by multiplying z-scores by the mean of the SDs of all speakers and then adding the grand mean of the sample to the resulting value, and these can be found in column ‚Äòf0.norm‚Äô.","Examples with annotations in the notes tier (labeled as error, hesitation break, error2, break and doubt) were excluded. In addition, examples with more than 7 F0 values that were +/- 3 standard deviations from the mean were also excluded.","Our answer was based on the p-values resulting from a post-hoc pairwise comparison of the 3 levels of the predictor typicality after building a model with s2 as response (i.e., the score relative to the second principal component), typicality as a predictor and a random by-speaker intercept.","The effect of typicality is 0.04 [0.02, 0.07].",Partial eta squared (the proportion of variance explained by the variable).,"When deciding on how to analyze the F0 curves, we discussed using either Generalized Additive Models or Functional Principal Component Analysis. We initially decided to divide the team to try both analyses. However, due to the dataset being big, running GAM models proved difficult since the beginning for some team members. Thus, the fact that GAMMs are computationally heavy was factored in when deciding which route to follow, and ultimately tipped the balance in favor of a fPCA + LMER analysis."
2022-04-14 14:07:07,trachurus_riukiuensis,https://osf.io/kvt5m/?view_only=ae642860326644128aacebb27e87a4d7,"Extraction of acoustic measurements (f0, duration) was accomplished with Praat.",We ran linear mixed effects models fit by REML using the lme4 package in R.,There was varying familiarity in our group from novice to very familiar.,"If there was going to be an effect, we predicted that the effect would manifest as a prosodic difference, and thus focussed exclusively on f0 and duration measurements.  Since adjectives were balanced across the three conditions, we decided to focus primarily on the adjective, but included the article and noun for good measure. 
Duration of article
Duration of adjective
F0 peak of article
F0 peak of adjective
F0 peak of noun
We hypothesized that due to the nature of the experiment, namely seeing objects with all kinds of colours, participants might vary in their f0 and durations of the adjective as a function of the adjective-noun‚Äôs typicality.

We hypothesized that this effect could go either of two directions:  1) the typical adjective-noun combinations, after speakers have seen so many unusual or impossible combinations, could lead speakers to add prosodic emphasis in the typical condition, probably leading to greater duration or f0 somewhere in the article-adjective-noun sequence; or 2) speakers could add prosodic emphasis to the atypical adjective-noun combinations because they are surprising.  Direction 1 (emphasis on typical) would reflect a reaction of surprise at finally seeing a picture that makes sense, while direction 2 would reflect emphasis on novel and unlikely combinations.","We selected typicality as the independent variable and coded this as a fixed effect in the model.  Typicality was selected since the research question asked whether the typicality of the adjective-noun combination would have acoustic effects.  We did not expect there to be any effect of gender or age (and, moreover, this information was not provided), and so did not include them as predictors.","Duration of the article and adjective were measured based on our segmentation criteria, which were hand-segmented according to the following principles:

The onset of the article was the beginning of the burst, or if none was visible, the onset of formant structure of the vowel.  The offset of the article (and hence onset of the adjective) was defined by the offset of F2 or the onset of silence/creak for ‚Äúder‚Äù, and the offset of the nasal formant structure for ‚Äúden‚Äù. In the case of ""der"" followed by an initial vowel with no creak or glottal stop, the offset of the article was placed at halfway through the F2 transition.

Since all adjectives end in [n], the offset of the adjective was taken to be the offset of nasal formant structure.  When the following noun began with a nasal [m] (as in Mandarine and M√∂hre), the boundary was taken to be the point when the distribution of frequencies of energy suddenly change in the spectrogram and waveform; in a handful of cases, there was no clear boundary and so the midpoint between the nasal formant structure of the [n#m] sequence was selected.

Peak f0 was extracted from the word for the article and adjective.  For the noun, peak f0 was extracted from the interval beginning at the onset of the noun, and ending at the offset of the stressed syllable.  For all three f0 measures, the peak was measured as of 30 ms after the onset of the word in question, in order to eliminate microprosodic effects associated with voiceless consonants.
",We did not transform any of the variables.,"
All tokens marked as having been produced with an error or hesitation were omitted, as were all tokens from one speaker (JW_3) who had a particularly high error rate; thus 70 tokens were removed, leaving a total of 830 observations over 29 participants before outliers were removed.

For each acoustic measure, data were inspected to ensure that measures were roughly normally distributed.  Outliers for the peak f0 measures were defined using the absolute deviation around the median by participant, using the ‚Äúmad‚Äù function from the ‚Äústats‚Äù package (see Leys et al., 2013).   The rejection criterion for the peak f0 of the adjective and noun was set at 2.5 standard deviations from the absolute deviation around the median; the rejection criterion was set at 2 standard deviations for the peak f0 of the determiner, as this less conservative cutoff was necessary to eliminate cases of creak that persisted throughout the determiner.  Thus for the peak f0 measures, the total number of observations were:  816 observations for the adjective; 802 for the noun; and 671 for the article.  No outliers were removed for the duration measures of the noun and adjective.

Leys, C., Ley, C., Klein, O., Bernard, P., & Licata, L. (2013). Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. Journal of Experimental Social Psychology, 49(4), 764‚Äì766. 
","For each analysis, we made a decision based on the 95% confidence intervals (CIs) for the coefficient, standard error (SE), and p-value.","Article duration - no effect
Typicality: atypical	-13.95 [-42.42,  14.52]	
Typicality: medium	0.95 [-27.52,  29.42]	

Article peak f0 - no effect
Typicality: atypical	1.21 [ -1.29,   3.71]	
Typicality: medium	-0.0008 [ -2.51,   2.51]	

Adjective duration - no effect
Typicality: atypical	13.96 [-111.81, 139.74]	
Typicality: medium	-7.66 [-133.43, 118.12]

Adjective peak f0 - no effect
Typicality: atypical	1.86 [ -5.55,   9.27]		
Typicality: medium	-0.66 [ -8.07,   6.75]	
Noun peak f0 - no effect
Typicality: atypical	5.58 [ -7.94,  19.10]		
Typicality: medium	7.58 [ -5.94,  21.11]","Because no effects were found, we report each analysis using the estimated coefficient in its original units, i.e., milliseconds for duration and Hz for f0.
","Creak that persists throughout the entire article, or rarely throughout the entire adjective or portion of noun measured.  We did not anticipate that two speakers would often creak throughout so much of their speech, making f0 measurements bimodal and unreliable for the question we were asking.

We didn't anticipate the pronunciation ""orangenen"" for ""orangen"", but this is unlikely to cause any problems in the analysis because of the overall variability of adjective durations and the matching of adjectives across conditions.
"
2022-04-14 14:52:29,varanus_eulophotes,https://osf.io/v9mwq/?view_only=7beb75402509436c9eabbd7868b550dd,"All utterances were extracted from the original audio files and saved as separate files using Praat. A Python script was used to create .txt file transcripts for each utterance using the trial data .csv files. Each individual utterance was contained in a separate .wav file with a corresponding .txt transcript file. These were then used as input to forced alignment using the Montreal Forced Aligner (McAuliffe et al. 2017) with a pre-trained German acoustic model and dictionary (available here). The forced aligner produced Praat TextGrid files marking the intervals of words and phones. 

Measurements were extracted in Praat (Boersma & Weenink, 2022) using a Praat script written by the team. Average f0 of the adjective was extracted by isolating the adjective using the MFA boundaries and creating a pitch contour. The Get mean‚Ä¶ function was then used on the Pitch object to extract the average f0 of the pitch contour of the adjective. The script output consisted of a database with filenames, adjective labels, duration of the adjective, and average f0 of the adjective (henceforth referred to as f0). This output was combined with the stimuli information from the project coordinators to include additional information on each utterance (typiciality, target name, target  color, etc.) using a Python script. This combined database was then read into R (R Core Team, 2022) for further analysis. 
",linear mixed effects model,"students leading the analysis are somewhat familiar (~1 stats class), faculty team leader is familiar (regularly uses lmms in research)",f0 averaged over the duration of the adjective ‚Äì¬†see write up for rationale,typicality with random intercepts for speaker,"f0 extracted and time-averaged over the word (see write up for rationale), typicality levels taken directly from the provided data (atypical, medium, typical)","Log and inverse transformations were attempted but also resulted in significant Shapiro-Wilks results, so we proceeded with the regression on the raw values. Results do not change if the regression is instead done with log-transformed f0.",no,p-values for typicality predictors in the linear regression,"Cohen's d for medium typicality vs atypical = 0.00486
Cohen's d for typical vs. atypical = -0.00918",Cohen's d,no
2022-04-14 17:23:51,comanthina_maculatus,https://osf.io/fpmwn/?view_only=19b6903124254cb68f67315bb3ec8517,"Extraction of acoustic measurements (duration, intensity, F0) was accomplished with Praat.",Unpaired two-tailed t-tests.,"Somewhat, it's covered in various undergrad stats classes.","Dependent variables were continuous, the measurements of duration, intensity, and F0.  The literature pointed to those three as good candidates of stress correlates, and the rationale was that the three levels of the typicality factor was hypothesized to elicit perhaps longer, perhaps more intense vowels from least to most typical.","Typicality, with three levels, as specified in the data provided: atypical, medium, typical.  It was thought that this would be the best independent variable for the research question provided of 'does typicality affect the phonetics?'","The measurements were extracted from the pre-tonic, tonic, and post-tonic vowels (if there was one) of the noun focus of each noun focus condition elicited phrase.  The pre-tonic and post-tonic measurements were not included in the statistical analysis.",n/a,"Tokens marked in the Notes tier in Praat as ""error"" or ""background noise"" were not included.","P-values for each of the three measurements, comparing pairwise across levels of typicality.",n/a,n/a,"My biggest roadblock was trying to figure out what to do when the residuals normality check came back indicating that I needed to use a non-parametric test.  Davidson's pdf did not cover what to do in that instance.  During the .wav file segmenting, I did have to go back and tweak some segments, as about halfway through I decided that I should try to mark a schwa if I could find it (however tiny) on the spectrogram, even if it wasn't audible (e.g. Bohnen /bo:n…ôn/ [bo:n])."
2022-04-14 15:28:21,paralichthys_undulatus,https://osf.io/sxp9g/?view_only=d7c7685bff7a4a9a810695c27226d5f5,Extraction of acoustic measurements was done with Praat.,Mixed model linear regression,I have used it a lot.,f0 range (f0max-min in semitones) - based on f0 being used for focus in a variety of languages,"Typicality and Word Class - based on the research question regarding typicality, and to examine whether there may be a difference between the adjective and the noun.","F0 range from the adjective and f0 range from the noun. In both cases, it was the f0 range on the whole word.",f0 was measured in semitones re 100Hz by a Praat script.,No exclusions based on measurements - only exclusions based on coding as errors or hesitations.,"p-values in the linear regression, at the level of 0.01.",0.17,Cohen's f2,No
2022-04-15 01:29:44,alosa_atun,https://osf.io/9e6af/?view_only=5e85e36bda2944b7adaaaf91c10c540b,"We used Praat to extract the acoustic measurements of our dependent variables.

However, it was necessary for us to first modify the textgrids provided to us by the MSA team, as trigger utterances were not segmented in the original textgrids and we wished to analyse them.  We use Praat to find silences, and then a Python script populated the interval tiers with the appropriate text (by reference to the participant csvs).   We manually checked the alignment and corrected any which were not  appropriatelyaligned. Then we uploaded the newly segmented and corrected textgrids into LaBB-CAT, the browser-based Language, Brain & Behaviour Corpus Analysis Too (LaBB-CAT; Fromont & Hay, 2012)., so that the individual phones could be automatically force-aligned using the BAS webservice, WebMAUSBasic.

We also used LaBB-CAT to calculate speech rate based on the CELEX lexical database (Baayen et al., 1996) for German, and the LaBB-CAT interface with praat to extract our acoustic variables, using the nzilbb.labbcat and rPraat R libraries.  Within R, we joined the resultant dataframe to the original MSA data, namely the participant/trial information and the typicality judgements from `ratings_summary.csv` collected from the original MSA database (Coretta, Roettger, & Casillas, 2021).

In addition to these programmes for extracting acoustic measurements, we also utilised Python and R programming (Van Rossum & Drake, 1995; R Core Team 2022) to further organise the TextGrids and acoustic data.

After we had prepared the textgrids and extracted the acoustic measurements, we used R to organise the data. We created an R script that would process and evaluate the measurements; the script was divided into two discrete chunks, to complete two separate tasks, thereby creating two steps.

In the first step, the first chunk of the script extracted the acoustic information of the vowels in ‚Äòder‚Äô and ‚Äòden‚Äô from the [LaBB-CAT](https://labbcat.canterbury.ac.nz/system/) corpus which was specifically created for this analysis, while, the second chunk of the script joined that dataframe to the original MSA data, namely the participant/trial information and the typicality judgements from `ratings_summary.csv` collected from the original MSA database (https://osf.io/5agn9/?view_only=6fec040a9589499bb9998240bdab12a1). 

Next, we used the nzilbb.labbcat package in R to extract the trialnumber and syllable articulation information: trialnumber, syllableCount, syllablesPerMinute, participant_syllablesPerMinute. 

F1 and F2 formants of the midpoint of a segment and the pitch and intensity measures were processed with Praat through the nzilbb.labcat package. Following this a combined analytical dataset was generated with dplyr and tidyr that is used for the modelling analysis detailed below (see also ‚ÄòData Dictionary‚Äô for additional information on individual variables).","In order to analyse the acoustic measurements extracted from Praat our initial analyses included linear mixed-effects models (lmer) using the model including random intercepts and random slopes.  Our final analysis only used intercepts.

A linear mixed-effects model (lmer) is a statistical model where fixed and random effects are added to simple linear modes in order to explain the relationship between the dependent and independent variables. To apply these models we used the lme4 package which allowed us to fit a linear mixed-effects model to our data via residual maximum likelihood.",Most of our team members were familiar with exploratory data analysis and statistical modelling in R.,"Our team chose to explore the vocalic components of the German definite article ‚Äòder‚Äô. We excluded tokens in which the definite article was ‚Äòden‚Äô.   We included both the target trials, and also the preceding ‚Äòcompetitor‚Äô trials, referred to in this document as the ‚Äòtrigger‚Äô.

The acoustic variables analysed are given in table 1.  Those that are measured at the level of a vowel are analysed in two sets of models - models of the nucleus, and those at the offglide.

The following is a list of dependent variables we employed:

‚Ä¢ The first formant measure at the midpoint of the target segment expressed as f1.
‚Ä¢ The second formant measure at the midpoint of the target segment expressed as f2.
‚Ä¢ The maximum amplitude of the target segment (i.e., /e/ or /@/)
‚Ä¢ The mean pitch of the target segment expressed as f0.
‚Ä¢ The duration of the target segment. 
‚Ä¢ The combined duration of the nucleus /e/ and the schwa-offglide.
‚Ä¢ The degree of formant movement between the nucleus and offglide, measured as the euclidean distance
‚Ä¢ The ratio of the duration between the nucleus and the offglide.
‚Ä¢ The duration of the target word.
‚Ä¢ The duration of the pause between the end of the word and the start of the following word.
‚Ä¢ The number of syllables in the sentence (either the trial or target, as relevant) divided by the summed word duration (excluding pauses)

These were tested across several stages of modelling. For our pre-registered model we focussed on acoustic characteristics of the nucleus. These models were not well-formed and we did not complete this modelling step, but abandoned it for models that have a better structure (described below). Vowel-specific factors were tested both for the nucleus (e) and the offglide (@). Three models were tested for properties of the combined nucleus/offglide. Finally, we tested the full word-level duration, and effects of the overall speech rate.

Our motivations for choosing variables associated with the /e/ and /…ô/ vowels, of each preceding definite article ‚Äòder‚Äô, is that we might expect changes in typicality to be signalled in surrounding words of the adjective+noun pairs, since a similar frequency effect is found in other multiword stretches. A similar phenomenon occurs in the English definite article ‚Äòthe‚Äô where the vowel can experience production changes from /…ô/ to /i:/ in situations where a speaker encounters a problem with the following utterance, such as when they are uncertain about pronunciation or word choice (Fox Tree & Clarke, 1997) . This is further supported by the findings in Jurafsky et al. (2001), Bell et al. (2009), and Tily et al. (2009) where acoustic features in an utterance are mediated by probabilistic relationships such as lexical frequency and predictability. As this analysis was fairly exploratory, we chose to investigate a wide range of potential acoustic factors.","Typicality: Our pre-registered model tested the three way Typicality Category supplied.  For other models we used a numeric predictor - the mean typicality from the typicality rating study. (P.typ_mean)

Condition: The trigger sentences were not part of the original design, and we coded them to a separate condition (T), so we tested a four level condition variable (T, NF, AF, ANF).

ScaledTrialNumber:  We scaled the trial number, and included this as a measure of how far through the experiment we were. 

syllablesPerMinute. The number of syllables in the clause (either the trigger or the target as appropriate), divided by the summed word duration.  Note: in our attempted pre-registered model, we used trialduration, as a speech rate control, and we switched to syllablesPerMinute for the other models.

We considered three way interactions between the first three variables, and included the speech rate variable as a control  (except in the speech rate model, where this became the dependent variable).

In the models of amplitude and pitch we also included a local control measure, which was the amplitude/pitch extracted from ‚Äòden‚Äô in the ‚Äòden Wurfel‚Äô, immediately preceding the analysed ‚Äòder‚Äô.


The first three independent variables are directly related to our hypotheses - firstly, that the typicality of the adjective+noun pairs and the combinations themselves could affect production of the vowel of the preceding definite article. Secondly this may be more pronounced in the adjective focus condition.  Thirdly, this phenomenon could decrease over time for each speaker.  In other words, the change in vowel quality/peripherality and duration may be less pronounced over time due to the normalising effects of regularly anticipating possible atypical adjective+noun pairs by the speakers as the experiment progresses. 

A control for speech rate is necessary, as duration effects will vary with speech rate, as may the other acoustic variables.  We preregistered that we would use the duration of the trial.  During modelling we realised that trials differed by number of syllables, and we switched to syllablesPerMinute for the final models as a more appropriate control.","The domain is indicated in the table above. Variables are defined at the domain of the nucleus, offglide, diphthong, or word.  The domain of speech rate is the local clause (either the trigger or the target clause in the trial).","We scaled and centred the continuous variables, trialnumber and trialduration (for the preregistered model), in order to help with model convergence.  These are in the columns scaledtrialnumber and scaledtrialduration.","For every variable, we remove outliers with 2.5sd +/- mean for each participant.   
We also applied a hard cut-off minimum of 800 to F2, as the 2.5sd criteria resulted in the inclusion of some unplausibly low values.",Significant pairwise ANOVA comparisons of minimally different lmer models.  We accepted a predictor as significant if its inclusion led to a significantly better model than a model that excluded it.,"As model fitting involves pairwise (ANOVA) anova comparison between minimally different models, in a backwards procedure, we used analysis of variance (ANOVA) to evaluate our models. The purpose of the ANOVA test was to compare two models and determine whether the removal of a variable significantly affected model performance.",We compared the explanatory power between our models using performance metrics from ANOVA.,"We preregistered a model that did not converge and had singularity errors, leading us to refine our model fitting, and adopt the numeric value of typicality rating.

Earlier modelling also showed the following:
(1) A significant effect of F2 on schwa.  Visualization of this effect revealed the problems with the low F2 values, leading us to impose a stricter F2 outlier criteria.
(2) Duration effects when speech rate was not properly controlled, leading us to adopt a more direct measure of speech rate, and analyse speech rate itself as a variable of interest."
2022-04-15 05:45:39,ceratophrys_elephantotus,https://osf.io/ckmsy/?view_only=db1e05e3b7be4ddfbf37e1ddf28c55ae,"Praat for acoustic measurements (f0, duration, formants) Weka implemented in R.",Weka implemented in R.,Familiar,"phone duration, formant values, f0 --> weka choses the most robust variables that allow classification, focalization in German is linked to duration and f0, formant values help because the vocabulary and thus the phones were limited","target_name, target_colour, word category in order to see if there are some links according to the target phones or the word category","f0 and duration were extracted at the phone level, formant values were extracted at the beginning, the center and the end of the phone. Concrete values were extracted, the mean of between two time points (time was of the essence).",none,observations with comments extracted from the original TG were excluded,Results of the automatic classification and linked confusion matrices + chi-square-test,"effect size Cramer's V (df = 2) = 0.35 (medium effect), towards ""medium typicality""",Cramer's V,I would have liked to for formant extraction and the cluster problems resulting from missing values but time was not on my side.
2022-04-15 07:30:30,naso_cassivellaunos,https://osf.io/f496y/?view_only=6e4619054bf94a06a3a761da54c3b35e,"We first used Praat to extract the targets for analysis, from ""auf"" to the end of ""ablegen"" in each utterance. The resulting multiple shorter audio files were batch-processed in the following order using a commercial program Myriad*: (1) Clear all meta-data and remove DC offset; (2) Fade-in and fade-out 10 ms on each side;(3) Add 200 ms of silence on each side; (4) Normalize to -23 LUFS. 

* Myriad 4.4.1 by Aurchitect Audio Software, LLC (discontinued). These basic procedures can be also achieved with batch processing in the open-source software Audacity (among others).

Next, we analyzed all the files using the ProPer workflow (https://osf.io/28ea5/). We used Praat to extract Intensity Tier, Pitch Object and Pitch Tier for each audio file. The Pitch Tiers were manually corrected by a group of annotators from the team to obtain reliable and smoothed F0 contours (10Hz smoothing in Praat). The Praat files (Intensity Tiers, Pitch Objects and Pitch Tiers) were then read into R using RStudio. The Pitch Tier files were read as the F0 time series; the Intensity Tier and Pitch Object files were read in order to calculate the periodic energy time series. The ProPer toolbox computes a prominence-related metric termed mass, which is based on the area under the periodic energy curve between two boundaries. ProPer also computes two pitch-related metrics, ‚àÜF0 and synchrony, based on informative interactions between F0 and periodic energy.

For all statistical analysis, we used R and Rstudio. Bayesian mixed-modelling was performed using Stan via the brms library in R.
","The inferential results that are described in our report were obtained using Bayesian Mixed-Effects modeling with both Gaussian and log-normal likelihoods.


We also carried out hierarchical cluster analysis (linkage-criterion: complete linkage, distance matrix calculated on the basis of Euclidean distances).
","We were familiar with the ProPer analysis and the cluster analysis. In the team, people have various levels of expertise with mixed-modeling and / or Bayesian approaches. Overall, our collective expertise is relatively satisfactory but not perfect. On a 10 point likert-scale (1 being complete beginner in all aspects, e.g. both in mixed-modeling and in Bayesian approaches, 10 being high-level of expertise in both domains), we may estimate our global familiarity as close to 6/7, although it is distributed over various people with specific knowledge. Some of us were beginners in Bayesian analyses but received help from more experienced users in the team at various stages of the analysis. Some aspects of mixed-modeling that were crucial to reaching an appropriate level of data interpretation were also addressed during meetings.","The dependent variables were mass, deltaF0 and synchrony. 

1. Mass is the area under the periodic energy curve within each syllable. 

2. ‚àÜF0 (DeltaF0) is the difference in F0 (Hz) between successive syllables (values taken from the center of periodic energy mass within each syllable) to account for the shape of the F0 curve across syllables. 

3. Synchrony measures the distance (in ms) between the center of periodic energy mass (CoM) and the tonal center of gravity (CoG) within each syllable, to account for the shape of the F0 curve within syllables. 

We chose these metrics as they allow for a holistic approach to measuring and describing speech prosody. 

For cluster analysis, time-series f0 measures were used as a dependent variable because they represent the f0 trajectory, unlike static measures such as f0 mean, f0 range, etc.
","We chose ‚Äòtypicality‚Äô as the independent variable, because the research question concerned the ‚Äòtypicality‚Äô. The ‚Äòmedium‚Äô level was not investigated thoroughly because there were not sufficient data points in this experimental condition.
We used the categorical distinction of 'typical' vs 'atypical', rather than the continuous mean values for typical also provided, as the latter did not in fact represent a continuous distribution of values, and therefore was essentially not informative beyond the binary/ternary distinction.

Defining ‚Äòindependent variable(s)‚Äô does not apply to the cluster analysis itself. However, the clustering output was checked against the predictor ‚Äòtypicality‚Äô.
","The ProPer analysis was syllable-based. The syllable boundaries were semi-automatically determined in the ProPer algorithm, informed by both the minima across the periodic energy curve and the location of manually annotated boundaries from the textgrids (boundaries were manually added by a group of annotators from the team).

Although we looked at a relatively long stretch of the utterance (from ""auf"" to the end of ""ablegen""), we eventually targeted 4 syllables in each trial: the 2 stressed syllables and the 2 final syllables within the adjective and the following noun. For each syllable we computed the three ProPer metrics (mass, deltaF0 and synchrony).

Bayesian models included all three levels of Typicality for global estimation but the ‚Äòmedium‚Äô level was not considered in the part that was dedicated to hypothesis testing: only specific comparisons between the ‚Äòtypical‚Äô and ‚Äòatypical‚Äô conditions were estimated.

For clustering analysis, F0 measures were taken from noun-final syllables; 30 measures per syllable to acquire a reasonably high resolution trajectory. Octave jumps were corrected and if this correction did not succeed, the f0 contours were discarded.
","We used the ProPer workflow to obtain a smooth F0 contour that is modulated by the corresponding periodic energy curve. Periodic energy was computed in R from the Praat Pitch Object and Intensity Tier files. The periodic energy curve was log-transformed and smoothed. Four different floor values were used to fit the periodic energy curve of the different speakers in order to adjust the zero value of the log-transformed periodic energy time series. We used a 12Hz low-pass filter to smooth the final periodic energy curve and a 6Hz low-pass filter to smooth the final F0 curve. The variable names we used were 'smogPP_12Hz' for the periodic energy curve (smoothed with a 12Hz low-pass filtering), and 'f0_interp_stretch_smooth' for the F0 curve (smoothed with a 6Hz low-pass filter).

ProPer computes the three metrics from the two variables, 'smogPP_12Hz‚Äô and 'f0_interp_stretch_smooth', and from landmarks that are derived from these two time series, such as the center of periodic energy mass (CoM) and the tonal center of gravity (CoG):

1. We normalized mass by computing the relative difference between the masses in each utterance, yielding the variable 'mass_rel'.

2. We normalized DeltaF0 by dividing the raw values by the F0 range of that speaker across the different analyzed trials, yielding the variable 'DeltaF0_rel'.

3. We normalized synchrony by dividing the raw values with the duration of the containing syllable, yielding the variable 'sync_rel'.

Only relative measures were used as Dependent Variables. No further transformations were performed for the inference part.

For clustering analysis, F0 time-series measures were speaker-corrected using standardisation (Rose, 1987; doi: 10.1016/0167-6393(87)90009-4).



","We used a subset of the data in two phases. In both phases we analyzed the targets (from ""auf"" to the end of ""ablegen"") only for the noun focus (NF) condition. We excluded the utterances with errors as commented by the MSA coordinators in the textgrid. 

In Phase 1, we controlled word length in the targets; we selected targets with a disyllabic adjective followed by a disyllabic noun. In Phase 2 we added two targets containing words with more than two syllables, ""o.ran.ge(.)n(en) Trau.ben"" and ""gel.ben Zit.ro.ne"", to mitigate the potential confound, the syllable structure, found in Phase 1. In Phase 1, all ‚Äòtypical‚Äô noun phrases were found to end with a closed syllable (Wal.nuss, Boh.nen) whilst all ‚Äòatypical‚Äô noun phrases ended with an open syllable (M√∂h.re, Kir.sche, Gur.ke). 


For cluster analysis, Clustering was applied using R and subsetting of the data was done following the procedure referenced in the report. From the total number of NF syllables, 364 were clustered.

In the Bayesian modeling part, no data were excluded. For each item, 4 syllables were entered into the analysis (Penultimate syllable / Adjective, Final syllable / Adjective, Penultimate syllable / Noun, Final syllable / Noun). Relative expressions of Mass, Synchrony and DeltaF0 were investigated as Dependent Variables. Initial explorations of distributional properties for each dependent variable were performed in order to target possible distribution fits for the modeling, which should limit the impact of possible extreme observations that may not fit the selected likelihood. Although all 3 levels of typicality were included in the analyses, the ‚Äòmedium‚Äô level was excluded from specific hypothesis testing for simplification (and due to a paucity of data for this condition), even though at least some partial investigations seemed to be in line with conclusions that were drawn from the ‚Äòatypical‚Äô condition.

In the Bayesian modeling part, no data were excluded. For each item, 4 syllables were entered into the analysis (Penultimate syllable / Adjective, Final syllable / Adjective, Penultimate syllable / Noun, Final syllable / Noun). Relative expressions of Mass, Synchrony and DeltaF0 were investigated as Dependent Variables. Initial explorations of distributional properties for each dependent variable were performed in order to target possible distribution fits for the modeling, which should limit the impact of possible extreme observations that may not fit the selected likelihood. Although all 3 levels of typicality were included in the analyses, the ‚Äòmedium‚Äô level was excluded from specific hypothesis testing for simplification (and due to a paucity of data for this condition), even though at least some partial investigations seemed to be in line with conclusions that were drawn from the ‚Äòatypical‚Äô condition.


","For Bayesian modeling, our criteria were based on the computation of 95%-credible intervals and posterior probabilities for each specific effect.

For clustering analysis, an evaluation method to find the ideal number of clusters was applied. This method is based on comparing information cost measures for several clustering rounds. We obtained the lowest information cost value for a cluster analysis with 6 clusters. 
","For each syllable, we tested the unidirectional posterior probability that, given our model, priors, and data, the estimate size is greater than zero (with the same sign, therefore in the same direction). The estimate is measured by subtracting the ‚ÄòTypical‚Äô from the ‚ÄòAtypical‚Äô condition, a negative sign indicating a lowering of the measurement in the Atypical condition with respect to the Typical condition.

The main effect which we focused on in our analysis, using a log-normal likelihood, is observed on the Noun Final syllable for ‚Äúrelative Mass‚Äù: Estimate: -0.59 [-0.69, -0.49].

The effect size as estimated by Cohen's d on the posterior distribution is -2.23.

","The unit for the effect size is a standardized mean difference. These were computed from the posterior distributions for each model and each specific comparison
","See answer to an earlier question on the observations excluded (explaining the rationale of phase 2).

An intermediate phase in the analysis influenced the choice of the main Dependent Variable. Indeed, we first looked at patterns of the Typicality effect on the 4 target syllables (corresponding to the noun and adjective in the sequence) as concerns the various measurements of interest (Synchrony, DeltaF0 and Mass). Our analysis was first focused on Mass, as it was the only measurement for which we observed a difference that seemed to be potentially systematic. However, it turned out in the subsequent analyses that some effects occurred on the other two Dependent Variables (DeltaF0 and synchrony) as well, although these were very small, not on the same syllables and with various effect sizes. However no changes in actual measurements or type of analysis applied were made. As these effects were so small and considering the computational power required by Bayesian modeling, due to time constraints, we did not pursue further aspects of the analyses for these two  other variables at this point (e.g. comparing models with varying likelihoods in order to select the best one as was done for Mass).
"
2022-04-15 08:13:58,trachyphyllia_lappa,https://osf.io/sf5z9/?view_only=dfceb143a48d427094c7b385a40a8edd,"Textgrids of recordings were manually corrected in Praat in two phases. In the first phase, 4 team members independently corrected the durations of adjectives and nouns for NF trials. In the second phase, a separate phonetically-trained member with knowledge of German revised the textgrids and identified other error trials to ensure maximal accuracy. After the textgrids were corrected, acoustic measurements were extracted via Praat scripts.",We ran linear mixed effects models using a Gaussian likelihood function.,The team members who conducted the analyses are highly familiar with using mixed effects models.,"We selected four outcome measures to assess the effect of typicality on speech production: duration, minimum pitch, maximum pitch, and pitch range. Pitch measures were included because pitch is a correlate of both stress and intonation ‚Äì both suprasegmental features that might reasonably be expected to be affected by typicality. Duration was included as another marker of stress and as an index of speaking rate.","Our primary independent variable was typicality, given that it was the primary manipulation in the experiment. We also included the color of the target as a covariate, and included random intercepts at the speaker and phrase level. Color was included as a covariate because different colors are likely to produce different baseline duration and pitch measurements, and we wanted to examine the effect of typicality while controlling for the effect of the target color. We also included a random intercept at the speaker level, to account for speaker-level differences in acoustic signals, and a random intercept at the phrase level to account for baseline differences in uttering different word combinations.","We operationalized duration, minimum pitch, maximum pitch, and pitch range at both the adjective (color) and noun (object) level.",We did not apply any additional transformations to the variables.,Observations were excluded if they were marked as an ‚Äúerror‚Äù trial by our team members after the second phase of phonetic coding.,We used a combination of X2 values and a non-overlapping 95% confidence interval of the effect size estimate with 0 as the criterion for evaluating the effect of typicality.,"Our analysis consisted of 8 different models investigating the effect of typicality. We report the effect sizes for each of the six models below:
Duration - adjective: Œ∑p2= .13 (95% CI=[0.00, 1.00])
Duration - noun: Œ∑p2= .94 (95% CI=[0.00, 1.00])
Pitch - maximum - adjective: Œ∑p2= .14 (95% CI=[0.00, 1.00])
Pitch - minimum - adjective: Œ∑p2= .38 (95% CI=[0.00, 1.00])
Pitch - range - adjective: Œ∑p2= .22 (95% CI=[0.00, 1.00])
Pitch - maximum - noun: Œ∑p2= .11 (95% CI=[0.00, 1.00])
Pitch - minimum - noun: Œ∑p2= .17 (95% CI=[0.00, 1.00])
Pitch - range - noun: Œ∑p2= .37 (95% CI=[0.00, 1.00])",Proportion of variance explained,NA
2022-04-15 07:03:37,linckia_nattereri,https://osf.io/8r6x7/?view_only=09f428bd46054ccb9585eab62c12bdad,"Data was first segmented using a custom-built Praat script, then we used Prosogram to propose an acoustic syllable segmentation, followed by SPPAS for forced-alignment of the data.
Next, we use custom-built Praat scripts to filter and extract the acoustic measurements using  Praat, version 6.2.10.
Next, we use custom-built Praat scripts to filter and extract the acoustic measurements using  Praat, version 6.2.10.","We used Principal Component Analysis on the combination of predictors followed by a supervised machine learning algorithm (extremely randomized random forests, with subsampling without replacement and permutation tests). 
We used Principal Component Analysis on the combination of predictors followed by a supervised machine learning algorithm (extremely randomized random forests, with subsampling without replacement and permutation tests). 
We then ran a Bayesian mixed-effects model with the categorical family and a logit link for the response distribution.
","For Machine learning, PCA: Highly. we employ these regularly and have a few publications   Bayesian analysis: very familiar with linear mixed models, not very familiar with Bayesian modeling.","Because we wanted to explore the data, we obtained various metrics on each of the adjective and noun as whole words. Formant-based metrics (F1, F2, F3; mean, SD, range, median), amplitude-based (HNR, in various bands; intensity, hammerberg index, energy components in various bands, energy of the glottal cycle, prominence, etc..) and f0-prosodic-based metrics (f0, min, max mean, median, SD, rise time and speed of rise time). We add our Praat scripts to the analyses for the specifics of how each of these were computed.
","As our independent variable, we only chose typicality. In our inferential statistics, we used both speaker and words. Typicality was used as our outcome in the PCA and in random forests. Within the latter, we specifically chose the first 20 speakers to be part of the training set, while the remaining 20 speakers were in the testing set. Typicality was also chosen as the outcome variable for Bayesian modeling.
","For all of our dependent variables (acoustic metrics), we decided to do the analyses on a word level, first by identifying the adjective and then the noun in the NF context. Once identified, we obtained all the acoustic measurements at a word level. Then we obtained an adjusted measure for each acoustic metric. This adjustment was done as e.g., F1_Mean_Adjusted = F1_Mean_Adjective - F1_Mean_Noun. 
","All of our variables were centered and scaled prior to being used in the PCA or the random forests. The f0 values were obtained using the semi-tones scale (with semitones re 1 Hz). Original formant values were in Bark. 
","None of the observations were excluded. The analyses were adapted to the speaker‚Äôs sex, for both f0 and formant-based analyses to minimise as much as possible extreme values and errors in extractions. 
","Because we decided to do an exploratory data analysis, we looked at % variance explained in our PCA and variable loadings on each of the dimensions. The biplots from the PCA allowed to show clear confusions and overlap between specific levels of typicality.
Next, we used random forests as a classification tool that allowed us to identify the percentage of accurate classification. We assessed accuracy of the classification model using separate training and testing sets, then an AUC-based evaluation of the performance. We used the sensitivity, specificity, recall, precision and the F-measure on the training set and only accuracy and the AUC value for the testing set. In addition, and to try to evaluate any specific differences emerging from the three levels of typicality, we used the confusion matrix to identify accuracies and confusions in classification, in addition to the cumulative gains and AUC-ROC curves. These two allowed us to pinpoint differences related to the typical context being mostly well produced by the participants as it was the best performing level in our classification metric. 
","In the initial steps of data analyses, we used PCA and machine learning in assessing group differences. Following this, we report on the 95% credible intervals from the Bayesian Logit regression as well as the magnitude and direction of the effect for each predictor.
We did not report on any effect size or confidence intervals. We decided to use machine learning to assess performance of the model in identifying differences.
",NA,"Initially, we started with intensity mean adjusted value. Next, we looked at intensity/amplitude-based and prosodic metrics. We tested our models and the performance was relatively low at 70% accuracy. We then increased the sample by looking at formant-based metrics. The combination of the three (formant, amplitude and f0-based metrics) increased the accuracy level of the model to 81%. We did not amend any of our initial analyses.   
"
2022-04-15 08:50:35,haematopus_fossor,https://osf.io/sr63u/?view_only=0305c5465df74ef5abf9b4c2b4b1afc2,"Montreal forced aligner for segmentation; praat for hand-correction and extraction of acoustic measurements (duration, intensity, vowel formants)",We used linear mixed-effects regression models in R.,Very familiar.,"The ‚Äúframe duration‚Äù: the duration in milliseconds from the onset of the utterance to
the start of the adjective. This is the length of time it takes to say ""und jetzt sollst du
den w√ºrfel auf der/den ‚Ä¶"". Rationale: speakers may slow down for atypical items if they are harder to process (cf Fox Tree & Clark 1997).
The duration in milliseconds of the adjective. Rationale: predictable items tend to be phonetically reduced.
The duration in milliseconds of the noun. Rationale: predictable items tend to be phonetically reduced.
The duration in milliseconds of the stressed vowel of the adjective. Rationale: predictable items tend to be phonetically reduced.
The duration in milliseconds of the stressed vowel of the noun. Rationale: predictable items tend to be phonetically reduced.
The mean intensity in dB of the stressed vowel of the adjective. Rationale: predictable items tend to be phonetically reduced.
The mean intensity in dB of the stressed vowel of the noun. Rationale: predictable items tend to be phonetically reduced.
A measure of vowel centralisation of the stressed vowel of the adjective. Rationale: predictable items tend to be phonetically reduced.
A measure of vowel centralisation of the stressed vowel of the noun. Rationale: predictable items tend to be phonetically reduced.","Trial number. Rationale: participants may ""acclimate"" to the unusual trials, such that by the end of the experiment the idea of a red banana isn't so strange.
Typicality. Rationale: this is the research question...
Interaction between typicality and trial. Rationale: There are possible interactions.","Duration was measured over the whole word and over the stressed vowel.
Intensity was measured as a mean over the stressed vowel.
Vowel formants were extracted from stressed vowel midpoint; these values were then used to calculate a centralisation metric.","IVs were centred around the mean prior to analysis. Centred variables have the suffix "".c"" in their column name (e.g. ""duration.c"") while the untransformed variables do not (e.g. ""duration"").",We excluded trials which were marked as having an error or hesitation.,"p-values for the ""typicality"" (and its interactions) variable.","frame duration, medium versus atypical: 0.10 [-0.03, 0.23]
frame duration, typical versus (medium and atypical): -0.07 [-0.20, 0.07]

adjective duration, medium versus atypical: -0.04 [-0.10, 0.02]
adjective duration, typical versus (medium and atypical): 0.07 [ 0.01, 0.13]

noun duration, medium versus atypical: 0.28 [-0.08, 0.64]
noun duration, typical versus (medium and atypical): 0.15 [-0.21, 0.50]

adjective vowel duration, medium versus atypical: 0.04 [-0.06, 0.13]
adjective vowel duration, typical versus (medium and atypical): 0.11 | [ 0.02, 0.21]

noun vowel duration, medium versus atypical: -0.08 [-0.44, 0.29]
noun vowel duration, typical versus (medium and atypical): 6.07e-03 [-0.48, 0.50]

adjective vowel intensity, medium versus atypical: 4.07e-03 [-0.06, 0.07]
adjective vowel intensity, typical versus (medium and atypical): -9.65e-03 [-0.08, 0.06]

noun vowel intensity, medium versus atypical: -0.08 [-0.27, 0.10]
noun vowel intensity, typical versus (medium and atypical): 0.11 [-0.10, 0.32]

adjective vowel centralisation, medium versus atypical: -0.04 [-0.13, 0.05]
adjective vowel centralisation, typical versus (medium and atypical): 0.03 [-0.05, 0.12]

noun vowel centralisation, medium versus atypical: 0.07 [-0.26, 0.41]
noun vowel centralisation, typical versus (medium and atypical): 0.44 [ 0.02, 0.87]
",Standardised regression coefficients,No
2022-04-15 09:13:40,gymnothorax_spinulosus,https://osf.io/p3vzw/?view_only=37c6f15b9840405a83f4f32515f7ba4f,"Manipulations and segmentation of textgrids were done with the TextGridTools library in Python. Forced alignment of the audio files was done with the WebMAUS web service for the Munich Automatic Segmentation System (MAUS). Extraction of acoustic measurements (f0, duration, intensity) was done with the Parselmouth library in Python, using algorithms from the Praat acoustic analyses software.",We ran linear mixed models with R.,We are quite familiar.,"The dependent variable is the intensity of the stressed vowel of the adjective (i.e. the target_colour) in dB minus the average intensity of the sentence (in dB as well). Since dB is a logarithmic scale, calculating the difference actually represents a ratio. We based our decision on the following paper:
Andreeva, B., Barry, W. J., & Steiner, I. (2007). Producing phrasal prominence in German. XVIth International Congress of Phonetic Sciences, (January 2007), 1209‚Äì1212. Retrieved from http://icphs2007.de/conference/Papers/1699/
Also this dependent variable gives very nice shape of the data (like a Gaussian as opposed to doing a ratio of the intensity in dB which was giving a skewed distribution).
","The predictors we selected are:
-	The condition since we expected it to have a high influence on the prosody of the vowel (we expected the adjective to be more stressed in the AF condition than in the NF condition).
-	The typicality. We decided to use the mean typicality (mean_typ). We selected it to answer the research question!",The predictors are the condition since we expected it to have a high influence on the prosody of the vowel (we expected the adjective to be more stressed in the AF condition than in the NF condition).,"No transformation was applied since our data already looked symmetrical, like Gaussian curve.",We excluded values that were +/- 1.5 standard deviations from the median.,"We made the decision based on the p value of post hoc comparisons, given that the interaction between condition and typicality was included in our resulting model.","The size effect we found is 0.799 [0.512, 1.086] 
The effect is found in the AF condition only. It means that the more typical is   the target_colour the higher will be the relative intensity of the stressed vowel vs the rest of the sentence. 
","I‚Äôd say it‚Äôs in dB by unit of typicality (typicality varying from 0 to 1, 0 for low typicality and 1 for high typicality, since we divided the typ_mean variable by 100).","Not really. But this is true that we extracted intensity, pitch and duration and selected intensity because it was the one which was varying the most according to condition and typicality (based on charts, not proper statistical analysis though)."
2022-04-15 10:56:23,clione_dorsalis,https://osf.io/sm9j5/?view_only=1b67eaa8cd3d4913bb4da71e37a7605e,"Acoustic measurements (word duration, F1, F2) were extracted using Praat. Textgrids were hand-labeled in Praat.",We used linear mixed-effects models with random effects for speakers.,Very familiar.,"Based on previous findings that less predictable words are hyperarticulated relative to more predictable words (e.g., Liebermann, 1967), our driving hypothesis was that typicality might condition hyperarticulation. Specifically, less typical words (which are less predictable in their contexts) were expected to be hyperarticulated relative to more typical words (which are more predictable in their contexts). Or, conversely, more typical words were expected to be reduced relative to less typical words.
Taking a multidimensional perspective on hyperarticulation, we looked at both word duration and vowel space areas for stressed vowels in the color words. We chose to analyze the color words rather than the nouns because they are consistent throughout the experiment. 
We predicted that less typical words would be longer and would have more peripheral vowel qualities, yielding bigger vowel space areas, relative to more typical words.
We also looked at F1 and F2 to see whether there were any vowel quality differences not captured by the vowel space area measurement.","Typicality--we predicted that the color words would be hyperarticulated in atypical context compared to typical context
Vowel--we controlled for expected differences in vowel quality across vowels
Gender--we controlled for expected differences in vowel quality by gender
Speaker--we included random intercepts for each speaker to control for individual differences","Duration--Because several of the targets had vowels adjacent to sonorants, making precise segmentation difficult, we decided not to use vowel durations.  Instead, we used full word durations (which were compared across typicality conditions). 
F1 and F2 measures were made at 5 points across the approximately segmented vowel, and points 2 (25% of the way through the vowel) and 4 (75% of the way through the vowel) were analyzed. Since the vowel boundaries were not precise (due to the adjacent sonorants), points 2 and 4 were not at 25% and 75% exactly, but they always captured points early and late in the vowel respectively. Since one of the vowels was a diphthong (/au/), the early and late measures represent values from the nucleus and glide for that vowel.
Vowel space areas (VSA) were calculated for each speaker for each condition (typical, medium, or atypical) using the vowelMeansPolygonArea function in the PhonR package in R. That function calculated the area for a polygon in the F1/F2 vowel space defined by the 5 vowels in the color terms: /y/, /e/, /a/, /au/, and /o/, in that order.
Independent variables (gender, vowel, speaker, typicality) were operationalised as they appeared in the data we were given.",Vowel space area was calculated as described above. No other transformations were applied to the data.,We excluded observations labeled NA for typicality.,We used p-values with an alpha level 0.05.,We found no relevant significant effects.,NA,NA
2022-04-15 12:40:29,neosilurus_omanensis,https://osf.io/k3zv6/?view_only=5924779e3f7549d1b3db9d355d31ac68,Annotation and extraction of duration and pitch was done using Praat. Statistical analyses were performed in R with lmerTest and lme4.,"We ran linear mixed models with random effects for five dependent variables, and performed False Discovery Rate correction for the five sets of results.","Advanced, but no expert.","We used five dependent variables, as previous literature indicated they are associated with prominence. The five variables are average pitch, pitch range (distance between maximum and minimum), pitch rise (difference in pitch between first tenth and last tenth), pitch variation (SD) and speech rate (canonical phonemes / second). For all these variables, we calculated the ratio between the measurement of the adjective and the rest of the utterance/sentence.","We used the median typicality as predictor, as well as the focus condition and an interaction, if these latter two improved the model fit significantly. We included typicality because it was in the research question, and focus condition (AF, ANF, NF) because we expect it has an effect, especially on the prosody of the adjective.",(See above) We extracted values from the adjective and from the rest of the utterance.,(See above) We used ratios.,We excluded speech rate and pitch values that were more than +/- 2 standard deviations from the mean of each sentence.,"We based our decision on the p-values, after FDR correction.","We report multiple effect sizes, but as we have not found a significant effect, we leave this field empty as there is no effect to report.",Eta-squared.,We had to exclude many more values as outliers than we had hoped (and the amount of outliers also prevented us from manually correcting pitch measurements).
2022-04-15 13:24:37,dunkleosteus_inscriptus,https://osf.io/4ud5s/?view_only=5b28c64535d64e03be2af8e606750a29,"Matlab: file manipulation, signal processing, TextGrid extraction and manipulation, speech auditing
BAS WebServices / WebMAUS: signal conditioning, phonemic transcription, forced alignment, acoustic segmentation, TextGrid generation
R: data wrangling, data cleaning, data visualization, statistical analysis","Linear Mixed-Effects models with NHST
Bayesian Mixed-Effects models with ROPE analysis",Quite familiar with LMEs and NHST; relatively new to Bayesian Mixed Effects modelling,"Duration of target noun phrase and associated constituents. Duration was chosen because it is easier to measure more consistently than other phonetic correlates of prosody, because it can be measured automatically, and because the analysis method I've proposed based on duration metrics is perfectly reproducable.","Condition, Typicality, Speaker ID, target_name, target_colour. The first two are intrinsic to the research question and study design; the last three are the major influences on duration.","Duration was calculated from acoustically segmented speech annotations generated by forced alignment of each trial. Duration was determined for the entire utterance and each major synatactic consistuent, down to the level of the individual words, quantified as a continuous variable in seconds, quantized to 10ms. All other variables were treated as they were presented in the Trial lists and the description of the study design.","Absolute durations were calculated for each word and constituent of interest. Two additional metrics were derived from these data: relative duration of noun and adjective, calculated as ratios of the total durations of the Determiner phrase that contains them.","All Trials flagged as 'error' in the original Praat notes tier were discarded.
All additional trials observed to contain production errors or large hesitation pauses were excluded.
All trials that could not be accurately acoustically segmented were excluded.
All trials with durations of constituents shorter than mean-2 s.d and longer than mean +3 s.d were excluded, and additional outliers were excluded based on constituent-specific determinations of extrema and non-linearities identified in quintile-quintile plots.",Bayesian posterial distributions with respect to Regions of Practical Equivalence,"Relative duration of target N/DP,    Atyp -> Typ:  -0.02  [-0.03, 0.00]
Relative duration of target Adj/DP, Atyp -> Typ:   0.01  [-0.01, 0.03]",Ratio of durations [0..1],"Recording noise adversly affected the accuracy of forced alignment for some speakers to a greater extent than was aparent from analyzing the first five speakers. Noise attenuation parameters were subesuently adjusted in WebMAUS to improve alignment, and the first five speakers were re-analyzed.

The set of duration measurements extracted from TextGrids with a Matlab script were later found to be incomplete, and one constituent was mis-labelled. These issues were corrected in R."
2022-04-15 13:25:52,anomalocaris_ornata,https://osf.io/xujc4/?view_only=74e83f9ff443424b928bf9f0fb15cb72,"New text grids were created in Praat. Updated Praat TextGrids were then uploaded to the WebMAUS interface (Kisler, Reichel, and Schiel 2017) and auto-aligned through the pipeline without ASR (CHUNKKPREP ‚Äì> G2P ‚Äì> MAUS ‚Äì> PHO2SYL) (Reichel 2012; Poerner and Schiel 2018) which is able to process German audio data. The TextGrids were then run through a Praat script which returned measurements for intensity, duration (both at word and phone level) and F0 at 10 time points within each voiced phoneme. The script used was adapted to include the German phonemes in the sample from a script made available online by Wendy Elvira Garc√≠a (available at: https://github.com/wendyelviragarcia/vowels).
",We ran linear mixed effects models with Gaussian distributions.,Both fairly familiar with the technique.,"We decided to look at three measures in order to approach the given research question: 1) Intensity in dB of the entire word (for colour and object), 2) F0 as an acoustic correlate of what is perceived as pitch (looking at only the stressed vowel in both colour and object) and 3) vowel duration (again of only the stressed vowels in both the colour and object levels). We selected these dependent variables as they are known to index accentedness in the German language.","Typicality (categorical) , col_obj (a 2 level variable of either colour or object), voice_type (2 levels of either lower or higher F0). We used categorical typicality as the mean typicality had very few values to run as a continuous variable. For F0. voice_type was important as findings for pitch have shown significant variation for gender. Not knowing the gender of the speaker, voice_type deals with some of this variation. We focused on just the colour and object as we thought this is where the variation would best show.

","We decided to look at three measures in order to approach the given research question: 1) Intensity in dB of the entire word (for colour and object), 2) F0 as an acoustic correlate of what is perceived as pitch (looking at only the stressed vowel in both colour and object) and 3) vowel duration (again of only the stressed vowels in both the colour and object levels). 

 Key for this analysis was the reduction of words down to only those of interest (object and colour) and the creation of a new two-level column `(col_obj)` which identified words as being either `colour` or `object`. The column `voice_type` was also added to the data frame which contained a two-level factor assigning each speaker as either having `low` or `high` pitch. Both authors coded the speakers auditorily and were in agreement for all 30 speakers. F0 means and ranges were also checked for each group with a cut off of <150Hz (~12 Semitones) for the lower group and >150 Hz for the higher group.

Col_obj was manually coded in R as either colour or object.","For Intensity in dB, we squared the predictor variable (Mean_dB^2) as the distribution was skewed left.

As the f0 residuals were bimodal, we cut the data into measures above and below 150Hz (~10 Semitones) where the two local peaks in the distributions were. This did produce model residuals for both models which were visually much more normally distributed and more appropriate for the assumptions made in linear modelling.","Values over 1.5* the interquartile range were removed to mitigate against alignment issues.

26 production errors were also added to the database. These were independently checked by the authors and are not completely in line with those provided in the original database.
","NHST, p-values","Intensity:
 The model included col_obj and speaker as random effects (formula: ~1 + col_obj | speaker). The model's total explanatory power is moderate (conditional R2 = 0.23) and the part related to the fixed effects alone (marginal R2) is of 0.04. The model's intercept, corresponding to typicality = typical, col_obj = colour and voice_type = high, is at 62.89 (95% CI [61.66, 64.12], t(11076) = 100.20, p < .001). Within this model:

  - The effect of typicality [medium] is statistically non-significant and negative (beta = -0.24, 95% CI [-0.62, 0.14], t(11076) = -1.23, p = 0.218; Std. beta = -0.04, 95% CI [-0.09, 0.02])
  - The effect of typicality [atypical] is statistically significant and negative (beta = -0.45, 95% CI [-0.83, -0.07], t(11076) = -2.31, p = 0.021; Std. beta = -0.07, 95% CI [-0.12, -0.01])
  - The effect of col_obj [object] is statistically significant and negative (beta = -0.49, 95% CI [-0.96, -0.01], t(11076) = -2.01, p = 0.045; Std. beta = -0.07, 95% CI [-0.14, -1.76e-03])
  - The effect of voice_type [low] is statistically non-significant and negative (beta = -1.30, 95% CI [-4.07, 1.47], t(11076) = -0.92, p = 0.359; Std. beta = -0.19, 95% CI [-0.61, 0.22])
  - The interaction effect of col_obj [object] on typicality [medium] is statistically significant and negative (beta = -1.99, 95% CI [-2.51, -1.46], t(11076) = -7.38, p < .001; Std. beta = -0.30, 95% CI [-0.38, -0.22])
  - The interaction effect of col_obj [object] on typicality [atypical] is statistically significant and negative (beta = -1.90, 95% CI [-2.45, -1.36], t(11076) = -6.87, p < .001; Std. beta = -0.29, 95% CI [-0.37, -0.20])


Duration
The model included Word and speaker as random effects (formula: list(~1 | Word, ~1 | speaker)). The model's total explanatory power is substantial (conditional R2 = 0.65) and the part related to the fixed effects alone (marginal R2) is of 0.06. The model's intercept, corresponding to typicality = typical and col_obj = colour, is at 91.02 (95% CI [54.41, 127.63], t(1691) = 4.87, p < .001). Within this model:

  - The effect of typicality [medium] is statistically non-significant and positive (beta = 3.06, 95% CI [-2.39, 8.52], t(1691) = 1.10, p = 0.270; Std. beta = 0.06, 95% CI [-0.05, 0.17])
  - The effect of typicality [atypical] is statistically non-significant and positive (beta = 2.37, 95% CI [-3.08, 7.82], t(1691) = 0.85, p = 0.394; Std. beta = 0.05, 95% CI [-0.06, 0.16])
  - The effect of col_obj [object] is statistically non-significant and positive (beta = 33.08, 95% CI [-16.02, 82.17], t(1691) = 1.32, p = 0.187; Std. beta = 0.66, 95% CI [-0.32, 1.64])
  - The interaction effect of col_obj [object] on typicality [medium] is statistically non-significant and negative (beta = -6.75, 95% CI [-47.84, 34.34], t(1691) = -0.32, p = 0.748; Std. beta = -0.13, 95% CI [-0.96, 0.69])
  - The interaction effect of col_obj [object] on typicality [atypical] is statistically non-significant and negative (beta = -33.14, 95% CI [-85.79, 19.50], t(1691) = -1.23, p = 0.217; Std. beta = -0.66, 95% CI [-1.72, 0.39])

F0 GLOBAL
The model included col_obj and speaker as random effects (formula: ~1 + col_obj | speaker). The model's total explanatory power is substantial (conditional R2 = 0.53) and the part related to the fixed effects alone (marginal R2) is of 0.07. The model's intercept, corresponding to col_obj = colour, typicality = typical and voice_type = high, is at 16.17 (95% CI [15.56, 16.79], t(54696) = 51.44, p < .001). Within this model:

  - The effect of col_obj [object] is statistically significant and negative (beta = -0.88, 95% CI [-1.18, -0.57], t(54696) = -5.67, p < .001; Std. beta = -0.40, 95% CI [-0.53, -0.26])
  - The effect of typicality [medium] is statistically non-significant and negative (beta = -0.04, 95% CI [-0.08, 3.84e-03], t(54696) = -1.78, p = 0.075; Std. beta = -0.02, 95% CI [-0.04, 1.73e-03])
  - The effect of typicality [atypical] is statistically significant and positive (beta = 0.20, 95% CI [0.16, 0.24], t(54696) = 9.53, p < .001; Std. beta = 0.09, 95% CI [0.07, 0.11])
  - The effect of voice_type [low] is statistically significant and negative (beta = -3.63, 95% CI [-5.16, -2.10], t(54696) = -4.65, p < .001; Std. beta = -1.64, 95% CI [-2.33, -0.95])
  - The interaction effect of typicality [medium] on col_obj [object] is statistically significant and positive (beta = 0.44, 95% CI [0.38, 0.51], t(54696) = 14.02, p < .001; Std. beta = 0.20, 95% CI [0.17, 0.23])
  - The interaction effect of typicality [atypical] on col_obj [object] is statistically significant and positive (beta = 0.28, 95% CI [0.21, 0.34], t(54696) = 8.42, p < .001; Std. beta = 0.13, 95% CI [0.10, 0.15])


F0 LOCAL
The model included col_obj and speaker as random effects (formula: ~1 + col_obj | speaker). The model's total explanatory power is substantial (conditional R2 = 0.57) and the part related to the fixed effects alone (marginal R2) is of 0.04. The model's intercept, corresponding to col_obj = colour, typicality = typical and voice_type = high, is at 4.75 (95% CI [3.91, 5.59], t(16863) = 11.10, p < .001). Within this model:

  - The effect of col_obj [object] is statistically non-significant and negative (beta = -0.17, 95% CI [-0.99, 0.65], t(16863) = -0.41, p = 0.682; Std. beta = -0.06, 95% CI [-0.34, 0.22])
  - The effect of typicality [medium] is statistically significant and positive (beta = 0.25, 95% CI [0.14, 0.36], t(16863) = 4.39, p < .001; Std. beta = 0.08, 95% CI [0.05, 0.12])
  - The effect of typicality [atypical] is statistically non-significant and positive (beta = 0.03, 95% CI [-0.08, 0.14], t(16863) = 0.47, p = 0.639; Std. beta = 9.07e-03, 95% CI [-0.03, 0.05])
  - The effect of voice_type [low] is statistically non-significant and positive (beta = 1.21, 95% CI [-0.67, 3.08], t(16863) = 1.26, p = 0.207; Std. beta = 0.41, 95% CI [-0.23, 1.06])
  - The interaction effect of typicality [medium] on col_obj [object] is statistically non-significant and positive (beta = 0.06, 95% CI [-0.08, 0.21], t(16863) = 0.85, p = 0.396; Std. beta = 0.02, 95% CI [-0.03, 0.07])
  - The interaction effect of typicality [atypical] on col_obj [object] is statistically significant and positive (beta = 0.34, 95% CI [0.18, 0.50], t(16863) = 4.16, p < .001; Std. beta = 0.12, 95% CI [0.06, 0.17])",Conditional R2 values?,There were issues normalising the data as some features of interest were not normally distributed. Left negative skews were particularly difficult to overcome.
2022-04-15 13:36:45,pseudopleuronectes_assasi,https://osf.io/uwqdc/?view_only=bbd44537dedb4794be0468fd1287fb69,"We used Praat to annotate the recordings and extract acoustic measurements (F0, duration, F1 + F2, intensity) from them, as well.
We used R for the statistical analysis (linear mixed-effects models).
We developed a number of customized Perl scripts to help us process the data, as well (segmenting the original sound files, converting Hz to Semi-tones, comparing timings for our inter-transcriber analysis, etc.) 
",We employed the linear mixed-effects models and ANOVA.,Very familiar.,"We used 4 dependent variables, taken from the stressed vowels of the adjective and noun of the target phase in each utterance: (1) Duration of the vowel, (2) Median F0 of the vowel (in Semi-Tones), (3) Median intensity of the vowel, and (4) Formant Distance. 
We calculated Formant Distance as the Euclidean Distance of the vowel from the speaker's mean F1/F2 in a normalized vowel space. We chose to look at the stressed vowels of the target words, so we measured the main acoustic properties of stress.
Each dependent variable was tested separately, i.e. we ran separate tests for each dependent variable.","We have four independent variables. 1) Word (v1, v2), Focus (AF, NF, ANF), Typicality (typical, atypical, medium typical), and Sex (male, female). These were the relevant variables for the research question and the data. Typicality is the main target variable for the question, but given the data we were given, word and focus were included as they were part of the design of the original study. We also included Sex only for the F0 test, since it is a known factor for F0. It was not possible to include all independent variables in a single test due to the nature of the original data, so each test has different independent variables.","We measured the stressed rhyme for each word in the target Adjective/Noun phrase. This includes the target vowel of diphthong and any sonorant codas (/l, r, n/). All measurements were extracted automatically with a Praat script, except for the Formant Distance. For that, we first measured with Praat F1 and F2 from a time point 1/3rd of the way through the rhyme, then converted those into Z-scores for each speaker (Lobanov's method), and then measured the Euclidean distance of each vowel from the speaker's mean F1+F2 point.","We converted the F0 values to Semi-Tones, using the formula provided by Reetz & Jongman (2020). We used 75 Hz as the baseline for this calculation. 
We also converted F1 + F2 measures to a general measure of ""Formant Distance"" according to the method described above.","We excluded items that were labeled as mistakes in the provided textgrids and any items that had mistakes, disfluencies, or noise.
We also excluded from the Formant Distance any items whose F1 or F2 z-scores were greater than 2, as this enabled us to eliminate the most obvious cases where the Praat formant tracker had missed measuring one of the formants. 
Finally, for F0 (ST), we set the lower end of the pitch range at 75 Hz for male speakers, and 100 Hz for female speakers. This enabled us to avoid the most obvious cases of pitch halving in the automatically generated F0 data.",p-values < .05,"We tested the effects of typicality on four different acoustic measures and found no significant effect for any of them. In those cases, there is no ""effect size"" to report. 
However, we also tested the effects of focus on adjectives and nouns in the ""atypical"" condition. These tests yielded significant results for all of our dependent measures, generally in the direction of greater acoustic prominence for focused items. 
We reported the estimates of these effect sizes from the output of the linear mixed-effects models in the write-up of our findings. Since there are many of them, we won't repeat them all here.",We were not sure how to standardize effect sizes for the output of linear mixed-effects models and thus just reported the raw effect size for the specific variables we tested in each case.,"No, we did not."
2022-04-15 14:18:08,lycodes_bradfieldi,https://osf.io/4b37w/?view_only=e49044c33697476b9091479baee28eb3,"mPraat was used to important audio and text data into MATLAB
acoustic features were extracted in MATLAB. Their extraction relied on:
The Signal Processing Toolbox
Code from the Varnet et al., 2017 paper TODO update
JASP was used to run final statistical models
",We ran repeated measures analysis of variance (ANOVAs) in JASP.,We are very familiar with these statistical techniques.,AM and Mi were our dependent variables. Our rationale was that surprise (driven by typicality) should lead to changes in the amplitude modulation spectrum indices.,"Typicality and frequency band were our independent variables. Typicality was our main variable of interest, but frequency band was also considered relevant as we wanted to know which bands (if any) showed a significant change in power by typicality condition. Their interaction was also included in our ANOVA.","Dependent variables were extracted as described above (in Analysis Workflow). 
Independent variables were operationalized as follows: 
Typicality: by the three levels (‚Äòatypical‚Äô, ‚Äòmedium‚Äô, ‚Äòtypical‚Äô provided in the data set, and determined based on a previous norming study). 
Frequency band: by standard ranges used in the neuroscientific literature 
",No transformations,"No data were excluded. The number of trials per condition is already very low. The analyzed utterances were meant to be instructions, to which no clear exclusion criterion applies.","The answer was decided based on significance, or absence thereof, of the main factor Typicality, and of its interaction with the Frequency Band factor. Significance was defined as p <.05, after Bonferroni correction.","Partial eta square values: Typicality = 0.253; Band = 0.816; Interaction = 0.206. However, in the context of the original eta square values, which for Typicality and the Interaction were below 0.004, we deem the effect size of the Typicality factor very small.    

Mean difference and CIs for all post hoc comparisons are reported below 

Post Hoc Tests
Post Hoc Comparisons - typicality ‚úª bands


95% CI for Mean Difference






Mean Difference
Lower
Upper
SE
t
p bonf
atypicality, delta


medium, delta


0.010


-0.026


0.045


0.010


1.006


1.000


 


typicality, delta


0.016


-0.019


0.052


0.010


1.724


1.000


atypicality, theta


medium, theta


0.011


-0.025


0.046


0.010


1.107


1.000


 


typicality, theta


0.042


0.006


0.077


0.010


4.361


0.004


atypicality, alpha


medium, alpha


0.013


-0.022


0.049


0.010


1.393


1.000


 


typicality, alpha


0.051


0.016


0.086


0.010


5.349


5.380e‚Äâ -5


atypicality, beta


medium, beta


0.024


-0.011


0.059


0.010


2.504


1.000


 


typicality, beta


0.052


0.017


0.087


0.010


5.465


3.155e‚Äâ -5


atypicality, lower_gamma


medium, lower_gamma


0.026


-0.009


0.061


0.010


2.727


1.000


 


typicality, lower_gamma


0.036


8.904e‚Äâ-4


0.071


0.010


3.778


0.036


atypicality, higher_gamma


medium, higher_gamma


-0.025


-0.060


0.011


0.010


-2.566


1.000


 


typicality, higher_gamma


-0.002


-0.037


0.033


0.010


-0.224


1.000






","Standardized mean difference.
Partial eta square measures the proportion of variance explained by the factor of interest, out of the total variance remaining after the variance explained by the other factor (or factors) is partialled out. Hence, it is a meaningful index only when evaluated in the context of the other factors.
","We initially were aiming at analyzing also the frequency modulation spectrum, but given the presence of many NaNs in the data (short utterances), this part of the project was put aside. 
"
2022-04-15 15:50:46,arapaima_modularis,https://osf.io/6fnq7/?view_only=dfb628c5b704478e86930ec7e383909c,Praat was used for coding (simultaneous analysis of audio and text grid) and for extraction of acoustic measurements (pitch),Repeated measure ANOVA,Quite familiar,"The outcome variable is the mean pitch score (numeric, in Hz). Speakers in many languages use pitch modifications as a phonetic cue to draw the listener‚Äôs attention to a certain part of the utterance. For example, for an English speaker, the pitch maximum in ‚Äòbananas‚Äô in the utterance ‚ÄúI bought bananas‚Äù will normally be higher in a contrastive focus context (e.g., ‚ÄúI bought bananas, not pears‚Äù) than in the non-contrastive declarative counterpart. I believe participants in this study could use a similar process to mark adjectives that appear in atypical adjective + noun combinations. For example, the atypicality of the adjective ‚Äòrot‚Äô (Eng., ‚Äòred‚Äô) in relation to the noun ‚ÄòGurke‚Äô (Eng., ‚Äòcucumber‚Äô) could prompt speakers to mark this atypicality by using higher pitch in the modifier adjective.","The within-subject factor is the typicality level between the target adjective and the noun (three levels: typical, medium, atypical). This factor was chosen as the independent variable since the research question seeks to answer if typicality variations have an effect on the acoustics of the utterance.","Pitch was operationalized as the pitch maxima (in Hz) in adjectives of NF adjective + noun combinations. 
Typicality was operationalized as having three levels: atypical, medium, typical, based on norming and production studies supplied by MSA project.",N/A,"Target adjectives were marked as containing errors in the following circumstances:
o	A pitch track was not visible at all.
o	Creaky voice inside the target adjective affected the pitch track.
o	The pitch track contained unusual excursions, such as sudden shifts or isolated spikes.
o	The trial contained an annotation in the ‚Äònotes‚Äô tier, as provided by the MSA project. In most cases, the annotation in these cases was simply ‚Äòerror‚Äô, but sometimes ‚Äòhesitation break‚Äô or ‚Äòstructure‚Äô were also used.  Since I do not know enough German to evaluate the error gravity or type for these trials, I decided to mark all of them as errors. 
All target adjectives with any kind of error were subsequently excluded from analysis.

During analysis, seven participants were excluded:
o	In order to preserve representativeness of each target adjective, speakers whose mean pitch score on any typicality level resulted from less than six original scores were excluded from analysis. This process eliminated three speakers.
o	Outliers were identified, using identify.outliers(), which determines limits for outliers by multiplying the interquartile range (IQR) by 1.5. This identified 13 outlier data points (that is, 13 mean pitch scores), 12 of which belonged to four participants. That is, these four participants had outlier scores in every typicality level. These four participants were removed from the analysis.","I use the p-value. F(2, 44) = 2.967, p = .0618",Effect size = .0028,eta_squared,I reran analysis only to account for outliers.
2022-04-15 15:48:32,epinephelus_aztecus,https://osf.io/k9u8b/?view_only=3184c9f971d44813af317a00e8fe34df,"All acoustic analysis was conducted with Praat. 
This included the following: 
-	Creating a textgrid tier below the existing tiers to specify the Det+Adj combinations to be analyzed. We chose to analyze the Det+Adj combination because the Det often included transitions that were difficult to segment from the adjective (e.g., ‚Äúder‚Äù).
-	Finding the boundaries for the Det+Adj was done in Praat by hand by multiple (n=6) coders. Each coder was assigned to annotate all trials for 5 participants and the first three trials for all other participants. Boundaries were created at zero crossings using large acoustic/auditory cues. Boundaries that were unable to be detected were flagged, and the team lead made a final decision as to the appropriate boundary, and whether to discard the trial if necessary. All trials that were flagged as errors in the original Praat files were discarded. 
-	Running a script to extract fundamental frequency (F0), duration and intensity measurements. The specific measurements extracted were: duration, average intensity, average F0, F0 range (max-min), intensity range (max-min).",We used path analysis from the Structural Equation Modeling (SEM) family of methods. The analysis was conducted in R (R Core Team 2020) and the package lavaan (version 0.6-7; Rosseel 2012) was used to fit the model using maximum likelihood estimation. The coefficients from SEM were compared with the estimates of the mixed-effect linear regression accounting for the random intercepts by speaker.,"The level of familiarity with the analytical/statistical technique was extremely variable in the team, ranging from zero experience to advanced. The team member in charge of the analysis was considered proficient-to-advanced.","1.	duration of the determiner-adjective combination
2.	mean intensity
3.	mean pitch
4.	intensity range
5.	pitch range. 

All dependent variables are measured and not latent. The dependent variables were chosen after consultation with the literature on the kinds of measurements that were done in similar research (e.g., information structure, typicality effects), and would likely show an effect. We also chose variables that were relatively easy to measure/define using Praat scripts over multiple trials/participants.
","‚Ä¢	Typicality, categorical with 3 levels: typical, medium, atypical. Coded into two dummy variables: ‚Äòmedium‚Äô (contrast medium and typical), and ‚Äòatypical‚Äô (contrast typical and atypical). Was chosen to address the research question that was posited.
‚Ä¢	Focus, categorical with 3 levels: Noun Focus, Adjective Focus, and double Noun-Adjective Focus. Coded into 2 dummy variables: ‚Äòaf‚Äô (contrast AF with NF which is a reference group) and ‚Äòanf‚Äô (contrast ANF and NF). Was chosen to determine how the acoustic characteristic change when the speaker is not faced with a need to facilitate disambiguation even if having to produce an atypical noun-adjective combination, either due to the lack of the risk to run into an atypical combination (as in AF, where nouns with no prototypical colors (sunglasses, paperclip, etc.) were used) or due to the lack of disambiguation that needs to be facilitated (as in ANF, where, even if the noun-adjective combination was atypical,  both noun and adjective competitors were different from targets). 
‚Ä¢	Random effect variable -  Speaker: categorical with 30 levels. Was chosen to account for the variability in acoustic measures between speakers.",The levels of the dependent variables were operationalized as defined by the original design of the study. We chose to analyze the Det+Adj combination because it was difficult to extract the Det from the Adj. We chose the whole two-word combination to extract information from.,"Typicality was treatment-coded, with two new categorical variables created: ‚Äòmedium‚Äô to represent the contrast between medium and typical conditions and ‚Äòatypical‚Äô to represent the difference between typical and atypical ones.
Condition was treatment-coded as well, with two new categorical variables created: ‚Äòanf‚Äô to compare ANF (adjective-noun focus) to NF (Noun Focus) that‚Äôs used as a reference group; and ‚Äòaf‚Äô to compare the sentences with AF (adjective focus) to NF.","Observations were excluded if they had been flagged as an error or hesitation in the original files provided to the team. Further, cases where boundaries were unable to be placed after inspection by multiple team members were also excluded.","Interpretations were made based on the combination of the goodness of fit parameters, p-values and confidence intervals. In the regression, the t-value was examined  instead of p-value.","Effect sizes for medium typicality: 
Multivariate effect size of duration for medium typicality= -0.07615017 (6.972037e-03)
Multivariate effect size of intensity mean for medium typicality = -0.10337176 (6.974900e-03)
Multivariate effect of intensity range for medium typicality = -0.15656113 (0.0069829895)
Multivariate effect size of pitch mean for medium typicality = 0.01753193 (0.0069688211)
Multivariate effect size of pitch range for medium typicality = -0.24361498 (1.196977340)
Effect sizes for atypical typicality: 
Multivariate effect size of duration for atypical condition = 0.01974490 (7.068778e-03)
Multivariate effect size of intensity mean for atypical condition = -0.10072332 (7.074492e-03)
Multivariate effect size of intensity range for atypical condition= 0.05763679 (0.0070704946)
Multivariate effect size of pitch mean for atypical condition= 0.06101845 (0.0070707301)
Multivariate effect size of pitch range for atypical condition = -0.10035993 (0.0070744492)

The sample sizes were calculated for the purposes of multivariate (not univariate) meta analysis using structural equation modeling which does not allow to compute the confidence intervals for the effect sizes. Instead, it provides the sample variances effect sizes which are equivalent to CIs for the purposes of meta analysis.",Standardized mean difference,The structure of the model was adjusted based on the badness of fit of the original hypothesized model and computed modification indices.
2022-04-15 15:58:58,cromileptes_saxatilis,https://osf.io/gy4h2/?view_only=c5414465ca3c487887c131eaa6f31c8a,Praat was used for editing textgrids and extracting word duration measurements. The Montreal Forced Aligner (MFA) was used to create word-level alignments.,Bayesian mixed-effects regression model with lognormal distribution for the outcome variable and a log link function.,I have used this technique before and have a good amount of experience with it.,"The analysis looks at the effect of the typicality of adjective + noun combinations in German (e.g. yellow banana) on their prosody, focusing specifically on the duration of the adjective as measured in seconds. This focus is motivated by two facts. First, the predictability of a word based on its preceding and following neighbours is known to have a strong, highly replicable and cross-linguistically robust effect on its duration (Bell et al. 2009, Seyfarth 2014, Tang & Shaw 2021). Arguably, the typicality of an adjective + noun sequence is closely related to the backward predictability of the adjective based on the noun, and the forward predictability of the noun based on the adjective. This link between typicality and predictability suggests that word duration is an ideal measure to investigate typicality effects. Second, the focus is on the duration of the adjective for the simple reason that while each adjective occurred in all three typicality conditions (atypi-cal, medium, typical), the nouns each only occurred in a single typicality condition. Be-cause of this, the phonological makeup of the nouns (which is inevitably the strongest predictor of word duration) is confounded with the typicality condition in which they appear. Any duration measure that involves the nouns will be affected by this confound, and even sophisticated controls for word duration (e.g. baseline durations based on speech synthesis; Seyfarth 2014) are unlikely to fully resolve this issue. Therefore, the focus here is on the adjectives only.",The categorical typicality variable was chosen as the fixed effect independent variable. The rationale was simply that this seemed to align the most closely with the experimental design.,"The dependent variable was the duration of the adjective (the entire word) as measured in seconds. The independent variable, as noted above, was simply ""typicality"" as operationalised in the original data (atypical, medium, typical).","No transformation was applied before entering the duration values into the model ‚Äì however, the lognormal model does perform a sort of a log transform internally through the link function. The rationale for using the lognormal model is that duration measurements are bounded at 0 and therefore follow a skewed distribution.","Only typical / atypical combinations were examined (medium excluded), and only in the NF condition. Tokens from utterances with any text on the notes tier (typically ""error"") were excluded, as were tokens where there was any pause between the adjective and noun as determined by the forced aligner.",The decision was based on Bayesian credible intervals.,"Cohen‚Äôs D: ‚Äì0.028, CrI [‚Äì0.58,0.51] (typical has a lower estimated duration than atypical; raw values in log duration: ‚Äì0.012 with a 95% CrI of [‚Äì0.256, 0.196])",Cohen's D,"There were very few revisions as the analysis was designed with minimising researcher degrees of freedom in mind (within reasonable limits, of course!). One change that took place after inspecting preliminary data was to add an alternative pronunciation of ""orangen"" to the forced aligner and re-align the textgrids. The final model also had to be pared down slightly to address divergent transitions (but, as noted in the text & analysis script, the original non-convergent model produces the same results).

I also originally wanted to analyse the duration of the entire adjective + noun sequence. After extracting & wrangling the duration data, I realised that this was not feasible due to the fact that each noun only appears in a single typicality condition, and therefore typicality is confounded with the phonological makeup of nouns. This was, however, before running the statistical analysis, and it did not lead to any changes in the data extraction / data wrangling process."
2022-04-15 15:33:29,polymetme_brevirostrum,https://osf.io/sdm95/?view_only=7da6647a70c641fca1a087e7b17d2243,"f0 and duration measurements were collected with PRAAT. Metadata originally included in the PRAAT TextGrids (e.g., Trial, Condition) were extracted by means of a PRAAT script. Audio was aligned to text using the Montreal Forced Aligner with the german_mfa dictionary and acoustic model. Statistical analysis and data visualization was conducted in R.","We analyzed the data using linear mixed effects regression under a Bayesian approach with the rstanarm package in R (Goodrich et al. 2020; R Core Team 2021). We used treatment coding for Condition, setting the reference level at NF based on the conjecture that conditions which involved focus specifically on the adjective should lead to higher pitch ranges than the case where focus was on the noun. We used custom contrasts for Typicality to allow us to compare Atypical category to the Typical category as well as compare the Atypical category to the mean of the other categories. Finally, we used treatment coding for Category, setting the reference level on NONFOOD because the expectation is that non-foods come in a wider variety of colors. We began by fitting a maximal model justified by the experimental design: main effects for Typicality, Category, and Condition as well as all interactions. Random Effects included random intercepts for Object, Colour, and Speaker and random slopes matching the fixed effects (except for any slopes dealing with Category for the Object random intercept, as it already captures the notion of Category. We fit this model using weakly informative Gaussian priors that were autoscaled by rstanarm. We set the adapt_delta parameter, which penalizes larger step transitions, to a conservative value of 0.999. We sampled four chains from the posterior distribution with 2000 steps equally split between warmup and sampling. Model fit was assessed using visual posterior predictive checks and leave-one-out cross validation with a Pareto k diagnostic set at a threshold of 0.7. We then simplified the model beginning by removing the random slopes and higher-order terms; we determined a stopping point based on a comparison of ELPD (expected log pointwise predictive density) values.",I have advanced intermediate knowledge of this technique,"The research question is whether the acoustic profile of an utterance with an typical referent differs from one with a more typical referent, so we began by searching for phonetic and phonological studies that focused on the manipulation of predictability and/or typicality in German. A cursory examination of the literature suggests that contrastive (narrow) focus in German is marked by a rising tone with a low boundary tone (L+H* L%; cf. Weber et al., 2006; Fery & Wang, 2018) as opposed to a flat high tone in broad focus utterances, and this pitch accent is also seen in unexpected or less common adjectives. We operationalize this hypothesized difference using pitch range within the adjective as a dependent variable. Specifically, we hypothesize that pitch range should be higher within the adjective for utterances where that adjective is atypical and the utterance is focused.","We included Condition, Typicality, and Category as independent variables in the model. We expected main effects of Condition (AF >= ANF > NF) and Typicality (Atypical > Medium >= Typical) as well as an interaction between Typicality and Category (e.g., Atypical foods should show wider pitch excursions than atypical nonfoods, because foods are much more likely than nonfoods to have one canonical color).","We selected f0 range within the adjective, assuming that this would index pitch excursion. Given that the adjective appears first and carries the majority of the pragmatic value in terms of typicality/surprisal, we restricted our analysis to this area specifically.We used treatment coding for Condition, setting the reference level at NF based on the conjecture that conditions which involved focus specifically on the adjective should lead to higher pitch ranges than the case where focus was on the noun. We used custom contrasts for Typicality to allow us to compare Atypical category to the Typical category as well as compare the Atypical category to the mean of the other categories. Finally, we used treatment coding for Category, setting the reference level on NONFOOD because the expectation is that non-foods come in a wider variety of colors. We began by fitting a maximal model justified by the experimental design: main effects for Typicality, Category, and Condition as well as all interactions. Random Effects included random intercepts for Object, Colour, and Speaker and random slopes matching the fixed effects (except for any slopes dealing with Category for the Object random intercept, as it already captures the notion of Category.",We did not transform the variables.,"We excluded any observation that had comments in the ""Notes"" tier of the text grids.",We used the posterior distribution and 95% credible intervals to answer the research question,No significant effects based on 89% % in region of practical equivalence (ROPE)  (cf. Schwaferts & Augustin 2020),% in region of practical equivalence (ROPE) based on 89% HDI (cf. Schwaferts & Augustin 2020),The only revisions I had to make pertained to formatting the data to reflect the information in the study pdf document.
2022-04-15 20:39:28,anthracoceros_coronata,https://osf.io/za76s/?view_only=c9b8e61d9bb0494e9d1498d78dc20ab9,"Segmenting of sound files and TextGrids was done in Praat.  The praatR and rPraat packages were used to control Praat from R, and in so doing to create Intensity and Pitch objects.  All acoustic measurements came from the Intensity, Pitch, and TextGrid objects, queried from Praat, through the R packages mentioned above.","I initially was using linear mixed models, but convergence issues suggested that the model couldn't support random effects, so I used standard linear regression.",Fairly familiar,"Dependent variables used were: maximum intensity for adjective and for noun, maximum pitch for adjective and noun, and duratiaon for adjective and noun.

Intensity, pitch, and duration were chosen since they are common correlates of stress or focus.

For intensity and pitch, I measured both the maximum and the mean for each word.  I ended up using the maximums because they were more normally distributed than the means.","The only independent variable was mean typicality, directly from the provided typ_mean column in the trial lists.  the ""typicality"" column was not used because it was not available for all conditions due to the setup of the experiment.","For dependent variables, the duration, pitch, and intensity were measured from each word.  I used the maximum of each of these measures for each word (as opposed to the mean) because they were more normally distributed.

For the independent variable, typ_mean, it was left as-is.",Durations for both adjectives and nouns were log transformed: the A_dur_log and N_dur_log columns.,We excluded values that were ¬±2 standard deviations from the mean.,"I used p-values (computed from t values from the linear models) and effect sizes (z-scores, based on the standardized and centered values of the variables).","Several measures were used, so I provide the data for each:

Looking at the interaction of condition and typ_mean, the dependent variables that showed significant correlations were N_max_pitch and N_dur_log.

For the N_max_pitch model, the interaction of condition AF and typ_mean was positive:
-0.002 [-0.006, 0.002]

For the N_dur_log model, the interaction of typ_mean showed negative correlations withboth condition AF and condition NF.
Interaction with condition AF: 0.026 [0.023, 0.030]
Interaction with condition NF: 0.010 [0.007, 0.013]",Z-score / standard units,"I had initially decided to use linear mixed models, but found that I ran into convergence issues whenever including speaker as a random intercept.  I therefore decided to switch to standard linear models."
2022-04-15 17:16:30,genyonemus_evotis,https://osf.io/drznv/?view_only=90b49153036c436b8f84bdbecdc722c4,"pydub, ffmpeg, sox, VoiceSauce, Praat","
tSNE, kernel density estimation, Generalized linear mixed-effects model",familiar,"The dependent variable is typicality. We regressed all the acoustic measures back to the levels of typicality. Given the large pool of acoustic measures, we expect collinearity and considerable correlation among the acoustic measures; thus, it is less valid to generate individual statistic models on treating each acoustic measure as dependent variables as a function of typicality. We argue that, based on the t-SNE visualizations and density plots, acoustic measures should be analyzed holistically as a group. Thus, we employed binary logistic regression with mixed-effects models to investigate the contributions of various acoustic measures on establishing a certain typicality category. Note that this model assumes a perception-production link during speech production, such that speakers would make an effort to produce and signal a less typical word combination to the listeners in a conversational setting. Further, there is a designed competitor in the different experimental conditions, so we expect speakers to actively distinguish between a less typical and a typical object and to convey the differences to be perceptible.","F0, H1*, H2*, H4*, H2K*, H5K*, H1*-H2*, H2*-H4*, H4*-2K*, H2K*-H5K*, cepstral peak prominence (CPP), harmonics-to-noise ratio (HNR05-35), subharmonics-to-harmonics ratio (SHR), energy, and strength of excitation (soe). We approach the question of phonetic modulation from the suprasegmental aspects of speech production. So, we employed acoustic measures of pitch and phonation to capture the levels of typicality.","From the syllable. We converted f0 to semitones and z-scored other acoustic measures, and then extracted quartiles over their cumulative density distribution. For categorical dependent variable ""typicality"", we assigned the typicality labels based on the median of the typicality ratings.","We take the four quartiles (25, 50, 75 and 95 percentiles of the sample) from the cumulative distribution function (cdf) of each measurement as representation of the overall distribution of acoustic measurements for F0 and voice quality for the segments of interest. F0 values were normalized to semitone with the 10th percentile as baseline.
All other measurements were normalized to Z-score with regard to the means of all the samples produced by individual speakers. These normalizations make it possible for comparisons with the same unit: the numeric values are relative to the sample distribution of measurements taken from the speaker. The final feature vector for each target word in each condition for a speaker thus consists of 4 points taken from the cdf of each acoustic measurement from the sonorant intervals.",We excluded values that are greater than the 97.5 percentile and smaller than the 2.5 percentile of the population.,"t-SNE visulizations, density plots of acoustic measures, and p-values of some predictors.","We interpret the effect size as the coefficients of the independent variables, which are included in the table of the report. More details output can be regenerated using the R code uploaded to the OSF page.","Coefficients of the independent predictors, which should be the normalized weight of the corresponding acoustic measure contributing to the overall probability of the response variable, typicality.",No.
2022-04-16 05:25:13,eosipterus_pytyopsittacus,https://osf.io/bzur7/?view_only=e391fe766e8f4b5ea009c1b0bc564dde,"We used Praat for all acoustic measurements (F0 and duration). Values based on those measurements (e.g., slope) were calculated in R using PoLaR-BEAR (citation).","Subsequently, R was used to perform a K-means clustering analysis based on slope measures, and to perform a Random Forest classification model of focus type (based on PoLaR labels, acoustic measures, and statistical analysis).",All team members are very familiar with PoLaR labelling and statistics on acoustic measures. Some of us are moderately familiar with the usage of K-means clustering and Random Forest classification on this sort of data; two of the four members have previously successfully used such techniques in previous research.,"For the k-means clustering analysis, the dependent variable was pitch accent type (as determined by PoLaR labels), the rationale being that different pitch accents should signal different meanings with respect to information structure (e.g., which words are semantically focused).

The primary goal was to run a random forest classification model, for which the dependent variable was the focus condition of the original experiment. The rationale was that we hoped that intonational information should be reliably conveyed and modulated by focus condition.
","For the k-means clustering analysis, the independent variables were normed f0-slopes (i.e. PoLaR ‚ÄúLevels‚Äù per second) that defined the close-copy of the f0 movements (as created by PoLaR labels)

For the random forest classification model, the independent variables were acoustic variables of the utterance (guided by PoLaR labels; e.g., f0 min (Hz), max (Hz), f0 range size (Hz)) as well as variables having to do with prominence (which word/words in relevant region is/are pitch-accented, and which types of pitch accent are used for each word type).
","Most of the operationalization was done according to PoLaR annotation conventions (guiding pitch range size, f0 turning points, and normed pitch values). Given the experimental design, we focused our analysis on the region of each utterance containing the determiner, adjective, and noun in the second half.
",We applied no transformations (though PoLaR annotations essentially transform f0 values to scaled/normed pitch levels 1-5).,"We excluded any observation in which the adjective-noun pairing was not ‚Äúmedium‚Äù, because only for ‚Äúmedium‚Äù typicality observations were there an equal number of observations across focus types.

Additionally, we excluded the files that were labelled as errors by the original labeller. We also excluded files where there were major disfluencies (involving the repetition of several words): MS‚Äôs 1st trial, CG‚Äôs 66th trial, and PB‚Äôs 31st trial.
","We examined the confusion matrix for the random forest classification, specifying that an error rate of less than 25% would indicate reliable performance. To decide whether a factor was a good predictor, we visually examined the relative importance of the factors in the classification model, and removed low-ranking variables while checking that the error rate in the confusion matrix did not go down.",N/A for this analysis.,N/A for this analysis.,"For utterances with more than two F0 turning points, we had to reassess how to calculate slope to capture multiple dynamic changes. In the end, we calculated a sum of slopes, which does have the limitation of not capturing differences between flat f0 contours and contours that both fall and rise. Additionally, rises comprised of two moderate excursions cannot be distinguished from a single extreme excursion."
2022-04-19 01:45:38,pervagor_adscensionis,https://osf.io/bx2ef/?view_only=f1d61131c1754f6a804b9abe38f71505,"Forced alignment - Montreal Forced Aligner, feature extraction - Matlab, generation of distances - Matlab, statistical analysis - R",Linear mixed effects models,Very,"Normalised distances between probability distributions generated from either MFCCs or f0 (we had planned to look at other acoustic measures, but didn't have time) -  we did this first using the entire utterance (as we didn't know which part of the utterance would be affected) and specifically with the adjective only. This process allowed us to compare entire utterances across all speakers and all utterances (if required). We approached this from a bottom-up perspective, as we had no strong hypotheses about what we would find and the research question we were asked was very general. We only considered the NF condition, as this was the only condition that appeared to address the research question",Typicality differences between adj-noun pairs in the NF condition (because our DV was distances between two utterances/ adjectives). We used the median typicality ratings from the norming experiment as this allowed us to keep the data continuous,"MFCCs and f0 were extracted at 10ms intervals across the entire utterance (we broke this down into the constituent parts of the utterances as well, to allow us to just look at the adjective). All of the data for each utterance/adjective were pooled to create a statistical model for that utterance/adjective","The raw data were (1) converted to a probability distribution using GMMs, (2) Kullback-Leibler divergences were calculated to measure the distance between utterances/adjectives (i.e. how different they are from each other) - this was done within speaker, (3) KL-divergences were z-scored within-speaker to normalise across speakers","We didn't remove outliers, but in retrospect, we might have taken the log of the normalised KL-divergences as the distributions aren't particularly normal",p-values,"MFCCs based on the whole utterance: -1.195e-03 (estimate) ¬±0.001 (95% confidence interval)

We've excluded the other analyses (see our report)",normalised divergence,"We had planned to do more feature selection and specific analyses of other elements of the utterance, but we didn't get to this due to time"
